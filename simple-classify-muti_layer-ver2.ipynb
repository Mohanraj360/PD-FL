{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# input_data shape\n",
    "Input: (batch_size, in_channel, width, height)\n",
    "# conv layer\n",
    "class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "input: (Batch_size, C_in, H_in, W_in)\n",
    "output: (Batch_size, C_out, H_out, W_out)\n",
    "\n",
    "weight(tensor): (out_channels, in_channels,kernel_size)\n",
    "bias(tensor): (out_channel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ripser import Rips\n",
    "from persim import PersistenceImager\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import re\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(111)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 500\n",
    "batchsize = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = os.listdir(\"./metric\")\n",
    "# data = []\n",
    "# a = []\n",
    "# for metric in dir:\n",
    "#     a = re.findall(\"\\d+\\.?\\d*\", metric)\n",
    "#     data.append([np.loadtxt(\"./metric/\"+metric), metric])\n",
    "#     # print(a[-2])\n",
    "#     print(metric[-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_dataset(Dataset):\n",
    "    def __init__(self, train):\n",
    "        super().__init__()\n",
    "        # 使用sin函数返回10000个时间序列,如果不自己构造数据，就使用numpy,pandas等读取自己的数据为x即可。\n",
    "        # 以下数据组织这块既可以放在init方法里，也可以放在getitem方法里\n",
    "        dir = os.listdir(\"./grids_trainset_cifar\")\n",
    "        dirtest = os.listdir(\"./grids_testset_cifar\")\n",
    "        data = []\n",
    "        a = []\n",
    "        if train:\n",
    "            # for metric in dir:\n",
    "            #     data.append(\n",
    "            #         [np.loadtxt(\"./grids_trainset_cifar/\"+metric), metric])\n",
    "            sample = []\n",
    "            for metric in dir:\n",
    "                sample.append([np.loadtxt(\"./grids_trainset_cifar/\"+metric), metric])\n",
    "                if len(sample) == 4:\n",
    "                    res = cv2.merge([i[0] for i in sample])\n",
    "                    res = np.transpose(res,(2,0,1))\n",
    "                    data.append([res, metric])\n",
    "                    sample = []\n",
    "            \n",
    "        else:\n",
    "            # for metric in dirtest:\n",
    "            #     data.append(\n",
    "            #         [np.loadtxt(\"./grids_testset_cifar/\"+metric), metric])\n",
    "            sample = []\n",
    "            for metric in dirtest:\n",
    "                sample.append([np.loadtxt(\"./grids_testset_cifar/\"+metric), metric])\n",
    "                if len(sample) == 4:\n",
    "                    res = cv2.merge([i[0] for i in sample])\n",
    "                    res = np.transpose(res,(2,0,1))\n",
    "                    data.append([res, metric])\n",
    "                    sample = []\n",
    "\n",
    "        self.x = [item[0] for item in data]\n",
    "        self.y = [math.ceil(int(re.findall(\"\\d+\\.?\\d*\", item[1])[0]) /\n",
    "                            (int(re.findall(\"\\d+\\.?\\d*\", item[1])[0])+1)) for item in data]\n",
    "        # self.y = [int(re.findall(\"\\d+\\.?\\d*\", item[1])[0]) for item in data]\n",
    "        self.src,  self.trg = [], []\n",
    "        for i in range(len(data)):\n",
    "            self.src.append(self.x[i])\n",
    "            self.trg.append(self.y[i])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.trg[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    " # 或者return len(self.trg), src和trg长度一样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5], [0.5])])\n",
    "     \n",
    "data_train = My_dataset(train=True)\n",
    "data_test = My_dataset(train=False)\n",
    "data_loader_train = DataLoader(data_train, batch_size=batchsize, shuffle=True)\n",
    "data_loader_test = DataLoader(data_test, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "# i_batch的多少根据batch size和def __len__(self)返回的长度确定\n",
    "# batch_data返回的值根据def __getitem__(self, index)来确定\n",
    "# 对训练集：(不太清楚enumerate返回什么的时候就多print试试)\n",
    "# for i_batch, batch_data in enumerate(data_loader_train):\n",
    "#     print(i_batch)  # 打印batch编号\n",
    "#     print(batch_data[0].shape)  # 打印该batch里面src\n",
    "#     print(batch_data[1])  # 打印该batch里面trg\n",
    "# # # 对测试集：（下面的语句也可以）\n",
    "# for i_batch, (src, trg) in enumerate(data_loader_test):\n",
    "#     print(i_batch)  # 打印batch编号\n",
    "#     print(src)  # 打印该batch里面src的尺寸\n",
    "#     print(trg)  # 打印该batch里面trg的尺寸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyNet(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MyNet, self).__init__()\n",
    "\n",
    "#         self.fc = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(256*256, 1024),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(1024, 1024),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(1024, 128),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(128, 64),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(64, 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1,256*256)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# model = MyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(4, 16, kernel_size=64),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16 * 32 * 32, 1024),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(1024, 1024),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(1024, 128),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyNet(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MyNet, self).__init__()\n",
    "#         self.layer1 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(4, 16, kernel_size=3),\n",
    "#             torch.nn.BatchNorm2d(16),\n",
    "#             torch.nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         self.layer2 = torch.nn.Sequential(\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.fc = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(16 * 63 * 63, 1024),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(1024, 1024),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(1024, 128),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(128, 64),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(64, 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# model = MyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyNet(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MyNet, self).__init__()\n",
    "#         self.layer1 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(4, 16, kernel_size=3),\n",
    "#             torch.nn.BatchNorm2d(16),\n",
    "#             torch.nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         self.layer2 = torch.nn.Sequential(\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.layer3 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(16, 32, kernel_size=3),\n",
    "#             torch.nn.BatchNorm2d(32),\n",
    "#             torch.nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         self.layer4 = torch.nn.Sequential(\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.fc = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(32 * 30 * 30, 1024),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(1024, 1024),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(1024, 128),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(128, 64),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(64, 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# model = MyNet().to(device)\n",
    "# # print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "# Softmax is internally computed.\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.694497466 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0002 cost = 0.694859803 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0003 cost = 0.695838511 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0004 cost = 0.693006337 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0005 cost = 0.695089340 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0006 cost = 0.693205476 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0007 cost = 0.692490339 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0008 cost = 0.691832066 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0009 cost = 0.690589845 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0010 cost = 0.692083538 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0011 cost = 0.692712367 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0012 cost = 0.691909671 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0013 cost = 0.690510631 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0014 cost = 0.691187799 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0015 cost = 0.690011501 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0016 cost = 0.690497220 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0017 cost = 0.689320028 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0018 cost = 0.690976083 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0019 cost = 0.689274371 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0020 cost = 0.688723743 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0021 cost = 0.688607693 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0022 cost = 0.688652515 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0023 cost = 0.687793374 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0024 cost = 0.687164247 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0025 cost = 0.687247872 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0026 cost = 0.687514126 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0027 cost = 0.687550008 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0028 cost = 0.685922623 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0029 cost = 0.686569750 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0030 cost = 0.685783386 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0031 cost = 0.686894178 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0032 cost = 0.684390187 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0033 cost = 0.684059322 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0034 cost = 0.685917795 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0035 cost = 0.684969485 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0036 cost = 0.683600664 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0037 cost = 0.682704151 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0038 cost = 0.683614671 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0039 cost = 0.683203936 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0040 cost = 0.683557093 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0041 cost = 0.681833208 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0042 cost = 0.681860149 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0043 cost = 0.681260169 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0044 cost = 0.681754947 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0045 cost = 0.680087984 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0046 cost = 0.680563331 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0047 cost = 0.680236936 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0048 cost = 0.679017425 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0049 cost = 0.679374099 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0050 cost = 0.679913700 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0051 cost = 0.678932905 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0052 cost = 0.677872896 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0053 cost = 0.677353561 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0054 cost = 0.677399993 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0055 cost = 0.676854432 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0056 cost = 0.674725533 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0057 cost = 0.675456703 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0058 cost = 0.675569952 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0059 cost = 0.676130772 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0060 cost = 0.676094294 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0061 cost = 0.674842179 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0062 cost = 0.673998535 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0063 cost = 0.673667073 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0064 cost = 0.672491431 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0065 cost = 0.673102915 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0066 cost = 0.671906173 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0067 cost = 0.672584534 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0068 cost = 0.670362651 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0069 cost = 0.671651900 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0070 cost = 0.670698881 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0071 cost = 0.669730663 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0072 cost = 0.669178963 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0073 cost = 0.669582784 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0074 cost = 0.667377055 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0075 cost = 0.668059826 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0076 cost = 0.665922999 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0077 cost = 0.667658091 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0078 cost = 0.665943444 trainacc: 50.53763440860215 testacc: 53.84615384615385\n",
      "Epoch: 0079 cost = 0.665753126 trainacc: 51.61290322580645 testacc: 53.84615384615385\n",
      "Epoch: 0080 cost = 0.664619625 trainacc: 51.61290322580645 testacc: 53.84615384615385\n",
      "Epoch: 0081 cost = 0.664377928 trainacc: 52.68817204301075 testacc: 53.84615384615385\n",
      "Epoch: 0082 cost = 0.664444804 trainacc: 53.763440860215056 testacc: 53.84615384615385\n",
      "Epoch: 0083 cost = 0.662945926 trainacc: 53.763440860215056 testacc: 53.84615384615385\n",
      "Epoch: 0084 cost = 0.662346184 trainacc: 53.763440860215056 testacc: 53.84615384615385\n",
      "Epoch: 0085 cost = 0.662452936 trainacc: 53.763440860215056 testacc: 53.84615384615385\n",
      "Epoch: 0086 cost = 0.661474466 trainacc: 54.83870967741935 testacc: 53.84615384615385\n",
      "Epoch: 0087 cost = 0.661555111 trainacc: 54.83870967741935 testacc: 53.84615384615385\n",
      "Epoch: 0088 cost = 0.661231756 trainacc: 55.913978494623656 testacc: 53.84615384615385\n",
      "Epoch: 0089 cost = 0.659226894 trainacc: 55.913978494623656 testacc: 53.84615384615385\n",
      "Epoch: 0090 cost = 0.659222364 trainacc: 55.913978494623656 testacc: 53.84615384615385\n",
      "Epoch: 0091 cost = 0.658607244 trainacc: 56.98924731182796 testacc: 53.84615384615385\n",
      "Epoch: 0092 cost = 0.657032371 trainacc: 58.064516129032256 testacc: 53.84615384615385\n",
      "Epoch: 0093 cost = 0.656513512 trainacc: 59.13978494623656 testacc: 53.84615384615385\n",
      "Epoch: 0094 cost = 0.655097842 trainacc: 59.13978494623656 testacc: 53.84615384615385\n",
      "Epoch: 0095 cost = 0.654964030 trainacc: 60.215053763440864 testacc: 53.84615384615385\n",
      "Epoch: 0096 cost = 0.654390156 trainacc: 60.215053763440864 testacc: 53.84615384615385\n",
      "Epoch: 0097 cost = 0.654621005 trainacc: 63.44086021505376 testacc: 53.84615384615385\n",
      "Epoch: 0098 cost = 0.654871702 trainacc: 73.11827956989248 testacc: 53.84615384615385\n",
      "Epoch: 0099 cost = 0.652652562 trainacc: 73.11827956989248 testacc: 53.84615384615385\n",
      "Epoch: 0100 cost = 0.651631594 trainacc: 73.11827956989248 testacc: 53.84615384615385\n",
      "Epoch: 0101 cost = 0.650967360 trainacc: 74.19354838709677 testacc: 53.84615384615385\n",
      "Epoch: 0102 cost = 0.650332570 trainacc: 75.26881720430107 testacc: 53.84615384615385\n",
      "Epoch: 0103 cost = 0.650062919 trainacc: 76.34408602150538 testacc: 53.84615384615385\n",
      "Epoch: 0104 cost = 0.648513675 trainacc: 76.34408602150538 testacc: 53.84615384615385\n",
      "Epoch: 0105 cost = 0.648508489 trainacc: 77.41935483870968 testacc: 53.84615384615385\n",
      "Epoch: 0106 cost = 0.645890057 trainacc: 76.34408602150538 testacc: 53.84615384615385\n",
      "Epoch: 0107 cost = 0.646138430 trainacc: 79.56989247311827 testacc: 53.84615384615385\n",
      "Epoch: 0108 cost = 0.645096004 trainacc: 80.64516129032258 testacc: 53.84615384615385\n",
      "Epoch: 0109 cost = 0.643971562 trainacc: 81.72043010752688 testacc: 53.84615384615385\n",
      "Epoch: 0110 cost = 0.641667545 trainacc: 76.34408602150538 testacc: 53.84615384615385\n",
      "Epoch: 0111 cost = 0.644361138 trainacc: 84.94623655913979 testacc: 53.84615384615385\n",
      "Epoch: 0112 cost = 0.640810728 trainacc: 82.79569892473118 testacc: 53.84615384615385\n",
      "Epoch: 0113 cost = 0.641134799 trainacc: 84.94623655913979 testacc: 53.84615384615385\n",
      "Epoch: 0114 cost = 0.641283751 trainacc: 88.17204301075269 testacc: 53.84615384615385\n",
      "Epoch: 0115 cost = 0.639688253 trainacc: 88.17204301075269 testacc: 53.84615384615385\n",
      "Epoch: 0116 cost = 0.639110446 trainacc: 89.24731182795699 testacc: 53.84615384615385\n",
      "Epoch: 0117 cost = 0.636493742 trainacc: 89.24731182795699 testacc: 53.84615384615385\n",
      "Epoch: 0118 cost = 0.635851562 trainacc: 89.24731182795699 testacc: 53.84615384615385\n",
      "Epoch: 0119 cost = 0.633979976 trainacc: 89.24731182795699 testacc: 53.84615384615385\n",
      "Epoch: 0120 cost = 0.634210706 trainacc: 93.54838709677419 testacc: 53.84615384615385\n",
      "Epoch: 0121 cost = 0.633971989 trainacc: 94.6236559139785 testacc: 53.84615384615385\n",
      "Epoch: 0122 cost = 0.632100344 trainacc: 94.6236559139785 testacc: 53.84615384615385\n",
      "Epoch: 0123 cost = 0.631345093 trainacc: 95.6989247311828 testacc: 53.84615384615385\n",
      "Epoch: 0124 cost = 0.629912496 trainacc: 95.6989247311828 testacc: 53.84615384615385\n",
      "Epoch: 0125 cost = 0.629325211 trainacc: 95.6989247311828 testacc: 53.84615384615385\n",
      "Epoch: 0126 cost = 0.627305567 trainacc: 95.6989247311828 testacc: 53.84615384615385\n",
      "Epoch: 0127 cost = 0.626565516 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0128 cost = 0.626051605 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0129 cost = 0.624577463 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0130 cost = 0.622331262 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0131 cost = 0.621631145 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0132 cost = 0.620008349 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0133 cost = 0.619830847 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0134 cost = 0.618213296 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0135 cost = 0.617967963 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0136 cost = 0.616247356 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0137 cost = 0.613159716 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0138 cost = 0.611632526 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0139 cost = 0.609597445 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0140 cost = 0.609985292 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0141 cost = 0.608212650 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0142 cost = 0.607173145 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0143 cost = 0.604041576 trainacc: 98.9247311827957 testacc: 53.84615384615385\n",
      "Epoch: 0144 cost = 0.603911400 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0145 cost = 0.601951241 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0146 cost = 0.601443052 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0147 cost = 0.598988891 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0148 cost = 0.600309968 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0149 cost = 0.595394075 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0150 cost = 0.592748165 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0151 cost = 0.592432559 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0152 cost = 0.589232206 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0153 cost = 0.588262439 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0154 cost = 0.585997939 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0155 cost = 0.583637178 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0156 cost = 0.583452284 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0157 cost = 0.580704749 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0158 cost = 0.578507304 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0159 cost = 0.576722324 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0160 cost = 0.574954510 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0161 cost = 0.571357727 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0162 cost = 0.570051312 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0163 cost = 0.567637384 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0164 cost = 0.566419244 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0165 cost = 0.563382268 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0166 cost = 0.559679866 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0167 cost = 0.557583451 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0168 cost = 0.554425597 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0169 cost = 0.550976932 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0170 cost = 0.549533367 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0171 cost = 0.544752717 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0172 cost = 0.542541206 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0173 cost = 0.540741801 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0174 cost = 0.538894773 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0175 cost = 0.535829067 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0176 cost = 0.533817291 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0177 cost = 0.528191864 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0178 cost = 0.525264025 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0179 cost = 0.521589875 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0180 cost = 0.519538701 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0181 cost = 0.515192389 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0182 cost = 0.512228370 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0183 cost = 0.507558405 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0184 cost = 0.504755914 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0185 cost = 0.501434565 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0186 cost = 0.497398466 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0187 cost = 0.493926227 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0188 cost = 0.491942883 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0189 cost = 0.488022864 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0190 cost = 0.483386874 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0191 cost = 0.479756773 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0192 cost = 0.478190184 trainacc: 100.0 testacc: 55.76923076923077\n",
      "Epoch: 0193 cost = 0.471625984 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0194 cost = 0.466542780 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0195 cost = 0.462463379 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0196 cost = 0.458957046 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0197 cost = 0.453707993 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0198 cost = 0.449654669 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0199 cost = 0.444995165 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0200 cost = 0.440841585 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0201 cost = 0.438189864 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0202 cost = 0.434974611 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0203 cost = 0.426135004 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0204 cost = 0.422301054 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0205 cost = 0.416138947 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0206 cost = 0.410816878 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0207 cost = 0.406735599 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0208 cost = 0.401440591 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0209 cost = 0.394823194 trainacc: 100.0 testacc: 57.69230769230769\n",
      "Epoch: 0210 cost = 0.389927059 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0211 cost = 0.386479795 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0212 cost = 0.379137576 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0213 cost = 0.374918193 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0214 cost = 0.368170023 trainacc: 100.0 testacc: 55.76923076923077\n",
      "Epoch: 0215 cost = 0.362224072 trainacc: 100.0 testacc: 55.76923076923077\n",
      "Epoch: 0216 cost = 0.356500089 trainacc: 100.0 testacc: 57.69230769230769\n",
      "Epoch: 0217 cost = 0.350656450 trainacc: 100.0 testacc: 57.69230769230769\n",
      "Epoch: 0218 cost = 0.345266610 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0219 cost = 0.340215802 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0220 cost = 0.335925221 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0221 cost = 0.328455567 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0222 cost = 0.320666254 trainacc: 100.0 testacc: 53.84615384615385\n",
      "Epoch: 0223 cost = 0.317249537 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0224 cost = 0.310747802 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0225 cost = 0.304813951 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0226 cost = 0.299186766 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0227 cost = 0.293185294 trainacc: 100.0 testacc: 59.61538461538461\n",
      "Epoch: 0228 cost = 0.288265735 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0229 cost = 0.283506513 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0230 cost = 0.276340008 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0231 cost = 0.271306753 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0232 cost = 0.265101403 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0233 cost = 0.259905010 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0234 cost = 0.252145767 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0235 cost = 0.247144148 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0236 cost = 0.241179243 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0237 cost = 0.235821888 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0238 cost = 0.231465340 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0239 cost = 0.224657744 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0240 cost = 0.221149176 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0241 cost = 0.215472713 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0242 cost = 0.208429649 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0243 cost = 0.204369202 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0244 cost = 0.199194655 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0245 cost = 0.193692237 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0246 cost = 0.188365102 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0247 cost = 0.183916286 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0248 cost = 0.178946778 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0249 cost = 0.175117552 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0250 cost = 0.169933200 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0251 cost = 0.166824728 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0252 cost = 0.162153706 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0253 cost = 0.157825753 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0254 cost = 0.153865069 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0255 cost = 0.149968252 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0256 cost = 0.146648109 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0257 cost = 0.142665222 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0258 cost = 0.138460457 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0259 cost = 0.134849831 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0260 cost = 0.131673694 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0261 cost = 0.128375515 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0262 cost = 0.125161514 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0263 cost = 0.122203767 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0264 cost = 0.118577957 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0265 cost = 0.115877368 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0266 cost = 0.113148019 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0267 cost = 0.110537842 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0268 cost = 0.107147262 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0269 cost = 0.105134174 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0270 cost = 0.102249071 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0271 cost = 0.099776357 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0272 cost = 0.099410489 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0273 cost = 0.095161162 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0274 cost = 0.092558049 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0275 cost = 0.090232618 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0276 cost = 0.088221379 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0277 cost = 0.086207934 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0278 cost = 0.084125623 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0279 cost = 0.082332239 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0280 cost = 0.080455445 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0281 cost = 0.078863174 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0282 cost = 0.076612234 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0283 cost = 0.074857973 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0284 cost = 0.073243618 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0285 cost = 0.071711466 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0286 cost = 0.070297368 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0287 cost = 0.068424150 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0288 cost = 0.066878729 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0289 cost = 0.065434530 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0290 cost = 0.063939005 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0291 cost = 0.062725350 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0292 cost = 0.061295949 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0293 cost = 0.060043320 trainacc: 100.0 testacc: 61.53846153846154\n",
      "Epoch: 0294 cost = 0.058786985 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0295 cost = 0.057709329 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0296 cost = 0.056580190 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0297 cost = 0.055371094 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0298 cost = 0.054229833 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0299 cost = 0.053021614 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0300 cost = 0.052138209 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0301 cost = 0.050776541 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0302 cost = 0.049898140 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0303 cost = 0.048977531 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0304 cost = 0.048087336 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0305 cost = 0.047395255 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0306 cost = 0.046344463 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0307 cost = 0.045433830 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0308 cost = 0.044420145 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0309 cost = 0.043462846 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0310 cost = 0.042771194 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0311 cost = 0.042135000 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0312 cost = 0.041042317 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0313 cost = 0.040635426 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0314 cost = 0.039811097 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0315 cost = 0.039083064 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0316 cost = 0.038501196 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0317 cost = 0.037764065 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0318 cost = 0.037286922 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0319 cost = 0.036564182 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0320 cost = 0.036046214 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0321 cost = 0.035332419 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0322 cost = 0.034800857 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0323 cost = 0.034199841 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0324 cost = 0.033540636 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0325 cost = 0.033301137 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0326 cost = 0.032401189 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0327 cost = 0.032007471 trainacc: 100.0 testacc: 71.15384615384616\n",
      "Epoch: 0328 cost = 0.031500332 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0329 cost = 0.031027630 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0330 cost = 0.030568004 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0331 cost = 0.030016754 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0332 cost = 0.029593263 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0333 cost = 0.029056799 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0334 cost = 0.028568115 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0335 cost = 0.028193032 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0336 cost = 0.027799018 trainacc: 100.0 testacc: 69.23076923076923\n",
      "Epoch: 0337 cost = 0.027521450 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0338 cost = 0.027044266 trainacc: 100.0 testacc: 69.23076923076923\n",
      "Epoch: 0339 cost = 0.026690854 trainacc: 100.0 testacc: 69.23076923076923\n",
      "Epoch: 0340 cost = 0.026431005 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0341 cost = 0.025860850 trainacc: 100.0 testacc: 71.15384615384616\n",
      "Epoch: 0342 cost = 0.025554538 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0343 cost = 0.025165623 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0344 cost = 0.024858978 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0345 cost = 0.024437718 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0346 cost = 0.024148289 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0347 cost = 0.023835732 trainacc: 100.0 testacc: 71.15384615384616\n",
      "Epoch: 0348 cost = 0.023474783 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0349 cost = 0.023138352 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0350 cost = 0.022804167 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0351 cost = 0.022498481 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0352 cost = 0.022288729 trainacc: 100.0 testacc: 71.15384615384616\n",
      "Epoch: 0353 cost = 0.021973554 trainacc: 100.0 testacc: 69.23076923076923\n",
      "Epoch: 0354 cost = 0.021773832 trainacc: 100.0 testacc: 71.15384615384616\n",
      "Epoch: 0355 cost = 0.021398697 trainacc: 100.0 testacc: 71.15384615384616\n",
      "Epoch: 0356 cost = 0.021102421 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0357 cost = 0.020901762 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0358 cost = 0.020667965 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0359 cost = 0.020396255 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0360 cost = 0.020024735 trainacc: 100.0 testacc: 71.15384615384616\n",
      "Epoch: 0361 cost = 0.019872058 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0362 cost = 0.019640431 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0363 cost = 0.019220937 trainacc: 100.0 testacc: 69.23076923076923\n",
      "Epoch: 0364 cost = 0.019169848 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0365 cost = 0.018907975 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0366 cost = 0.018685725 trainacc: 100.0 testacc: 69.23076923076923\n",
      "Epoch: 0367 cost = 0.018478464 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0368 cost = 0.018221475 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0369 cost = 0.018078992 trainacc: 100.0 testacc: 69.23076923076923\n",
      "Epoch: 0370 cost = 0.017857406 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0371 cost = 0.017669939 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0372 cost = 0.017625924 trainacc: 100.0 testacc: 63.46153846153846\n",
      "Epoch: 0373 cost = 0.017263703 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0374 cost = 0.017080929 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0375 cost = 0.016948970 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0376 cost = 0.016654812 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0377 cost = 0.016576501 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0378 cost = 0.016382050 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0379 cost = 0.016151324 trainacc: 100.0 testacc: 69.23076923076923\n",
      "Epoch: 0380 cost = 0.016034188 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0381 cost = 0.015837472 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0382 cost = 0.015594537 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0383 cost = 0.015495449 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0384 cost = 0.015331379 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0385 cost = 0.015116503 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0386 cost = 0.014984259 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0387 cost = 0.014758345 trainacc: 100.0 testacc: 69.23076923076923\n",
      "Epoch: 0388 cost = 0.014735674 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0389 cost = 0.014572104 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0390 cost = 0.014367037 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0391 cost = 0.014211965 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0392 cost = 0.014086220 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0393 cost = 0.013970007 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0394 cost = 0.013772152 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0395 cost = 0.013656357 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0396 cost = 0.013530730 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0397 cost = 0.013399957 trainacc: 100.0 testacc: 67.3076923076923\n",
      "Epoch: 0398 cost = 0.013282206 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0399 cost = 0.013162264 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0400 cost = 0.013033167 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0401 cost = 0.012936124 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0402 cost = 0.012815157 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0403 cost = 0.012700127 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0404 cost = 0.012522084 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0405 cost = 0.012493880 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0406 cost = 0.012371797 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0407 cost = 0.012213555 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0408 cost = 0.012087029 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0409 cost = 0.012019602 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0410 cost = 0.011914050 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0411 cost = 0.011755663 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0412 cost = 0.011676470 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0413 cost = 0.011605595 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0414 cost = 0.011480337 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0415 cost = 0.011368168 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0416 cost = 0.011272948 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0417 cost = 0.011204517 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0418 cost = 0.011153535 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0419 cost = 0.011029766 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0420 cost = 0.010930767 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0421 cost = 0.010759769 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0422 cost = 0.010767573 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0423 cost = 0.010593565 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0424 cost = 0.010539754 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0425 cost = 0.010447176 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0426 cost = 0.010345568 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0427 cost = 0.010303442 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0428 cost = 0.010258527 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0429 cost = 0.010133778 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0430 cost = 0.010019177 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0431 cost = 0.009929748 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0432 cost = 0.009895982 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0433 cost = 0.009776384 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0434 cost = 0.009804490 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0435 cost = 0.009698546 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0436 cost = 0.009558270 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0437 cost = 0.009437254 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0438 cost = 0.009440457 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0439 cost = 0.009304976 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0440 cost = 0.009290779 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0441 cost = 0.009213965 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0442 cost = 0.009160860 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0443 cost = 0.009094396 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0444 cost = 0.008974470 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0445 cost = 0.008941770 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0446 cost = 0.008872739 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0447 cost = 0.008777946 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0448 cost = 0.008695530 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0449 cost = 0.008614995 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0450 cost = 0.008641585 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0451 cost = 0.008548865 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0452 cost = 0.008466816 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0453 cost = 0.008432380 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0454 cost = 0.008339232 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0455 cost = 0.008352460 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0456 cost = 0.008215195 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0457 cost = 0.008219391 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0458 cost = 0.008153548 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0459 cost = 0.008084959 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0460 cost = 0.008032056 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0461 cost = 0.007957772 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0462 cost = 0.007947460 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0463 cost = 0.007809485 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0464 cost = 0.007692430 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0465 cost = 0.007644777 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0466 cost = 0.007696479 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0467 cost = 0.007626080 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0468 cost = 0.007595141 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0469 cost = 0.007548204 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0470 cost = 0.007475839 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0471 cost = 0.007439627 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0472 cost = 0.007379395 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0473 cost = 0.007369105 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0474 cost = 0.007294290 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0475 cost = 0.007239135 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0476 cost = 0.007242010 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0477 cost = 0.007187262 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0478 cost = 0.007152564 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0479 cost = 0.007020677 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0480 cost = 0.007026040 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0481 cost = 0.006992956 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0482 cost = 0.006941692 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0483 cost = 0.006902880 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0484 cost = 0.006833477 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0485 cost = 0.006786549 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0486 cost = 0.006787677 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0487 cost = 0.006746126 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0488 cost = 0.006623562 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0489 cost = 0.006677533 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0490 cost = 0.006559661 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0491 cost = 0.006578667 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0492 cost = 0.006534108 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0493 cost = 0.006499425 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0494 cost = 0.006444169 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0495 cost = 0.006453588 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0496 cost = 0.006367516 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0497 cost = 0.006338086 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0498 cost = 0.006299025 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0499 cost = 0.006282953 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Epoch: 0500 cost = 0.006178201 trainacc: 100.0 testacc: 65.38461538461539\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader_train)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    avg_cost = 0\n",
    "\n",
    "    # for X, Y in data_loader_train:\n",
    "    #     X = torch.autograd.Variable(X).to(device).float()\n",
    "    #     Y = Y.to(device)\n",
    "\n",
    "    #     optimizer.zero_grad()\n",
    "    #     hypothesis = model(X)\n",
    "    #     cost = criterion(hypothesis, Y)\n",
    "    #     cost.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    #     avg_cost += cost / total_batch\n",
    "    # model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader_train):\n",
    "        inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        avg_cost += loss / total_batch\n",
    "\n",
    "    \n",
    "    # if avg_cost < 0.1:\n",
    "    #     break\n",
    "    model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for X_test, Y_test in data_loader_test:\n",
    "    #         X_test = X_test.to(device).float()\n",
    "    #         Y_test = Y_test.to(device)\n",
    "\n",
    "    #     prediction = model(X_test)\n",
    "    #     correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    #     accuracy = correct_prediction.float().mean()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader_train):\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    trainacc = 100.*correct/total\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader_test):\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    testacc = 100.*correct/total\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost), 'trainacc:', trainacc, 'testacc:', testacc)\n",
    "\n",
    "print('Learning finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_batch = len(data_loader_train)\n",
    "# model.train()\n",
    "# for epoch in range(training_epochs):\n",
    "#     avg_cost = 0\n",
    "\n",
    "#     for X, Y in data_loader_train:\n",
    "#         X = torch.autograd.Variable(X).to(device).float()\n",
    "#         Y = Y.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         hypothesis = model(X)\n",
    "#         cost = criterion(hypothesis, Y)\n",
    "#         cost.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         avg_cost += cost / total_batch\n",
    "\n",
    "#     print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "# print('Learning finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1895e-01, -2.2535e-01],\n",
      "        [ 1.2109e-01, -2.1165e-01],\n",
      "        [-2.7388e-01,  2.3258e-01],\n",
      "        [ 1.2152e-01, -2.8349e-01],\n",
      "        [-1.8620e-01,  1.0789e-01],\n",
      "        [ 1.6650e-01, -2.8374e-01],\n",
      "        [-5.3804e-02, -4.0294e-02],\n",
      "        [ 7.8285e-02, -2.0636e-01],\n",
      "        [ 1.8691e-01, -3.1492e-01],\n",
      "        [ 2.2518e-04, -9.1857e-02],\n",
      "        [-6.8734e-03, -1.0427e-01],\n",
      "        [-2.4301e-01,  1.9451e-01],\n",
      "        [-7.0770e-02, -4.8966e-02],\n",
      "        [-3.5307e-01,  3.0423e-01],\n",
      "        [ 3.1977e-02, -1.5208e-01],\n",
      "        [-1.7466e-03, -7.7578e-02],\n",
      "        [-2.1794e-01,  1.2970e-01],\n",
      "        [-1.5280e-01,  2.2714e-02],\n",
      "        [-3.1461e-01,  2.7028e-01],\n",
      "        [ 6.5254e-02, -1.8216e-01],\n",
      "        [-2.5813e-01,  1.9621e-01],\n",
      "        [-2.7304e-01,  2.1858e-01],\n",
      "        [ 1.3397e-01, -2.4446e-01],\n",
      "        [ 2.0213e-01, -3.2848e-01],\n",
      "        [-4.9636e-02,  5.0565e-04]], device='cuda:0') tensor([False,  True,  True,  True,  True,  True, False, False,  True, False,\n",
      "         True, False, False,  True,  True,  True, False,  True,  True, False,\n",
      "         True,  True,  True,  True, False], device='cuda:0') tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0], device='cuda:0')\n",
      "Accuracy: 0.6399999856948853\n",
      "tensor([[ 0.1668, -0.2716],\n",
      "        [ 0.0566, -0.1962],\n",
      "        [-0.2852,  0.2521],\n",
      "        [-0.0332, -0.0580],\n",
      "        [ 0.0949, -0.1880],\n",
      "        [ 0.4623, -0.5776],\n",
      "        [ 0.2284, -0.3503],\n",
      "        [ 0.0647, -0.1881],\n",
      "        [-0.1719,  0.1063],\n",
      "        [-0.0114, -0.0742],\n",
      "        [ 0.0491, -0.1740],\n",
      "        [-0.0756,  0.0185],\n",
      "        [ 0.0640, -0.1310],\n",
      "        [ 0.0415, -0.1005],\n",
      "        [ 0.4671, -0.6347],\n",
      "        [ 0.0044, -0.1307],\n",
      "        [-0.3592,  0.2954],\n",
      "        [ 0.1834, -0.2849],\n",
      "        [-0.3144,  0.2535],\n",
      "        [ 0.1028, -0.2118],\n",
      "        [ 0.3427, -0.4817],\n",
      "        [-0.0694, -0.0197],\n",
      "        [ 0.0116, -0.1331],\n",
      "        [ 0.2204, -0.3423],\n",
      "        [-0.3070,  0.2248]], device='cuda:0') tensor([ True,  True, False,  True,  True,  True, False,  True, False, False,\n",
      "         True,  True, False,  True,  True,  True,  True, False,  True, False,\n",
      "        False,  True,  True,  True,  True], device='cuda:0') tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        1], device='cuda:0')\n",
      "Accuracy: 0.6800000071525574\n",
      "tensor([[ 0.0716, -0.2248],\n",
      "        [-0.1919,  0.0863]], device='cuda:0') tensor([False,  True], device='cuda:0') tensor([1, 1], device='cuda:0')\n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "# model.load_state_dict(torch.load(\".\\merge_save\\merge_attack0normal10_05-07--21-10-32.pth\")[\"state_dict\"])\n",
    "# model.load_state_dict(torch.load(r\".\\myfed_normal_save\\model18-24-02.pth\")[\"state_dict\"])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_test, Y_test in data_loader_test:\n",
    "        X_test = X_test.to(device).float()\n",
    "        Y_test = Y_test.to(device)\n",
    "\n",
    "        prediction = model(X_test)\n",
    "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        print(prediction,correct_prediction, Y_test)\n",
    "        accuracy = correct_prediction.float().mean()\n",
    "        print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # Get one and predict\n",
    "    # r = random.randint(0, len(mnist_test) - 1)\n",
    "    # X_single_data = mnist_test.data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    # Y_single_data = mnist_test.targets[r:r + 1].to(device)\n",
    "\n",
    "    # print('Label: ', Y_single_data.item())\n",
    "    # single_prediction = model(X_single_data)\n",
    "    # print('Prediction: ', torch.argmax(single_prediction, 1).item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d28dcb1746f058774bc7e3103f8fd674f9a46f28636659ba6a6fdaf0b17e1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
