{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# input_data shape\n",
    "Input: (batch_size, in_channel, width, height)\n",
    "# conv layer\n",
    "class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "input: (Batch_size, C_in, H_in, W_in)\n",
    "output: (Batch_size, C_out, H_out, W_out)\n",
    "\n",
    "weight(tensor): (out_channels, in_channels,kernel_size)\n",
    "bias(tensor): (out_channel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\git\\TDA-NN\\simple-classify-muti_layer-mnist.ipynb Cell 2\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/git/TDA-NN/simple-classify-muti_layer-mnist.ipynb#ch0000001?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/git/TDA-NN/simple-classify-muti_layer-mnist.ipynb#ch0000001?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m multivariate_normal \u001b[39mas\u001b[39;00m mvn\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/git/TDA-NN/simple-classify-muti_layer-mnist.ipynb#ch0000001?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/git/TDA-NN/simple-classify-muti_layer-mnist.ipynb#ch0000001?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mripser\u001b[39;00m \u001b[39mimport\u001b[39;00m Rips\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/git/TDA-NN/simple-classify-muti_layer-mnist.ipynb#ch0000001?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpersim\u001b[39;00m \u001b[39mimport\u001b[39;00m PersistenceImager\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\matplotlib\\pyplot.py:49\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcycler\u001b[39;00m \u001b[39mimport\u001b[39;00m cycler\n\u001b[0;32m     48\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolorbar\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\matplotlib\\colorbar.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, collections, cm, colors, contour, ticker\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39martist\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmartist\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpatches\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpatches\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\matplotlib\\contour.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, docstring\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m MouseButton\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpath\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpath\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mticker\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mticker\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\matplotlib\\backend_bases.py:46\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     47\u001b[0m     _api, backend_tools \u001b[39mas\u001b[39;00m tools, cbook, colors, docstring, textpath,\n\u001b[0;32m     48\u001b[0m     tight_bbox, transforms, widgets, get_backend, is_interactive, rcParams)\n\u001b[0;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_pylab_helpers\u001b[39;00m \u001b[39mimport\u001b[39;00m Gcf\n\u001b[0;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_managers\u001b[39;00m \u001b[39mimport\u001b[39;00m ToolManager\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\matplotlib\\textpath.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparse\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _text_helpers, dviread, font_manager\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfont_manager\u001b[39;00m \u001b[39mimport\u001b[39;00m FontProperties, get_font\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mft2font\u001b[39;00m \u001b[39mimport\u001b[39;00m LOAD_NO_HINTING, LOAD_TARGET_LIGHT\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:941\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1039\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ripser import Rips\n",
    "from persim import PersistenceImager\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import re\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(111)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 600\n",
    "batchsize = 25\n",
    "testbatchsize = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = os.listdir(\"./metric\")\n",
    "# data = []\n",
    "# a = []\n",
    "# for metric in dir:\n",
    "#     a = re.findall(\"\\d+\\.?\\d*\", metric)\n",
    "#     data.append([np.loadtxt(\"./metric/\"+metric), metric])\n",
    "#     # print(a[-2])\n",
    "#     print(metric[-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_dataset(Dataset):\n",
    "    def __init__(self, train):\n",
    "        super().__init__()\n",
    "        # 使用sin函数返回10000个时间序列,如果不自己构造数据，就使用numpy,pandas等读取自己的数据为x即可。\n",
    "        # 以下数据组织这块既可以放在init方法里，也可以放在getitem方法里\n",
    "        dir = os.listdir(\"./grids_trainset\")\n",
    "        dirtest = os.listdir(\"./grids_testset\")\n",
    "        data = []\n",
    "        a = []\n",
    "        if train:\n",
    "            # for metric in dir:\n",
    "            #     data.append(\n",
    "            #         [np.loadtxt(\"./grids_trainset_cifar/\"+metric), metric])\n",
    "            sample = []\n",
    "            for metric in dir:\n",
    "                sample.append([np.loadtxt(\"./grids_trainset/\"+metric), metric])\n",
    "                if len(sample) == 4:\n",
    "                    res = cv2.merge([i[0] for i in sample])\n",
    "                    res = np.transpose(res,(2,0,1))\n",
    "                    data.append([res, metric])\n",
    "                    sample = []\n",
    "            \n",
    "        else:\n",
    "            # for metric in dirtest:\n",
    "            #     data.append(\n",
    "            #         [np.loadtxt(\"./grids_testset_cifar/\"+metric), metric])\n",
    "            sample = []\n",
    "            for metric in dirtest:\n",
    "                sample.append([np.loadtxt(\"./grids_testset/\"+metric), metric])\n",
    "                if len(sample) == 4:\n",
    "                    res = cv2.merge([i[0] for i in sample])\n",
    "                    res = np.transpose(res,(2,0,1))\n",
    "                    data.append([res, metric])\n",
    "                    sample = []\n",
    "\n",
    "        # data = [item for item in data if (int(re.findall(\"\\d+\\.?\\d*\", item[1])[0]) == 0 or int(re.findall(\"\\d+\\.?\\d*\", item[1])[0]) > 1)]\n",
    "        self.x = [item[0] for item in data]\n",
    "        self.y = [math.ceil(int(re.findall(\"\\d+\\.?\\d*\", item[1])[0]) /\n",
    "                            (int(re.findall(\"\\d+\\.?\\d*\", item[1])[0])+1)) for item in data]\n",
    "        # self.y = [int(re.findall(\"\\d+\\.?\\d*\", item[1])[0]) for item in data]\n",
    "        self.src,  self.trg = [], []\n",
    "        for i in range(len(data)):\n",
    "            self.src.append(self.x[i])\n",
    "            self.trg.append(self.y[i])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.trg[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    " # 或者return len(self.trg), src和trg长度一样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "0\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
      "        1])\n",
      "1\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0])\n",
      "2\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "        1])\n",
      "3\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1])\n",
      "4\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
      "        1])\n",
      "test\n",
      "0\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 2.,  ..., 0., 0., 0.],\n",
      "          [0., 2., 2.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 2., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 6., 3.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 2., 3.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 5., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 7., 3.,  ..., 0., 0., 0.],\n",
      "          [0., 6., 5.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], dtype=torch.float64)\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1])\n",
      "1\n",
      "tensor([[[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  1.,  2.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  3.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  2.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  4.,  1.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  3.,  4.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  2.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  4.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  6.,  2.,  ...,  0.,  0.,  0.],\n",
      "          [ 0., 24.,  3.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  4.,  1.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  3.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  2.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  2.,  8.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  2.,  5.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0., 19.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  6.,  3.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]]], dtype=torch.float64)\n",
      "tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1])\n",
      "2\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 4., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 2., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 4., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 9., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 2., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 5., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], dtype=torch.float64)\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "data_tf = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5], [0.5])])\n",
    "     \n",
    "data_train = My_dataset(train=True)\n",
    "data_test = My_dataset(train=False)\n",
    "data_loader_train = DataLoader(data_train, batch_size=batchsize, shuffle=True)\n",
    "data_loader_test = DataLoader(data_test, batch_size=testbatchsize, shuffle=True)\n",
    "\n",
    "\n",
    "# i_batch的多少根据batch size和def __len__(self)返回的长度确定\n",
    "# batch_data返回的值根据def __getitem__(self, index)来确定\n",
    "# 对训练集：(不太清楚enumerate返回什么的时候就多print试试)\n",
    "print(\"train\")\n",
    "for i_batch, batch_data in enumerate(data_loader_train):\n",
    "    print(i_batch)  # 打印batch编号\n",
    "    print(batch_data[0].shape)  # 打印该batch里面src\n",
    "    print(batch_data[1])  # 打印该batch里面trg\n",
    "# # 对测试集：（下面的语句也可以）\n",
    "print(\"test\")\n",
    "for i_batch, (src, trg) in enumerate(data_loader_test):\n",
    "    print(i_batch)  # 打印batch编号\n",
    "    print(src)  # 打印该batch里面src的尺寸\n",
    "    print(trg)  # 打印该batch里面trg的尺寸\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyNet(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MyNet, self).__init__()\n",
    "\n",
    "#         self.fc = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(256*256, 1024),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(1024, 1024),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(1024, 128),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(128, 64),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(64, 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1,256*256)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# model = MyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(4, 16, kernel_size=64),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16 * 32 * 32, 1024),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(1024, 1024),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(1024, 128),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "# Softmax is internally computed.\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.692817569 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0002 cost = 0.692546427 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0003 cost = 0.692036808 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0004 cost = 0.691783845 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0005 cost = 0.691407800 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0006 cost = 0.691054165 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0007 cost = 0.690825343 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0008 cost = 0.690489948 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0009 cost = 0.690196335 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0010 cost = 0.689880252 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0011 cost = 0.689448655 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0012 cost = 0.689189315 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0013 cost = 0.688831568 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0014 cost = 0.688656807 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0015 cost = 0.688206792 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0016 cost = 0.688012302 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0017 cost = 0.687527239 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0018 cost = 0.687164545 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0019 cost = 0.686976552 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0020 cost = 0.686669111 trainacc: 50.4 testacc: 50.0\n",
      "Epoch: 0021 cost = 0.686235309 trainacc: 51.2 testacc: 50.0\n",
      "Epoch: 0022 cost = 0.685867667 trainacc: 51.2 testacc: 50.0\n",
      "Epoch: 0023 cost = 0.685578346 trainacc: 51.2 testacc: 50.0\n",
      "Epoch: 0024 cost = 0.685346782 trainacc: 51.2 testacc: 50.0\n",
      "Epoch: 0025 cost = 0.684986651 trainacc: 51.2 testacc: 50.0\n",
      "Epoch: 0026 cost = 0.684684157 trainacc: 51.2 testacc: 50.0\n",
      "Epoch: 0027 cost = 0.684305310 trainacc: 51.2 testacc: 50.0\n",
      "Epoch: 0028 cost = 0.684038401 trainacc: 52.0 testacc: 50.0\n",
      "Epoch: 0029 cost = 0.683693051 trainacc: 52.8 testacc: 50.0\n",
      "Epoch: 0030 cost = 0.683486223 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0031 cost = 0.683054686 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0032 cost = 0.682944357 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0033 cost = 0.682477951 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0034 cost = 0.682209432 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0035 cost = 0.681874037 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0036 cost = 0.681575358 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0037 cost = 0.681115568 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0038 cost = 0.680830956 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0039 cost = 0.680679440 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0040 cost = 0.680071294 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0041 cost = 0.679928422 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0042 cost = 0.679411113 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0043 cost = 0.679329276 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0044 cost = 0.678744137 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0045 cost = 0.678438008 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0046 cost = 0.678025424 trainacc: 53.6 testacc: 50.0\n",
      "Epoch: 0047 cost = 0.677519858 trainacc: 54.4 testacc: 50.0\n",
      "Epoch: 0048 cost = 0.677583992 trainacc: 55.2 testacc: 50.0\n",
      "Epoch: 0049 cost = 0.676916957 trainacc: 55.2 testacc: 50.0\n",
      "Epoch: 0050 cost = 0.676489115 trainacc: 55.2 testacc: 50.0\n",
      "Epoch: 0051 cost = 0.676248252 trainacc: 55.2 testacc: 50.0\n",
      "Epoch: 0052 cost = 0.675923467 trainacc: 56.0 testacc: 50.0\n",
      "Epoch: 0053 cost = 0.675517738 trainacc: 56.0 testacc: 50.0\n",
      "Epoch: 0054 cost = 0.674846888 trainacc: 56.0 testacc: 50.0\n",
      "Epoch: 0055 cost = 0.674527407 trainacc: 56.0 testacc: 50.0\n",
      "Epoch: 0056 cost = 0.674329937 trainacc: 56.0 testacc: 50.0\n",
      "Epoch: 0057 cost = 0.673807144 trainacc: 56.8 testacc: 50.0\n",
      "Epoch: 0058 cost = 0.673590899 trainacc: 57.6 testacc: 50.0\n",
      "Epoch: 0059 cost = 0.673091590 trainacc: 57.6 testacc: 50.0\n",
      "Epoch: 0060 cost = 0.672656775 trainacc: 57.6 testacc: 50.0\n",
      "Epoch: 0061 cost = 0.672064066 trainacc: 57.6 testacc: 50.0\n",
      "Epoch: 0062 cost = 0.671623230 trainacc: 57.6 testacc: 50.0\n",
      "Epoch: 0063 cost = 0.671162486 trainacc: 57.6 testacc: 50.0\n",
      "Epoch: 0064 cost = 0.670594692 trainacc: 58.4 testacc: 50.0\n",
      "Epoch: 0065 cost = 0.670361280 trainacc: 58.4 testacc: 50.0\n",
      "Epoch: 0066 cost = 0.669871807 trainacc: 58.4 testacc: 50.0\n",
      "Epoch: 0067 cost = 0.669595242 trainacc: 58.4 testacc: 50.0\n",
      "Epoch: 0068 cost = 0.669216514 trainacc: 59.2 testacc: 50.0\n",
      "Epoch: 0069 cost = 0.668568492 trainacc: 60.8 testacc: 50.0\n",
      "Epoch: 0070 cost = 0.667765915 trainacc: 60.8 testacc: 50.0\n",
      "Epoch: 0071 cost = 0.667596817 trainacc: 60.8 testacc: 50.0\n",
      "Epoch: 0072 cost = 0.667075396 trainacc: 60.8 testacc: 50.0\n",
      "Epoch: 0073 cost = 0.666654170 trainacc: 61.6 testacc: 50.0\n",
      "Epoch: 0074 cost = 0.666253567 trainacc: 61.6 testacc: 50.0\n",
      "Epoch: 0075 cost = 0.665694535 trainacc: 61.6 testacc: 50.0\n",
      "Epoch: 0076 cost = 0.665187716 trainacc: 61.6 testacc: 50.0\n",
      "Epoch: 0077 cost = 0.664576113 trainacc: 62.4 testacc: 50.0\n",
      "Epoch: 0078 cost = 0.664357185 trainacc: 64.8 testacc: 50.0\n",
      "Epoch: 0079 cost = 0.663467765 trainacc: 64.8 testacc: 50.0\n",
      "Epoch: 0080 cost = 0.662688851 trainacc: 65.6 testacc: 50.0\n",
      "Epoch: 0081 cost = 0.662252605 trainacc: 65.6 testacc: 50.0\n",
      "Epoch: 0082 cost = 0.661525965 trainacc: 65.6 testacc: 50.0\n",
      "Epoch: 0083 cost = 0.661571801 trainacc: 66.4 testacc: 50.0\n",
      "Epoch: 0084 cost = 0.660479367 trainacc: 67.2 testacc: 50.0\n",
      "Epoch: 0085 cost = 0.660056353 trainacc: 67.2 testacc: 50.0\n",
      "Epoch: 0086 cost = 0.659055531 trainacc: 68.8 testacc: 50.0\n",
      "Epoch: 0087 cost = 0.658441126 trainacc: 68.8 testacc: 50.0\n",
      "Epoch: 0088 cost = 0.658091128 trainacc: 68.8 testacc: 50.0\n",
      "Epoch: 0089 cost = 0.657121360 trainacc: 68.8 testacc: 50.0\n",
      "Epoch: 0090 cost = 0.656674087 trainacc: 69.6 testacc: 50.0\n",
      "Epoch: 0091 cost = 0.656226933 trainacc: 71.2 testacc: 50.0\n",
      "Epoch: 0092 cost = 0.655467093 trainacc: 71.2 testacc: 50.0\n",
      "Epoch: 0093 cost = 0.654773831 trainacc: 71.2 testacc: 50.0\n",
      "Epoch: 0094 cost = 0.654564798 trainacc: 72.8 testacc: 50.0\n",
      "Epoch: 0095 cost = 0.653245151 trainacc: 75.2 testacc: 50.0\n",
      "Epoch: 0096 cost = 0.652452350 trainacc: 76.0 testacc: 50.0\n",
      "Epoch: 0097 cost = 0.651912808 trainacc: 78.4 testacc: 50.0\n",
      "Epoch: 0098 cost = 0.651017129 trainacc: 77.6 testacc: 50.0\n",
      "Epoch: 0099 cost = 0.650647759 trainacc: 79.2 testacc: 50.0\n",
      "Epoch: 0100 cost = 0.649963975 trainacc: 79.2 testacc: 50.0\n",
      "Epoch: 0101 cost = 0.649064958 trainacc: 79.2 testacc: 50.0\n",
      "Epoch: 0102 cost = 0.648023009 trainacc: 79.2 testacc: 50.0\n",
      "Epoch: 0103 cost = 0.647661269 trainacc: 80.8 testacc: 50.0\n",
      "Epoch: 0104 cost = 0.646249115 trainacc: 80.0 testacc: 50.0\n",
      "Epoch: 0105 cost = 0.645970345 trainacc: 80.0 testacc: 50.0\n",
      "Epoch: 0106 cost = 0.645210743 trainacc: 81.6 testacc: 50.0\n",
      "Epoch: 0107 cost = 0.644768953 trainacc: 82.4 testacc: 50.0\n",
      "Epoch: 0108 cost = 0.644027829 trainacc: 82.4 testacc: 50.0\n",
      "Epoch: 0109 cost = 0.642564356 trainacc: 82.4 testacc: 50.0\n",
      "Epoch: 0110 cost = 0.640894055 trainacc: 82.4 testacc: 50.0\n",
      "Epoch: 0111 cost = 0.641196847 trainacc: 83.2 testacc: 50.0\n",
      "Epoch: 0112 cost = 0.640138388 trainacc: 82.4 testacc: 50.0\n",
      "Epoch: 0113 cost = 0.638976574 trainacc: 82.4 testacc: 53.125\n",
      "Epoch: 0114 cost = 0.637557864 trainacc: 83.2 testacc: 53.125\n",
      "Epoch: 0115 cost = 0.636991203 trainacc: 83.2 testacc: 53.125\n",
      "Epoch: 0116 cost = 0.635673404 trainacc: 84.0 testacc: 53.125\n",
      "Epoch: 0117 cost = 0.635369837 trainacc: 86.4 testacc: 56.25\n",
      "Epoch: 0118 cost = 0.634082258 trainacc: 86.4 testacc: 56.25\n",
      "Epoch: 0119 cost = 0.633173823 trainacc: 86.4 testacc: 56.25\n",
      "Epoch: 0120 cost = 0.632649660 trainacc: 88.8 testacc: 56.25\n",
      "Epoch: 0121 cost = 0.631209135 trainacc: 88.8 testacc: 56.25\n",
      "Epoch: 0122 cost = 0.629752874 trainacc: 88.8 testacc: 56.25\n",
      "Epoch: 0123 cost = 0.628701985 trainacc: 88.8 testacc: 56.25\n",
      "Epoch: 0124 cost = 0.627666950 trainacc: 89.6 testacc: 59.375\n",
      "Epoch: 0125 cost = 0.626344502 trainacc: 89.6 testacc: 59.375\n",
      "Epoch: 0126 cost = 0.625246167 trainacc: 91.2 testacc: 53.125\n",
      "Epoch: 0127 cost = 0.624147713 trainacc: 92.0 testacc: 53.125\n",
      "Epoch: 0128 cost = 0.623536587 trainacc: 92.0 testacc: 53.125\n",
      "Epoch: 0129 cost = 0.622437596 trainacc: 92.0 testacc: 53.125\n",
      "Epoch: 0130 cost = 0.620699346 trainacc: 92.0 testacc: 53.125\n",
      "Epoch: 0131 cost = 0.620437026 trainacc: 92.0 testacc: 53.125\n",
      "Epoch: 0132 cost = 0.618066311 trainacc: 92.8 testacc: 53.125\n",
      "Epoch: 0133 cost = 0.616756976 trainacc: 92.0 testacc: 53.125\n",
      "Epoch: 0134 cost = 0.615372777 trainacc: 93.6 testacc: 53.125\n",
      "Epoch: 0135 cost = 0.614619493 trainacc: 94.4 testacc: 53.125\n",
      "Epoch: 0136 cost = 0.612826347 trainacc: 94.4 testacc: 53.125\n",
      "Epoch: 0137 cost = 0.611320317 trainacc: 94.4 testacc: 56.25\n",
      "Epoch: 0138 cost = 0.609934151 trainacc: 94.4 testacc: 56.25\n",
      "Epoch: 0139 cost = 0.608568847 trainacc: 94.4 testacc: 56.25\n",
      "Epoch: 0140 cost = 0.607637644 trainacc: 94.4 testacc: 56.25\n",
      "Epoch: 0141 cost = 0.605231762 trainacc: 94.4 testacc: 53.125\n",
      "Epoch: 0142 cost = 0.603776395 trainacc: 94.4 testacc: 56.25\n",
      "Epoch: 0143 cost = 0.601433754 trainacc: 94.4 testacc: 56.25\n",
      "Epoch: 0144 cost = 0.600865960 trainacc: 94.4 testacc: 56.25\n",
      "Epoch: 0145 cost = 0.598974526 trainacc: 94.4 testacc: 56.25\n",
      "Epoch: 0146 cost = 0.597688913 trainacc: 94.4 testacc: 53.125\n",
      "Epoch: 0147 cost = 0.595374584 trainacc: 94.4 testacc: 56.25\n",
      "Epoch: 0148 cost = 0.594196498 trainacc: 94.4 testacc: 53.125\n",
      "Epoch: 0149 cost = 0.592576206 trainacc: 95.2 testacc: 50.0\n",
      "Epoch: 0150 cost = 0.589592040 trainacc: 94.4 testacc: 53.125\n",
      "Epoch: 0151 cost = 0.587869108 trainacc: 94.4 testacc: 53.125\n",
      "Epoch: 0152 cost = 0.585699201 trainacc: 94.4 testacc: 53.125\n",
      "Epoch: 0153 cost = 0.584047198 trainacc: 94.4 testacc: 53.125\n",
      "Epoch: 0154 cost = 0.582202911 trainacc: 94.4 testacc: 56.25\n",
      "Epoch: 0155 cost = 0.580500484 trainacc: 95.2 testacc: 46.875\n",
      "Epoch: 0156 cost = 0.577862918 trainacc: 95.2 testacc: 50.0\n",
      "Epoch: 0157 cost = 0.576045334 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0158 cost = 0.573476672 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0159 cost = 0.571448863 trainacc: 94.4 testacc: 50.0\n",
      "Epoch: 0160 cost = 0.569117606 trainacc: 95.2 testacc: 50.0\n",
      "Epoch: 0161 cost = 0.567121148 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0162 cost = 0.565323353 trainacc: 96.0 testacc: 43.75\n",
      "Epoch: 0163 cost = 0.561529458 trainacc: 96.0 testacc: 43.75\n",
      "Epoch: 0164 cost = 0.560020983 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0165 cost = 0.557882965 trainacc: 96.0 testacc: 43.75\n",
      "Epoch: 0166 cost = 0.554626524 trainacc: 96.0 testacc: 43.75\n",
      "Epoch: 0167 cost = 0.552087903 trainacc: 96.0 testacc: 50.0\n",
      "Epoch: 0168 cost = 0.549201190 trainacc: 96.0 testacc: 43.75\n",
      "Epoch: 0169 cost = 0.546958923 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0170 cost = 0.544314146 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0171 cost = 0.540818751 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0172 cost = 0.537198544 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0173 cost = 0.534530818 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0174 cost = 0.532116950 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0175 cost = 0.529158533 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0176 cost = 0.527284503 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0177 cost = 0.523466110 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0178 cost = 0.519128144 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0179 cost = 0.516479909 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0180 cost = 0.512042105 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0181 cost = 0.508952737 trainacc: 96.0 testacc: 50.0\n",
      "Epoch: 0182 cost = 0.506104171 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0183 cost = 0.501872718 trainacc: 96.0 testacc: 46.875\n",
      "Epoch: 0184 cost = 0.498344570 trainacc: 96.0 testacc: 50.0\n",
      "Epoch: 0185 cost = 0.493946195 trainacc: 96.0 testacc: 50.0\n",
      "Epoch: 0186 cost = 0.489609897 trainacc: 96.0 testacc: 53.125\n",
      "Epoch: 0187 cost = 0.486142039 trainacc: 96.0 testacc: 53.125\n",
      "Epoch: 0188 cost = 0.482764870 trainacc: 96.0 testacc: 50.0\n",
      "Epoch: 0189 cost = 0.477707773 trainacc: 96.0 testacc: 53.125\n",
      "Epoch: 0190 cost = 0.473979086 trainacc: 96.0 testacc: 53.125\n",
      "Epoch: 0191 cost = 0.470924735 trainacc: 96.0 testacc: 53.125\n",
      "Epoch: 0192 cost = 0.468312174 trainacc: 96.0 testacc: 50.0\n",
      "Epoch: 0193 cost = 0.462127924 trainacc: 96.0 testacc: 53.125\n",
      "Epoch: 0194 cost = 0.455899298 trainacc: 96.0 testacc: 53.125\n",
      "Epoch: 0195 cost = 0.453509718 trainacc: 96.0 testacc: 56.25\n",
      "Epoch: 0196 cost = 0.448521852 trainacc: 96.0 testacc: 56.25\n",
      "Epoch: 0197 cost = 0.444020540 trainacc: 97.6 testacc: 56.25\n",
      "Epoch: 0198 cost = 0.439687490 trainacc: 96.0 testacc: 53.125\n",
      "Epoch: 0199 cost = 0.435093433 trainacc: 97.6 testacc: 56.25\n",
      "Epoch: 0200 cost = 0.428653896 trainacc: 97.6 testacc: 56.25\n",
      "Epoch: 0201 cost = 0.424804896 trainacc: 97.6 testacc: 56.25\n",
      "Epoch: 0202 cost = 0.420332223 trainacc: 97.6 testacc: 56.25\n",
      "Epoch: 0203 cost = 0.414856136 trainacc: 97.6 testacc: 56.25\n",
      "Epoch: 0204 cost = 0.409816653 trainacc: 97.6 testacc: 56.25\n",
      "Epoch: 0205 cost = 0.403596997 trainacc: 97.6 testacc: 56.25\n",
      "Epoch: 0206 cost = 0.401036352 trainacc: 97.6 testacc: 59.375\n",
      "Epoch: 0207 cost = 0.395792276 trainacc: 98.4 testacc: 62.5\n",
      "Epoch: 0208 cost = 0.390182823 trainacc: 98.4 testacc: 59.375\n",
      "Epoch: 0209 cost = 0.382884860 trainacc: 97.6 testacc: 59.375\n",
      "Epoch: 0210 cost = 0.379251927 trainacc: 98.4 testacc: 59.375\n",
      "Epoch: 0211 cost = 0.372712404 trainacc: 98.4 testacc: 62.5\n",
      "Epoch: 0212 cost = 0.367050827 trainacc: 98.4 testacc: 59.375\n",
      "Epoch: 0213 cost = 0.361654967 trainacc: 98.4 testacc: 59.375\n",
      "Epoch: 0214 cost = 0.355977774 trainacc: 98.4 testacc: 62.5\n",
      "Epoch: 0215 cost = 0.349999189 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0216 cost = 0.344933867 trainacc: 98.4 testacc: 59.375\n",
      "Epoch: 0217 cost = 0.340578973 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0218 cost = 0.333919019 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0219 cost = 0.329079777 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0220 cost = 0.323418349 trainacc: 98.4 testacc: 65.625\n",
      "Epoch: 0221 cost = 0.317161202 trainacc: 98.4 testacc: 65.625\n",
      "Epoch: 0222 cost = 0.311920673 trainacc: 98.4 testacc: 65.625\n",
      "Epoch: 0223 cost = 0.306817085 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0224 cost = 0.299493849 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0225 cost = 0.295559764 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0226 cost = 0.290809035 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0227 cost = 0.284386396 trainacc: 98.4 testacc: 65.625\n",
      "Epoch: 0228 cost = 0.278642356 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0229 cost = 0.274484336 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0230 cost = 0.266802460 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0231 cost = 0.264136791 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0232 cost = 0.257340729 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0233 cost = 0.251560658 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0234 cost = 0.246831253 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0235 cost = 0.242390022 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0236 cost = 0.234119520 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0237 cost = 0.229699373 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0238 cost = 0.225428522 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0239 cost = 0.219211102 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0240 cost = 0.215526417 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0241 cost = 0.210781515 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0242 cost = 0.205422387 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0243 cost = 0.199972078 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0244 cost = 0.195813000 trainacc: 99.2 testacc: 68.75\n",
      "Epoch: 0245 cost = 0.190919518 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0246 cost = 0.187022045 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0247 cost = 0.185257882 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0248 cost = 0.177792057 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0249 cost = 0.174124613 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0250 cost = 0.169498891 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0251 cost = 0.166338697 trainacc: 99.2 testacc: 65.625\n",
      "Epoch: 0252 cost = 0.162629843 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0253 cost = 0.157428101 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0254 cost = 0.152949199 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0255 cost = 0.147666886 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0256 cost = 0.145916045 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0257 cost = 0.142486662 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0258 cost = 0.138500199 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0259 cost = 0.133654922 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0260 cost = 0.130902544 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0261 cost = 0.130210564 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0262 cost = 0.123942375 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0263 cost = 0.123058826 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0264 cost = 0.120247386 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0265 cost = 0.115002893 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0266 cost = 0.113691106 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0267 cost = 0.110836506 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0268 cost = 0.108014099 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0269 cost = 0.104519390 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0270 cost = 0.101680622 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0271 cost = 0.100133121 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0272 cost = 0.095268779 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0273 cost = 0.095998637 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0274 cost = 0.093323186 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0275 cost = 0.092219293 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0276 cost = 0.088494241 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0277 cost = 0.085749328 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0278 cost = 0.084167205 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0279 cost = 0.082214057 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0280 cost = 0.080638021 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0281 cost = 0.078815170 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0282 cost = 0.077073917 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0283 cost = 0.074536785 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0284 cost = 0.072280727 trainacc: 100.0 testacc: 65.625\n",
      "Epoch: 0285 cost = 0.069755338 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0286 cost = 0.068928413 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0287 cost = 0.067076527 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0288 cost = 0.065191284 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0289 cost = 0.065065920 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0290 cost = 0.062883087 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0291 cost = 0.060849845 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0292 cost = 0.059569728 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0293 cost = 0.057841964 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0294 cost = 0.057683088 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0295 cost = 0.056199979 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0296 cost = 0.054800518 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0297 cost = 0.053948104 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0298 cost = 0.053262845 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0299 cost = 0.051320396 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0300 cost = 0.049957685 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0301 cost = 0.049610950 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0302 cost = 0.048230119 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0303 cost = 0.047513288 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0304 cost = 0.046208389 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0305 cost = 0.045245577 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0306 cost = 0.044551447 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0307 cost = 0.043368727 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0308 cost = 0.041871861 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0309 cost = 0.041693300 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0310 cost = 0.041341171 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0311 cost = 0.040458456 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0312 cost = 0.038929138 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0313 cost = 0.039177570 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0314 cost = 0.038016122 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0315 cost = 0.037160706 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0316 cost = 0.036663599 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0317 cost = 0.035675883 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0318 cost = 0.035211250 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0319 cost = 0.033811256 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0320 cost = 0.034077987 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0321 cost = 0.032568656 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0322 cost = 0.032857616 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0323 cost = 0.032838207 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0324 cost = 0.031269848 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0325 cost = 0.031281799 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0326 cost = 0.030834202 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0327 cost = 0.029142000 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0328 cost = 0.029301360 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0329 cost = 0.029087422 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0330 cost = 0.028530020 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0331 cost = 0.028173901 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0332 cost = 0.028479664 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0333 cost = 0.027129777 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0334 cost = 0.026186727 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0335 cost = 0.025595067 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0336 cost = 0.025905769 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0337 cost = 0.025451723 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0338 cost = 0.024971647 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0339 cost = 0.023917554 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0340 cost = 0.023218082 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0341 cost = 0.022732386 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0342 cost = 0.023547074 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0343 cost = 0.022943435 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0344 cost = 0.022722755 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0345 cost = 0.022228271 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0346 cost = 0.021359811 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0347 cost = 0.021669663 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0348 cost = 0.021068268 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0349 cost = 0.021460708 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0350 cost = 0.020721968 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0351 cost = 0.020610174 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0352 cost = 0.020379091 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0353 cost = 0.019522777 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0354 cost = 0.019033769 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0355 cost = 0.018463822 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0356 cost = 0.019320853 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0357 cost = 0.018587606 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0358 cost = 0.018578142 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0359 cost = 0.018392814 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0360 cost = 0.017518505 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0361 cost = 0.017685870 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0362 cost = 0.017379077 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0363 cost = 0.017331932 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0364 cost = 0.017302336 trainacc: 100.0 testacc: 68.75\n",
      "Epoch: 0365 cost = 0.016990257 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0366 cost = 0.016840655 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0367 cost = 0.016186485 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0368 cost = 0.016198248 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0369 cost = 0.015923794 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0370 cost = 0.015468537 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0371 cost = 0.015211822 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0372 cost = 0.015264951 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0373 cost = 0.015342437 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0374 cost = 0.014942272 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0375 cost = 0.014943957 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0376 cost = 0.014732317 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0377 cost = 0.014634239 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0378 cost = 0.014439777 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0379 cost = 0.014155855 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0380 cost = 0.014203877 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0381 cost = 0.013672559 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0382 cost = 0.013760582 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0383 cost = 0.013607226 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0384 cost = 0.013633071 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0385 cost = 0.013258125 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0386 cost = 0.012656803 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0387 cost = 0.012876898 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0388 cost = 0.012844926 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0389 cost = 0.012823677 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0390 cost = 0.012369674 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0391 cost = 0.012551531 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0392 cost = 0.012256165 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0393 cost = 0.012092961 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0394 cost = 0.012200136 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0395 cost = 0.011960679 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0396 cost = 0.011818081 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0397 cost = 0.011659401 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0398 cost = 0.011686722 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0399 cost = 0.011337023 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0400 cost = 0.011384178 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0401 cost = 0.011230547 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0402 cost = 0.010921057 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0403 cost = 0.011022282 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0404 cost = 0.011125140 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0405 cost = 0.010735659 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0406 cost = 0.010367603 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0407 cost = 0.010566492 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0408 cost = 0.010361532 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0409 cost = 0.010115377 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0410 cost = 0.010043997 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0411 cost = 0.010037402 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0412 cost = 0.009977305 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0413 cost = 0.009930693 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0414 cost = 0.009913810 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0415 cost = 0.009819174 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0416 cost = 0.009554196 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0417 cost = 0.009650187 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0418 cost = 0.009173252 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0419 cost = 0.009290591 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0420 cost = 0.009059277 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0421 cost = 0.009233929 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0422 cost = 0.009360893 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0423 cost = 0.008964433 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0424 cost = 0.008981995 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0425 cost = 0.008949564 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0426 cost = 0.008757950 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0427 cost = 0.008898057 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0428 cost = 0.008403630 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0429 cost = 0.008442816 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0430 cost = 0.008576652 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0431 cost = 0.008511239 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0432 cost = 0.008497192 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0433 cost = 0.008344998 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0434 cost = 0.008300230 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0435 cost = 0.008235079 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0436 cost = 0.008077441 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0437 cost = 0.008008250 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0438 cost = 0.008054192 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0439 cost = 0.008175425 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0440 cost = 0.007806103 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0441 cost = 0.007854296 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0442 cost = 0.007513737 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0443 cost = 0.007600741 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0444 cost = 0.007610564 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0445 cost = 0.007599941 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0446 cost = 0.007473852 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0447 cost = 0.007262851 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0448 cost = 0.007355408 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0449 cost = 0.007236701 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0450 cost = 0.007376337 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0451 cost = 0.007126033 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0452 cost = 0.007249195 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0453 cost = 0.006968775 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0454 cost = 0.007097849 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0455 cost = 0.006950757 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0456 cost = 0.006921759 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0457 cost = 0.006614732 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0458 cost = 0.006924477 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0459 cost = 0.006748487 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0460 cost = 0.006778427 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0461 cost = 0.006529816 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0462 cost = 0.006704308 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0463 cost = 0.006587807 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0464 cost = 0.006545243 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0465 cost = 0.006444647 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0466 cost = 0.006591295 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0467 cost = 0.006383556 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0468 cost = 0.006202741 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0469 cost = 0.006399659 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0470 cost = 0.006121173 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0471 cost = 0.006205105 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0472 cost = 0.006429783 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0473 cost = 0.006132440 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0474 cost = 0.006222103 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0475 cost = 0.005969400 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0476 cost = 0.005885478 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0477 cost = 0.006199329 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0478 cost = 0.006029162 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0479 cost = 0.005830118 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0480 cost = 0.005922795 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0481 cost = 0.005990522 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0482 cost = 0.005702002 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0483 cost = 0.005788481 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0484 cost = 0.005741898 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0485 cost = 0.005727667 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0486 cost = 0.005553836 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0487 cost = 0.005730025 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0488 cost = 0.005571132 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0489 cost = 0.005632288 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0490 cost = 0.005429736 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0491 cost = 0.005519718 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0492 cost = 0.005469139 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0493 cost = 0.005433522 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0494 cost = 0.005302074 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0495 cost = 0.005433921 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0496 cost = 0.005294489 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0497 cost = 0.005330339 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0498 cost = 0.005344176 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0499 cost = 0.005094117 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0500 cost = 0.005195645 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0501 cost = 0.005191147 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0502 cost = 0.005224623 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0503 cost = 0.005066196 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0504 cost = 0.005064862 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0505 cost = 0.005071956 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0506 cost = 0.004828193 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0507 cost = 0.004969927 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0508 cost = 0.004789895 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0509 cost = 0.004992840 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0510 cost = 0.004920444 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0511 cost = 0.004883245 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0512 cost = 0.004852355 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0513 cost = 0.004753463 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0514 cost = 0.004661836 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0515 cost = 0.004763634 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0516 cost = 0.004833299 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0517 cost = 0.004488666 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0518 cost = 0.004726320 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0519 cost = 0.004606470 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0520 cost = 0.004667313 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0521 cost = 0.004657039 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0522 cost = 0.004520604 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0523 cost = 0.004643079 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0524 cost = 0.004552352 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0525 cost = 0.004513200 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0526 cost = 0.004343269 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0527 cost = 0.004396583 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0528 cost = 0.004477351 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0529 cost = 0.004314084 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0530 cost = 0.004430389 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0531 cost = 0.004221952 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0532 cost = 0.004288581 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0533 cost = 0.004361537 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0534 cost = 0.004255827 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0535 cost = 0.004359839 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0536 cost = 0.004251256 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0537 cost = 0.004259877 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0538 cost = 0.004013826 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0539 cost = 0.004130415 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0540 cost = 0.004004166 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0541 cost = 0.004199080 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0542 cost = 0.004012021 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0543 cost = 0.004057562 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0544 cost = 0.004064078 trainacc: 100.0 testacc: 71.875\n",
      "Epoch: 0545 cost = 0.004180044 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0546 cost = 0.004030173 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0547 cost = 0.004060709 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0548 cost = 0.004024811 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0549 cost = 0.003908203 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0550 cost = 0.003962477 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0551 cost = 0.003955300 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0552 cost = 0.003881286 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0553 cost = 0.003887252 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0554 cost = 0.003658895 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0555 cost = 0.003952182 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0556 cost = 0.003845170 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0557 cost = 0.003799017 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0558 cost = 0.003789047 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0559 cost = 0.003812135 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0560 cost = 0.003740492 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0561 cost = 0.003666478 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0562 cost = 0.003750079 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0563 cost = 0.003714004 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0564 cost = 0.003668292 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0565 cost = 0.003512664 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0566 cost = 0.003674086 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0567 cost = 0.003678853 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0568 cost = 0.003566116 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0569 cost = 0.003505562 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0570 cost = 0.003602602 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0571 cost = 0.003592485 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0572 cost = 0.003494680 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0573 cost = 0.003504995 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0574 cost = 0.003468747 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0575 cost = 0.003550794 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0576 cost = 0.003422939 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0577 cost = 0.003558667 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0578 cost = 0.003407154 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0579 cost = 0.003492507 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0580 cost = 0.003363878 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0581 cost = 0.003416846 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0582 cost = 0.003348095 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0583 cost = 0.003370234 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0584 cost = 0.003323076 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0585 cost = 0.003284290 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0586 cost = 0.003253098 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0587 cost = 0.003311549 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0588 cost = 0.003283849 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0589 cost = 0.003359331 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0590 cost = 0.003331107 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0591 cost = 0.003295086 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0592 cost = 0.003211549 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0593 cost = 0.003130417 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0594 cost = 0.003193890 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0595 cost = 0.003191154 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0596 cost = 0.003148020 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0597 cost = 0.003240021 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0598 cost = 0.003088891 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0599 cost = 0.003079027 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0600 cost = 0.003171517 trainacc: 100.0 testacc: 75.0\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader_train)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    avg_cost = 0\n",
    "\n",
    "    # for X, Y in data_loader_train:\n",
    "    #     X = torch.autograd.Variable(X).to(device).float()\n",
    "    #     Y = Y.to(device)\n",
    "\n",
    "    #     optimizer.zero_grad()\n",
    "    #     hypothesis = model(X)\n",
    "    #     cost = criterion(hypothesis, Y)\n",
    "    #     cost.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    #     avg_cost += cost / total_batch\n",
    "    # model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader_train):\n",
    "        inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        avg_cost += loss / total_batch\n",
    "\n",
    "    \n",
    "    # if avg_cost < 0.1:\n",
    "    #     break\n",
    "    model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for X_test, Y_test in data_loader_test:\n",
    "    #         X_test = X_test.to(device).float()\n",
    "    #         Y_test = Y_test.to(device)\n",
    "\n",
    "    #     prediction = model(X_test)\n",
    "    #     correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    #     accuracy = correct_prediction.float().mean()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader_train):\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    trainacc = 100.*correct/total\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader_test):\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    testacc = 100.*correct/total\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost), 'trainacc:', trainacc, 'testacc:', testacc)\n",
    "\n",
    "print('Learning finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.3377e+00,  2.9925e+00],\n",
      "        [-2.8413e+00,  2.5115e+00],\n",
      "        [-3.2303e+00,  2.7332e+00],\n",
      "        [-5.4596e-04, -2.9934e-01],\n",
      "        [-6.4422e-01,  4.5668e-01],\n",
      "        [-6.4423e-01,  3.0870e-01],\n",
      "        [-7.5181e-01,  4.2383e-01],\n",
      "        [-3.3621e+00,  2.9561e+00],\n",
      "        [-3.2106e+00,  2.8649e+00],\n",
      "        [-1.7504e+00,  1.3910e+00],\n",
      "        [ 9.1613e-01, -9.9515e-01],\n",
      "        [ 2.7600e+00, -2.8394e+00],\n",
      "        [-8.8490e-01,  6.3188e-01],\n",
      "        [-4.9878e-01, -9.9034e-03],\n",
      "        [-2.6865e+00,  2.3277e+00]], device='cuda:0') tensor([ True,  True, False, False,  True,  True, False,  True, False,  True,\n",
      "         True,  True, False,  True,  True], device='cuda:0') tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1], device='cuda:0')\n",
      "Accuracy: 0.6666666865348816\n",
      "tensor([[-0.9546,  0.5854],\n",
      "        [ 2.0866, -2.0870],\n",
      "        [ 0.6185, -0.8793],\n",
      "        [ 3.0498, -3.0567],\n",
      "        [ 2.2323, -2.3653],\n",
      "        [-3.0995,  2.7563],\n",
      "        [-1.9152,  1.5265],\n",
      "        [-1.6749,  1.4595],\n",
      "        [ 0.6655, -0.8236],\n",
      "        [-2.1163,  1.8338],\n",
      "        [-0.2009, -0.0622],\n",
      "        [-1.6286,  1.2077],\n",
      "        [-1.2519,  0.9542],\n",
      "        [ 3.0472, -3.1138],\n",
      "        [ 1.1999, -1.2209]], device='cuda:0') tensor([ True,  True, False,  True,  True,  True, False,  True,  True,  True,\n",
      "         True,  True, False,  True,  True], device='cuda:0') tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.8000000715255737\n",
      "tensor([[ 1.0277, -1.2302],\n",
      "        [ 1.5713, -1.6425]], device='cuda:0') tensor([True, True], device='cuda:0') tensor([0, 0], device='cuda:0')\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "# model.load_state_dict(torch.load(\".\\merge_save\\merge_attack0normal10_05-07--21-10-32.pth\")[\"state_dict\"])\n",
    "# model.load_state_dict(torch.load(r\".\\myfed_normal_save\\model18-24-02.pth\")[\"state_dict\"])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_test, Y_test in data_loader_test:\n",
    "        X_test = X_test.to(device).float()\n",
    "        Y_test = Y_test.to(device)\n",
    "\n",
    "        prediction = model(X_test)\n",
    "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        print(prediction,correct_prediction, Y_test)\n",
    "        accuracy = correct_prediction.float().mean()\n",
    "        print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # Get one and predict\n",
    "    # r = random.randint(0, len(mnist_test) - 1)\n",
    "    # X_single_data = mnist_test.data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    # Y_single_data = mnist_test.targets[r:r + 1].to(device)\n",
    "\n",
    "    # print('Label: ', Y_single_data.item())\n",
    "    # single_prediction = model(X_single_data)\n",
    "    # print('Prediction: ', torch.argmax(single_prediction, 1).item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d28dcb1746f058774bc7e3103f8fd674f9a46f28636659ba6a6fdaf0b17e1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
