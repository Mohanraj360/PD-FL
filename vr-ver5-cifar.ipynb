{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 10 MNIST and softmax\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import datetime\n",
    "\n",
    "# torch.set_printoptions(profile=\"full\")\n",
    "# np.set_printoptions(threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(111)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomGrayscale(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform1 = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "     \n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='CIFAR10_data/', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='CIFAR10_data/', train=False,\n",
    "                                       download=True, transform=transform1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=50,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 5\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,64,3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(64,64,3,padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64,128,3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3,padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128,128, 3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 128, 3,padding=1)\n",
    "        self.conv7 = nn.Conv2d(128, 128, 1,padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv8 = nn.Conv2d(128, 256, 3,padding=1)\n",
    "        self.conv9 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(256, 256, 1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv11 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv13 = nn.Conv2d(512, 512, 1, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc14 = nn.Linear(512*4*4,1024)\n",
    "        self.drop1 = nn.Dropout2d()\n",
    "        self.fc15 = nn.Linear(1024,128)\n",
    "        self.drop2 = nn.Dropout2d()\n",
    "        self.fc16 = nn.Linear(128,10)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.conv11(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.pool5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu5(x)\n",
    "        # print(\" x shape \",x.size())\n",
    "        x = x.view(-1,512*4*4)\n",
    "        x = F.relu(self.fc14(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc15(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc16(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "model = model = torch.nn.DataParallel(MyNet().to(device))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['merge_cifartest_attack0normal15_06-05--21-30-03.pth', 'merge_cifartest_attack0normal15_06-05--22-12-15.pth', 'merge_cifartest_attack0normal15_06-05--22-12-21.pth', 'merge_cifartest_attack0normal15_06-05--22-12-25.pth', 'merge_cifartest_attack0normal15_06-05--22-13-02.pth', 'merge_cifartest_attack0normal15_06-05--22-13-11.pth', 'merge_cifartest_attack0normal15_06-05--22-13-29.pth', 'merge_cifartest_attack0normal15_06-05--22-13-32.pth', 'merge_cifartest_attack0normal15_06-05--22-13-50.pth', 'merge_cifartest_attack0normal15_06-05--22-13-54.pth', 'merge_cifartest_attack0normal15_06-05--22-13-58.pth', 'merge_cifartest_attack0normal15_06-05--22-14-02.pth', 'merge_cifartest_attack0normal15_06-05--22-14-07.pth', 'merge_cifartest_attack0normal15_06-05--22-14-10.pth', 'merge_cifartest_attack0normal15_06-05--22-14-14.pth', 'merge_cifartest_attack0normal15_06-05--22-14-18.pth', 'merge_cifartest_attack0normal15_06-05--22-14-42.pth', 'merge_cifartest_attack0normal15_06-05--22-14-45.pth', 'merge_cifartest_attack0normal15_06-05--22-14-49.pth', 'merge_cifartest_attack0normal15_06-05--22-14-52.pth', 'merge_cifartest_attack0normal15_06-05--22-14-55.pth', 'merge_cifartest_attack0normal15_06-05--22-14-58.pth', 'merge_cifartest_attack0normal15_06-05--22-15-02.pth', 'merge_cifartest_attack1normal14_06-05--21-23-14.pth', 'merge_cifartest_attack1normal14_06-05--21-25-10.pth', 'merge_cifartest_attack1normal14_06-05--21-25-36.pth', 'merge_cifartest_attack1normal14_06-05--21-26-28.pth', 'merge_cifartest_attack1normal14_06-05--21-26-32.pth', 'merge_cifartest_attack1normal14_06-05--21-26-41.pth', 'merge_cifartest_attack1normal14_06-05--21-27-54.pth', 'merge_cifartest_attack2normal13_06-05--21-22-55.pth', 'merge_cifartest_attack2normal13_06-05--21-23-05.pth', 'merge_cifartest_attack2normal13_06-05--21-23-18.pth', 'merge_cifartest_attack2normal13_06-05--21-24-34.pth', 'merge_cifartest_attack2normal13_06-05--21-25-07.pth', 'merge_cifartest_attack2normal13_06-05--21-25-16.pth', 'merge_cifartest_attack2normal13_06-05--21-25-40.pth', 'merge_cifartest_attack2normal13_06-05--21-26-20.pth', 'merge_cifartest_attack2normal13_06-05--21-26-37.pth', 'merge_cifartest_attack2normal13_06-05--21-27-51.pth', 'merge_cifartest_attack2normal13_06-05--21-28-02.pth', 'merge_cifartest_attack2normal13_06-05--21-28-22.pth', 'merge_cifartest_attack2normal13_06-05--21-29-59.pth', 'merge_cifartest_attack2normal13_06-05--21-30-13.pth', 'merge_cifartest_attack3normal12_06-05--21-23-22.pth', 'merge_cifartest_attack3normal12_06-05--21-23-30.pth', 'merge_cifartest_attack3normal12_06-05--21-25-02.pth', 'merge_cifartest_attack3normal12_06-05--21-25-20.pth', 'merge_cifartest_attack3normal12_06-05--21-25-24.pth', 'merge_cifartest_attack3normal12_06-05--21-25-32.pth', 'merge_cifartest_attack3normal12_06-05--21-26-17.pth', 'merge_cifartest_attack3normal12_06-05--21-27-40.pth', 'merge_cifartest_attack3normal12_06-05--21-28-09.pth', 'merge_cifartest_attack3normal12_06-05--21-28-26.pth', 'merge_cifartest_attack3normal12_06-05--21-29-55.pth', 'merge_cifartest_attack3normal12_06-05--21-30-09.pth']\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(\"./myfed_normal_save/model18-32-05.pth\")[\"state_dict\"])\n",
    "dir = os.listdir(\"./merge_save\")\n",
    "modelnames = []\n",
    "for mt in dir:\n",
    "    if(mt[0:len(\"merge_cifartest_attack\")] == \"merge_cifartest_attack\"):\n",
    "        modelnames.append(mt)\n",
    "print(modelnames)\n",
    "modelset = []\n",
    "for mn in modelnames:\n",
    "    modelset.append([model.load_state_dict(torch.load(\"./merge_save/\"+mn)[\"state_dict\"]),mn])\n",
    "# print(dir)\n",
    "savepath = \"./metric/vr_metric=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode, modelname in modelset:\n",
    "    parm = {}\n",
    "    for name,parameters in model.named_parameters():\n",
    "        parm[name]=parameters\n",
    "        # print(parm[name])\n",
    "    tensors = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        t=10\n",
    "        for i in range(0,t):\n",
    "            r = random.randint(0, (batch_size) - 1)\n",
    "\n",
    "            for X_test, Y_test in testloader:\n",
    "                X_test = X_test.to(device)\n",
    "                Y_test = Y_test.to(device)\n",
    "            \n",
    "\n",
    "            X_single_data = X_test[r]\n",
    "            Y_single_data = Y_test[r]\n",
    "\n",
    "            # print(X_single_data)\n",
    "            \n",
    "            layerP = model.module.conv1(torch.unsqueeze(X_single_data,dim=0))\n",
    "            # print(\"1\",layerP)\n",
    "            layerP = model.module.conv2(layerP)\n",
    "            # print(\"2\",layerP)\n",
    "            layerP = model.module.pool1(layerP)\n",
    "            layerP = model.module.bn1(layerP)\n",
    "            layerP = model.module.relu1(layerP)\n",
    "            # print(\"3\",layerP)\n",
    "            layerP = model.module.conv3(layerP)\n",
    "            layerP = model.module.conv4(layerP)\n",
    "            layerP = model.module.pool2(layerP)\n",
    "            layerP = model.module.bn2(layerP)\n",
    "            layerP = model.module.relu2(layerP)\n",
    "            layerP = model.module.conv5(layerP)\n",
    "            layerP = model.module.conv6(layerP)\n",
    "            layerP = model.module.conv7(layerP)\n",
    "            layerP = model.module.pool3(layerP)\n",
    "            layerP = model.module.bn3(layerP)\n",
    "            layerP = model.module.relu3(layerP)\n",
    "            layerP = model.module.conv8(layerP)\n",
    "            layerP = model.module.conv9(layerP)\n",
    "            layerP = model.module.conv10(layerP)\n",
    "            layerP = model.module.pool4(layerP)\n",
    "            layerP = model.module.bn4(layerP)\n",
    "            layerP = model.module.relu4(layerP)\n",
    "            layerP = model.module.conv11(layerP)\n",
    "            layerP = model.module.conv12(layerP)\n",
    "            layerP = model.module.conv13(layerP)\n",
    "            layerP = model.module.pool5(layerP)\n",
    "            layerP = model.module.bn5(layerP)\n",
    "            layerP = model.module.relu5(layerP)\n",
    "            layerP = layerP.detach().squeeze().reshape(-1,512*4*4)\n",
    "\n",
    "            tensor1 = torch.mm(layerP, parm['module.fc14.weight'].data.permute(1,0)) + parm['module.fc14.bias']\n",
    "            tensor1 = torch.mm(tensor1, parm['module.fc15.weight'].data.permute(1,0)) + parm['module.fc15.bias']\n",
    "            \n",
    "            tensors.append(tensor1.cpu().detach().numpy()[0])\n",
    "\n",
    "    nb = np.linspace(0,0,tensors[0].shape[0])\n",
    "    for i in tensors:\n",
    "        nb += i\n",
    "    nb /= t\n",
    "\n",
    "    ns = np.linspace(0,0,tensors[0].shape[0])\n",
    "    vp = np.linspace(0,0,tensors[0].shape[0] ** 2).reshape(tensors[0].shape[0],tensors[0].shape[0])\n",
    "\n",
    "    for i in range(0,len(ns)):\n",
    "        for nps in tensors:\n",
    "            ns[i] += nps[i] ** 2\n",
    "        ns[i] = ns[i] ** 0.5\n",
    "\n",
    "    for i in range(0,len(ns)):\n",
    "        for j in range(i+1,len(ns)):\n",
    "            for nps in tensors:\n",
    "                vp[i,j] += (nps[i] - nb[i]) * (nps[j] - nb[j])\n",
    "            vp[i,j] /= (ns[i] * ns[j])\n",
    "\n",
    "    dt = datetime.datetime.now()\n",
    "    # np.savetxt(\"./misc/vr_metric\"+dt.strftime(\"%d--%H-%M-%S\")+\".dat\", vp+vp.T, fmt=\"%1.5f\")\n",
    "    np.savetxt(savepath+modelname+dt.strftime(\"%m-%d--%H-%M-%S\")+\".dat\", vp+vp.T, fmt=\"%1.5f\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61d7d72412218704c5ba1799d65c7a83b08e24a9ca7847de9a479f6f426633e7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
