{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# input_data shape\n",
    "Input: (batch_size, in_channel, width, height)\n",
    "# conv layer\n",
    "class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "input: (Batch_size, C_in, H_in, W_in)\n",
    "output: (Batch_size, C_out, H_out, W_out)\n",
    "\n",
    "weight(tensor): (out_channels, in_channels,kernel_size)\n",
    "bias(tensor): (out_channel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ripser import Rips\n",
    "from persim import PersistenceImager\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import re\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(111)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 600\n",
    "batchsize = 32\n",
    "testbatchsize = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# dir = os.listdir(\"./metric\")\n",
    "# data = []\n",
    "# a = []\n",
    "# for metric in dir:\n",
    "#     a = re.findall(\"\\d+\\.?\\d*\", metric)\n",
    "#     data.append([np.loadtxt(\"./metric/\"+metric), metric])\n",
    "#     # print(a[-2])\n",
    "#     print(metric[-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class My_dataset(Dataset):\n",
    "    def __init__(self, train):\n",
    "        super().__init__()\n",
    "        # 使用sin函数返回10000个时间序列,如果不自己构造数据，就使用numpy,pandas等读取自己的数据为x即可。\n",
    "        # 以下数据组织这块既可以放在init方法里，也可以放在getitem方法里\n",
    "        dir = os.listdir(\"./grids_single_trainset\")\n",
    "        dirtest = os.listdir(\"./grids_single_testset\")\n",
    "        data = []\n",
    "        a = []\n",
    "        if train:\n",
    "            # for metric in dir:\n",
    "            #     data.append(\n",
    "            #         [np.loadtxt(\"./grids_trainset_cifar/\"+metric), metric])\n",
    "            sample = []\n",
    "            for metric in dir:\n",
    "                sample.append([np.loadtxt(\"./grids_single_trainset/\"+metric), metric])\n",
    "                if len(sample) == 4:\n",
    "                    res = cv2.merge([i[0] for i in sample])\n",
    "                    res = np.transpose(res,(2,0,1))\n",
    "                    data.append([res, metric])\n",
    "                    sample = []\n",
    "            \n",
    "        else:\n",
    "            # for metric in dirtest:\n",
    "            #     data.append(\n",
    "            #         [np.loadtxt(\"./grids_testset_cifar/\"+metric), metric])\n",
    "            sample = []\n",
    "            for metric in dirtest:\n",
    "                sample.append([np.loadtxt(\"./grids_single_testset/\"+metric), metric])\n",
    "                if len(sample) == 4:\n",
    "                    res = cv2.merge([i[0] for i in sample])\n",
    "                    res = np.transpose(res,(2,0,1))\n",
    "                    data.append([res, metric])\n",
    "                    sample = []\n",
    "\n",
    "        self.x = [item[0] for item in data]\n",
    "        self.y = [int(item[1][0:len(\"h_vr_metric-single-mnist_moreFC=mnist_moreFC_attack\")] == \"h_vr_metric-single-mnist_moreFC=mnist_moreFC_attack\") for item in data]\n",
    "        # self.y = [int(re.findall(\"\\d+\\.?\\d*\", item[1])[0]) for item in data]\n",
    "        self.src,  self.trg = [], []\n",
    "        for i in range(len(data)):\n",
    "            self.src.append(self.x[i])\n",
    "            self.trg.append(self.y[i])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.trg[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    " # 或者return len(self.trg), src和trg长度一样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "0\n",
      "torch.Size([32, 4, 128, 128])\n",
      "tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1])\n",
      "1\n",
      "torch.Size([32, 4, 128, 128])\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0])\n",
      "2\n",
      "torch.Size([32, 4, 128, 128])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1])\n",
      "3\n",
      "torch.Size([32, 4, 128, 128])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "4\n",
      "torch.Size([32, 4, 128, 128])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 1])\n",
      "5\n",
      "torch.Size([16, 4, 128, 128])\n",
      "tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1])\n",
      "test\n",
      "0\n",
      "torch.Size([16, 4, 128, 128])\n",
      "tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1])\n",
      "1\n",
      "torch.Size([16, 4, 128, 128])\n",
      "tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "data_tf = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5], [0.5])])\n",
    "     \n",
    "data_train = My_dataset(train=True)\n",
    "data_test = My_dataset(train=False)\n",
    "data_loader_train = DataLoader(data_train, batch_size=batchsize, shuffle=True)\n",
    "data_loader_test = DataLoader(data_test, batch_size=testbatchsize, shuffle=True)\n",
    "\n",
    "\n",
    "# i_batch的多少根据batch size和def __len__(self)返回的长度确定\n",
    "# batch_data返回的值根据def __getitem__(self, index)来确定\n",
    "# 对训练集：(不太清楚enumerate返回什么的时候就多print试试)\n",
    "print(\"train\")\n",
    "for i_batch, batch_data in enumerate(data_loader_train):\n",
    "    print(i_batch)  # 打印batch编号\n",
    "    print(batch_data[0].shape)  # 打印该batch里面src\n",
    "    print(batch_data[1])  # 打印该batch里面trg\n",
    "# # 对测试集：（下面的语句也可以）\n",
    "print(\"test\")\n",
    "for i_batch, (src, trg) in enumerate(data_loader_test):\n",
    "    print(i_batch)  # 打印batch编号\n",
    "    print(batch_data[0].shape)  # 打印该batch里面src\n",
    "    print(batch_data[1])  # 打印该batch里面trg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# class MyNet(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MyNet, self).__init__()\n",
    "#         self.layer1 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(4, 25, kernel_size=64),\n",
    "#             torch.nn.BatchNorm2d(25),\n",
    "#             torch.nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         self.layer2 = torch.nn.Sequential(\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.layer3 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(25, 50, kernel_size=16),\n",
    "#             torch.nn.BatchNorm2d(50),\n",
    "#             torch.nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         self.layer4 = torch.nn.Sequential(\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.fc = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(3200, 1024),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(1024, 1024),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(1024, 128),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(128, 64),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(64, 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# model = MyNet().to(device)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(4, 16, kernel_size=64),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16 * 32 * 32, 1024),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(1024, 1024),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(1024, 128),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "# Softmax is internally computed.\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.693177640 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0002 cost = 0.692977011 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0003 cost = 0.692933202 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0004 cost = 0.693579793 trainacc: 50.56818181818182 testacc: 50.0\n",
      "Epoch: 0005 cost = 0.691765785 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0006 cost = 0.691769123 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0007 cost = 0.692198813 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0008 cost = 0.691116095 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0009 cost = 0.691079438 trainacc: 51.13636363636363 testacc: 50.0\n",
      "Epoch: 0010 cost = 0.690778673 trainacc: 51.13636363636363 testacc: 50.0\n",
      "Epoch: 0011 cost = 0.690746784 trainacc: 51.13636363636363 testacc: 50.0\n",
      "Epoch: 0012 cost = 0.690197110 trainacc: 51.70454545454545 testacc: 50.0\n",
      "Epoch: 0013 cost = 0.690130115 trainacc: 51.70454545454545 testacc: 50.0\n",
      "Epoch: 0014 cost = 0.690355778 trainacc: 52.27272727272727 testacc: 50.0\n",
      "Epoch: 0015 cost = 0.690278411 trainacc: 52.27272727272727 testacc: 50.0\n",
      "Epoch: 0016 cost = 0.690039933 trainacc: 53.40909090909091 testacc: 50.0\n",
      "Epoch: 0017 cost = 0.689369678 trainacc: 53.97727272727273 testacc: 50.0\n",
      "Epoch: 0018 cost = 0.689301074 trainacc: 53.97727272727273 testacc: 50.0\n",
      "Epoch: 0019 cost = 0.689276934 trainacc: 55.11363636363637 testacc: 50.0\n",
      "Epoch: 0020 cost = 0.688873708 trainacc: 55.11363636363637 testacc: 50.0\n",
      "Epoch: 0021 cost = 0.688168526 trainacc: 55.11363636363637 testacc: 50.0\n",
      "Epoch: 0022 cost = 0.688133359 trainacc: 55.11363636363637 testacc: 50.0\n",
      "Epoch: 0023 cost = 0.688074112 trainacc: 55.68181818181818 testacc: 50.0\n",
      "Epoch: 0024 cost = 0.688057184 trainacc: 55.68181818181818 testacc: 50.0\n",
      "Epoch: 0025 cost = 0.687955499 trainacc: 56.25 testacc: 50.0\n",
      "Epoch: 0026 cost = 0.687354147 trainacc: 58.52272727272727 testacc: 53.57142857142857\n",
      "Epoch: 0027 cost = 0.687578857 trainacc: 58.52272727272727 testacc: 57.142857142857146\n",
      "Epoch: 0028 cost = 0.687172711 trainacc: 58.52272727272727 testacc: 57.142857142857146\n",
      "Epoch: 0029 cost = 0.686335385 trainacc: 59.65909090909091 testacc: 57.142857142857146\n",
      "Epoch: 0030 cost = 0.686848044 trainacc: 60.22727272727273 testacc: 57.142857142857146\n",
      "Epoch: 0031 cost = 0.686494410 trainacc: 60.22727272727273 testacc: 57.142857142857146\n",
      "Epoch: 0032 cost = 0.685798526 trainacc: 60.79545454545455 testacc: 57.142857142857146\n",
      "Epoch: 0033 cost = 0.685789883 trainacc: 61.36363636363637 testacc: 57.142857142857146\n",
      "Epoch: 0034 cost = 0.685617030 trainacc: 62.5 testacc: 57.142857142857146\n",
      "Epoch: 0035 cost = 0.684866965 trainacc: 62.5 testacc: 57.142857142857146\n",
      "Epoch: 0036 cost = 0.684968472 trainacc: 63.06818181818182 testacc: 57.142857142857146\n",
      "Epoch: 0037 cost = 0.684374154 trainacc: 63.06818181818182 testacc: 60.714285714285715\n",
      "Epoch: 0038 cost = 0.684354722 trainacc: 64.77272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0039 cost = 0.684732974 trainacc: 64.20454545454545 testacc: 64.28571428571429\n",
      "Epoch: 0040 cost = 0.684384406 trainacc: 65.9090909090909 testacc: 64.28571428571429\n",
      "Epoch: 0041 cost = 0.683712602 trainacc: 66.47727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0042 cost = 0.683657944 trainacc: 66.47727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0043 cost = 0.682827353 trainacc: 64.77272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0044 cost = 0.683214545 trainacc: 67.04545454545455 testacc: 64.28571428571429\n",
      "Epoch: 0045 cost = 0.682024837 trainacc: 65.9090909090909 testacc: 64.28571428571429\n",
      "Epoch: 0046 cost = 0.682050109 trainacc: 64.77272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0047 cost = 0.681922257 trainacc: 65.3409090909091 testacc: 64.28571428571429\n",
      "Epoch: 0048 cost = 0.681281030 trainacc: 65.9090909090909 testacc: 64.28571428571429\n",
      "Epoch: 0049 cost = 0.681047618 trainacc: 66.47727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0050 cost = 0.680541515 trainacc: 66.47727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0051 cost = 0.680530667 trainacc: 66.47727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0052 cost = 0.680148721 trainacc: 67.04545454545455 testacc: 64.28571428571429\n",
      "Epoch: 0053 cost = 0.681373835 trainacc: 69.31818181818181 testacc: 64.28571428571429\n",
      "Epoch: 0054 cost = 0.681139827 trainacc: 69.31818181818181 testacc: 60.714285714285715\n",
      "Epoch: 0055 cost = 0.679399550 trainacc: 69.88636363636364 testacc: 64.28571428571429\n",
      "Epoch: 0056 cost = 0.679772675 trainacc: 70.45454545454545 testacc: 64.28571428571429\n",
      "Epoch: 0057 cost = 0.679241300 trainacc: 69.31818181818181 testacc: 60.714285714285715\n",
      "Epoch: 0058 cost = 0.680328310 trainacc: 69.88636363636364 testacc: 67.85714285714286\n",
      "Epoch: 0059 cost = 0.678857028 trainacc: 69.88636363636364 testacc: 64.28571428571429\n",
      "Epoch: 0060 cost = 0.678223312 trainacc: 70.45454545454545 testacc: 64.28571428571429\n",
      "Epoch: 0061 cost = 0.679584682 trainacc: 69.88636363636364 testacc: 60.714285714285715\n",
      "Epoch: 0062 cost = 0.677507520 trainacc: 71.02272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0063 cost = 0.679060161 trainacc: 69.88636363636364 testacc: 60.714285714285715\n",
      "Epoch: 0064 cost = 0.678334713 trainacc: 70.45454545454545 testacc: 67.85714285714286\n",
      "Epoch: 0065 cost = 0.677497983 trainacc: 70.45454545454545 testacc: 67.85714285714286\n",
      "Epoch: 0066 cost = 0.677438915 trainacc: 70.45454545454545 testacc: 67.85714285714286\n",
      "Epoch: 0067 cost = 0.676379323 trainacc: 71.02272727272727 testacc: 67.85714285714286\n",
      "Epoch: 0068 cost = 0.676371396 trainacc: 71.02272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0069 cost = 0.676950216 trainacc: 71.02272727272727 testacc: 67.85714285714286\n",
      "Epoch: 0070 cost = 0.675369620 trainacc: 72.72727272727273 testacc: 67.85714285714286\n",
      "Epoch: 0071 cost = 0.675207436 trainacc: 72.72727272727273 testacc: 67.85714285714286\n",
      "Epoch: 0072 cost = 0.674901903 trainacc: 72.72727272727273 testacc: 67.85714285714286\n",
      "Epoch: 0073 cost = 0.674551368 trainacc: 73.29545454545455 testacc: 67.85714285714286\n",
      "Epoch: 0074 cost = 0.675408125 trainacc: 74.43181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0075 cost = 0.673494279 trainacc: 73.29545454545455 testacc: 67.85714285714286\n",
      "Epoch: 0076 cost = 0.674202204 trainacc: 74.43181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0077 cost = 0.672958314 trainacc: 74.43181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0078 cost = 0.672972620 trainacc: 75.56818181818181 testacc: 67.85714285714286\n",
      "Epoch: 0079 cost = 0.673275828 trainacc: 76.70454545454545 testacc: 67.85714285714286\n",
      "Epoch: 0080 cost = 0.671855032 trainacc: 76.70454545454545 testacc: 67.85714285714286\n",
      "Epoch: 0081 cost = 0.671897292 trainacc: 76.70454545454545 testacc: 67.85714285714286\n",
      "Epoch: 0082 cost = 0.671282172 trainacc: 75.56818181818181 testacc: 67.85714285714286\n",
      "Epoch: 0083 cost = 0.670434535 trainacc: 76.13636363636364 testacc: 67.85714285714286\n",
      "Epoch: 0084 cost = 0.672325253 trainacc: 78.4090909090909 testacc: 67.85714285714286\n",
      "Epoch: 0085 cost = 0.670830309 trainacc: 76.70454545454545 testacc: 67.85714285714286\n",
      "Epoch: 0086 cost = 0.670563757 trainacc: 78.4090909090909 testacc: 64.28571428571429\n",
      "Epoch: 0087 cost = 0.670527697 trainacc: 78.97727272727273 testacc: 67.85714285714286\n",
      "Epoch: 0088 cost = 0.669530869 trainacc: 78.4090909090909 testacc: 67.85714285714286\n",
      "Epoch: 0089 cost = 0.668078303 trainacc: 77.8409090909091 testacc: 67.85714285714286\n",
      "Epoch: 0090 cost = 0.668734789 trainacc: 78.4090909090909 testacc: 67.85714285714286\n",
      "Epoch: 0091 cost = 0.668903530 trainacc: 78.97727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0092 cost = 0.667117655 trainacc: 78.97727272727273 testacc: 67.85714285714286\n",
      "Epoch: 0093 cost = 0.667413354 trainacc: 79.54545454545455 testacc: 64.28571428571429\n",
      "Epoch: 0094 cost = 0.667434871 trainacc: 81.81818181818181 testacc: 64.28571428571429\n",
      "Epoch: 0095 cost = 0.666152060 trainacc: 80.68181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0096 cost = 0.665271103 trainacc: 80.11363636363636 testacc: 67.85714285714286\n",
      "Epoch: 0097 cost = 0.666080594 trainacc: 81.25 testacc: 67.85714285714286\n",
      "Epoch: 0098 cost = 0.665558577 trainacc: 80.11363636363636 testacc: 67.85714285714286\n",
      "Epoch: 0099 cost = 0.664272428 trainacc: 81.25 testacc: 64.28571428571429\n",
      "Epoch: 0100 cost = 0.664892375 trainacc: 82.38636363636364 testacc: 67.85714285714286\n",
      "Epoch: 0101 cost = 0.663273513 trainacc: 81.81818181818181 testacc: 67.85714285714286\n",
      "Epoch: 0102 cost = 0.663562179 trainacc: 81.25 testacc: 64.28571428571429\n",
      "Epoch: 0103 cost = 0.662364483 trainacc: 81.25 testacc: 67.85714285714286\n",
      "Epoch: 0104 cost = 0.663270950 trainacc: 82.38636363636364 testacc: 67.85714285714286\n",
      "Epoch: 0105 cost = 0.662036598 trainacc: 82.38636363636364 testacc: 64.28571428571429\n",
      "Epoch: 0106 cost = 0.662426829 trainacc: 82.95454545454545 testacc: 64.28571428571429\n",
      "Epoch: 0107 cost = 0.661293745 trainacc: 84.0909090909091 testacc: 67.85714285714286\n",
      "Epoch: 0108 cost = 0.660978913 trainacc: 82.95454545454545 testacc: 64.28571428571429\n",
      "Epoch: 0109 cost = 0.661143124 trainacc: 83.52272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0110 cost = 0.660260379 trainacc: 84.0909090909091 testacc: 64.28571428571429\n",
      "Epoch: 0111 cost = 0.659125686 trainacc: 84.0909090909091 testacc: 60.714285714285715\n",
      "Epoch: 0112 cost = 0.658149183 trainacc: 83.52272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0113 cost = 0.657628059 trainacc: 82.95454545454545 testacc: 64.28571428571429\n",
      "Epoch: 0114 cost = 0.656404614 trainacc: 83.52272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0115 cost = 0.657553911 trainacc: 84.6590909090909 testacc: 57.142857142857146\n",
      "Epoch: 0116 cost = 0.654907405 trainacc: 83.52272727272727 testacc: 60.714285714285715\n",
      "Epoch: 0117 cost = 0.656590998 trainacc: 86.93181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0118 cost = 0.654875398 trainacc: 85.22727272727273 testacc: 60.714285714285715\n",
      "Epoch: 0119 cost = 0.655048013 trainacc: 85.79545454545455 testacc: 57.142857142857146\n",
      "Epoch: 0120 cost = 0.654537201 trainacc: 85.79545454545455 testacc: 60.714285714285715\n",
      "Epoch: 0121 cost = 0.652581334 trainacc: 85.79545454545455 testacc: 57.142857142857146\n",
      "Epoch: 0122 cost = 0.653929532 trainacc: 86.93181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0123 cost = 0.651436269 trainacc: 86.93181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0124 cost = 0.651340365 trainacc: 86.93181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0125 cost = 0.651383996 trainacc: 86.36363636363636 testacc: 64.28571428571429\n",
      "Epoch: 0126 cost = 0.649795651 trainacc: 86.36363636363636 testacc: 60.714285714285715\n",
      "Epoch: 0127 cost = 0.651627302 trainacc: 86.93181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0128 cost = 0.649532378 trainacc: 86.93181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0129 cost = 0.647732794 trainacc: 86.36363636363636 testacc: 64.28571428571429\n",
      "Epoch: 0130 cost = 0.647571981 trainacc: 86.93181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0131 cost = 0.646656930 trainacc: 87.5 testacc: 64.28571428571429\n",
      "Epoch: 0132 cost = 0.646189153 trainacc: 87.5 testacc: 60.714285714285715\n",
      "Epoch: 0133 cost = 0.646371961 trainacc: 88.06818181818181 testacc: 60.714285714285715\n",
      "Epoch: 0134 cost = 0.645808458 trainacc: 88.06818181818181 testacc: 60.714285714285715\n",
      "Epoch: 0135 cost = 0.643845558 trainacc: 86.93181818181819 testacc: 64.28571428571429\n",
      "Epoch: 0136 cost = 0.642478168 trainacc: 87.5 testacc: 60.714285714285715\n",
      "Epoch: 0137 cost = 0.642241061 trainacc: 88.06818181818181 testacc: 57.142857142857146\n",
      "Epoch: 0138 cost = 0.642648697 trainacc: 88.63636363636364 testacc: 60.714285714285715\n",
      "Epoch: 0139 cost = 0.640024364 trainacc: 87.5 testacc: 64.28571428571429\n",
      "Epoch: 0140 cost = 0.640104413 trainacc: 88.06818181818181 testacc: 60.714285714285715\n",
      "Epoch: 0141 cost = 0.640606821 trainacc: 89.77272727272727 testacc: 60.714285714285715\n",
      "Epoch: 0142 cost = 0.637366354 trainacc: 89.20454545454545 testacc: 60.714285714285715\n",
      "Epoch: 0143 cost = 0.636675537 trainacc: 89.20454545454545 testacc: 60.714285714285715\n",
      "Epoch: 0144 cost = 0.636559725 trainacc: 89.77272727272727 testacc: 60.714285714285715\n",
      "Epoch: 0145 cost = 0.634743810 trainacc: 89.20454545454545 testacc: 60.714285714285715\n",
      "Epoch: 0146 cost = 0.633200169 trainacc: 89.20454545454545 testacc: 64.28571428571429\n",
      "Epoch: 0147 cost = 0.634701967 trainacc: 88.63636363636364 testacc: 64.28571428571429\n",
      "Epoch: 0148 cost = 0.632641315 trainacc: 90.3409090909091 testacc: 57.142857142857146\n",
      "Epoch: 0149 cost = 0.633550107 trainacc: 90.3409090909091 testacc: 60.714285714285715\n",
      "Epoch: 0150 cost = 0.631490231 trainacc: 90.3409090909091 testacc: 60.714285714285715\n",
      "Epoch: 0151 cost = 0.630486906 trainacc: 90.9090909090909 testacc: 64.28571428571429\n",
      "Epoch: 0152 cost = 0.626898170 trainacc: 89.20454545454545 testacc: 64.28571428571429\n",
      "Epoch: 0153 cost = 0.627033710 trainacc: 89.20454545454545 testacc: 64.28571428571429\n",
      "Epoch: 0154 cost = 0.626943350 trainacc: 90.9090909090909 testacc: 60.714285714285715\n",
      "Epoch: 0155 cost = 0.626823366 trainacc: 91.47727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0156 cost = 0.624488771 trainacc: 90.9090909090909 testacc: 60.714285714285715\n",
      "Epoch: 0157 cost = 0.625335813 trainacc: 92.61363636363636 testacc: 60.714285714285715\n",
      "Epoch: 0158 cost = 0.622061372 trainacc: 93.18181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0159 cost = 0.622042477 trainacc: 92.61363636363636 testacc: 60.714285714285715\n",
      "Epoch: 0160 cost = 0.619516194 trainacc: 91.47727272727273 testacc: 60.714285714285715\n",
      "Epoch: 0161 cost = 0.618853688 trainacc: 92.61363636363636 testacc: 60.714285714285715\n",
      "Epoch: 0162 cost = 0.618256688 trainacc: 93.18181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0163 cost = 0.614537835 trainacc: 93.18181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0164 cost = 0.613065183 trainacc: 90.9090909090909 testacc: 64.28571428571429\n",
      "Epoch: 0165 cost = 0.612907708 trainacc: 92.61363636363636 testacc: 60.714285714285715\n",
      "Epoch: 0166 cost = 0.612328827 trainacc: 92.04545454545455 testacc: 64.28571428571429\n",
      "Epoch: 0167 cost = 0.609714210 trainacc: 93.18181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0168 cost = 0.610472620 trainacc: 93.18181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0169 cost = 0.610987127 trainacc: 92.61363636363636 testacc: 60.714285714285715\n",
      "Epoch: 0170 cost = 0.606202483 trainacc: 93.75 testacc: 60.714285714285715\n",
      "Epoch: 0171 cost = 0.601537824 trainacc: 93.18181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0172 cost = 0.601030827 trainacc: 93.18181818181819 testacc: 64.28571428571429\n",
      "Epoch: 0173 cost = 0.600533128 trainacc: 93.18181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0174 cost = 0.600430906 trainacc: 93.18181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0175 cost = 0.598336101 trainacc: 94.31818181818181 testacc: 67.85714285714286\n",
      "Epoch: 0176 cost = 0.594504476 trainacc: 93.75 testacc: 64.28571428571429\n",
      "Epoch: 0177 cost = 0.593322158 trainacc: 93.75 testacc: 60.714285714285715\n",
      "Epoch: 0178 cost = 0.591101587 trainacc: 93.18181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0179 cost = 0.589271247 trainacc: 93.75 testacc: 64.28571428571429\n",
      "Epoch: 0180 cost = 0.586067080 trainacc: 93.75 testacc: 60.714285714285715\n",
      "Epoch: 0181 cost = 0.583347201 trainacc: 93.18181818181819 testacc: 64.28571428571429\n",
      "Epoch: 0182 cost = 0.583021879 trainacc: 94.31818181818181 testacc: 67.85714285714286\n",
      "Epoch: 0183 cost = 0.578103244 trainacc: 93.75 testacc: 60.714285714285715\n",
      "Epoch: 0184 cost = 0.577165425 trainacc: 94.88636363636364 testacc: 64.28571428571429\n",
      "Epoch: 0185 cost = 0.575280845 trainacc: 94.31818181818181 testacc: 64.28571428571429\n",
      "Epoch: 0186 cost = 0.580022693 trainacc: 95.45454545454545 testacc: 67.85714285714286\n",
      "Epoch: 0187 cost = 0.569912255 trainacc: 95.45454545454545 testacc: 64.28571428571429\n",
      "Epoch: 0188 cost = 0.569885135 trainacc: 95.45454545454545 testacc: 60.714285714285715\n",
      "Epoch: 0189 cost = 0.567705035 trainacc: 96.02272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0190 cost = 0.564872682 trainacc: 96.02272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0191 cost = 0.559487522 trainacc: 95.45454545454545 testacc: 60.714285714285715\n",
      "Epoch: 0192 cost = 0.562440574 trainacc: 94.31818181818181 testacc: 67.85714285714286\n",
      "Epoch: 0193 cost = 0.558009624 trainacc: 94.88636363636364 testacc: 67.85714285714286\n",
      "Epoch: 0194 cost = 0.552876830 trainacc: 96.02272727272727 testacc: 67.85714285714286\n",
      "Epoch: 0195 cost = 0.550790429 trainacc: 96.02272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0196 cost = 0.545195341 trainacc: 93.75 testacc: 67.85714285714286\n",
      "Epoch: 0197 cost = 0.545610785 trainacc: 96.02272727272727 testacc: 67.85714285714286\n",
      "Epoch: 0198 cost = 0.538770556 trainacc: 97.1590909090909 testacc: 64.28571428571429\n",
      "Epoch: 0199 cost = 0.535947800 trainacc: 96.02272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0200 cost = 0.535502672 trainacc: 96.5909090909091 testacc: 67.85714285714286\n",
      "Epoch: 0201 cost = 0.529855669 trainacc: 97.72727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0202 cost = 0.525854647 trainacc: 96.5909090909091 testacc: 64.28571428571429\n",
      "Epoch: 0203 cost = 0.522125483 trainacc: 96.5909090909091 testacc: 57.142857142857146\n",
      "Epoch: 0204 cost = 0.519099295 trainacc: 97.72727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0205 cost = 0.518706203 trainacc: 96.02272727272727 testacc: 67.85714285714286\n",
      "Epoch: 0206 cost = 0.509954810 trainacc: 97.1590909090909 testacc: 67.85714285714286\n",
      "Epoch: 0207 cost = 0.508354843 trainacc: 97.72727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0208 cost = 0.500709176 trainacc: 98.29545454545455 testacc: 64.28571428571429\n",
      "Epoch: 0209 cost = 0.504297137 trainacc: 97.72727272727273 testacc: 67.85714285714286\n",
      "Epoch: 0210 cost = 0.493426442 trainacc: 97.72727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0211 cost = 0.489694357 trainacc: 98.86363636363636 testacc: 67.85714285714286\n",
      "Epoch: 0212 cost = 0.482437372 trainacc: 97.72727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0213 cost = 0.478317320 trainacc: 96.02272727272727 testacc: 64.28571428571429\n",
      "Epoch: 0214 cost = 0.475174963 trainacc: 93.75 testacc: 57.142857142857146\n",
      "Epoch: 0215 cost = 0.473180592 trainacc: 92.61363636363636 testacc: 60.714285714285715\n",
      "Epoch: 0216 cost = 0.471736431 trainacc: 97.1590909090909 testacc: 67.85714285714286\n",
      "Epoch: 0217 cost = 0.461436182 trainacc: 82.38636363636364 testacc: 53.57142857142857\n",
      "Epoch: 0218 cost = 0.460715860 trainacc: 96.02272727272727 testacc: 60.714285714285715\n",
      "Epoch: 0219 cost = 0.448913008 trainacc: 97.1590909090909 testacc: 67.85714285714286\n",
      "Epoch: 0220 cost = 0.445929646 trainacc: 95.45454545454545 testacc: 60.714285714285715\n",
      "Epoch: 0221 cost = 0.441053271 trainacc: 97.72727272727273 testacc: 57.142857142857146\n",
      "Epoch: 0222 cost = 0.431955189 trainacc: 99.43181818181819 testacc: 64.28571428571429\n",
      "Epoch: 0223 cost = 0.427730083 trainacc: 96.5909090909091 testacc: 57.142857142857146\n",
      "Epoch: 0224 cost = 0.423385322 trainacc: 98.86363636363636 testacc: 67.85714285714286\n",
      "Epoch: 0225 cost = 0.413899481 trainacc: 97.72727272727273 testacc: 57.142857142857146\n",
      "Epoch: 0226 cost = 0.409410924 trainacc: 98.86363636363636 testacc: 64.28571428571429\n",
      "Epoch: 0227 cost = 0.401505768 trainacc: 98.86363636363636 testacc: 67.85714285714286\n",
      "Epoch: 0228 cost = 0.401076317 trainacc: 94.88636363636364 testacc: 57.142857142857146\n",
      "Epoch: 0229 cost = 0.391901195 trainacc: 96.5909090909091 testacc: 67.85714285714286\n",
      "Epoch: 0230 cost = 0.384679198 trainacc: 98.86363636363636 testacc: 57.142857142857146\n",
      "Epoch: 0231 cost = 0.384066254 trainacc: 96.02272727272727 testacc: 57.142857142857146\n",
      "Epoch: 0232 cost = 0.376242757 trainacc: 98.29545454545455 testacc: 67.85714285714286\n",
      "Epoch: 0233 cost = 0.367321849 trainacc: 98.86363636363636 testacc: 71.42857142857143\n",
      "Epoch: 0234 cost = 0.359624177 trainacc: 99.43181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0235 cost = 0.347862512 trainacc: 94.31818181818181 testacc: 57.142857142857146\n",
      "Epoch: 0236 cost = 0.346271753 trainacc: 94.88636363636364 testacc: 57.142857142857146\n",
      "Epoch: 0237 cost = 0.344740450 trainacc: 96.5909090909091 testacc: 53.57142857142857\n",
      "Epoch: 0238 cost = 0.323402554 trainacc: 97.72727272727273 testacc: 64.28571428571429\n",
      "Epoch: 0239 cost = 0.325756460 trainacc: 99.43181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0240 cost = 0.313988060 trainacc: 98.86363636363636 testacc: 71.42857142857143\n",
      "Epoch: 0241 cost = 0.309782386 trainacc: 98.29545454545455 testacc: 57.142857142857146\n",
      "Epoch: 0242 cost = 0.311583936 trainacc: 98.86363636363636 testacc: 53.57142857142857\n",
      "Epoch: 0243 cost = 0.294937134 trainacc: 96.02272727272727 testacc: 57.142857142857146\n",
      "Epoch: 0244 cost = 0.293354869 trainacc: 100.0 testacc: 53.57142857142857\n",
      "Epoch: 0245 cost = 0.296763122 trainacc: 86.36363636363636 testacc: 53.57142857142857\n",
      "Epoch: 0246 cost = 0.303372234 trainacc: 99.43181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0247 cost = 0.269422948 trainacc: 99.43181818181819 testacc: 64.28571428571429\n",
      "Epoch: 0248 cost = 0.267945915 trainacc: 93.18181818181819 testacc: 57.142857142857146\n",
      "Epoch: 0249 cost = 0.262461871 trainacc: 99.43181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0250 cost = 0.254561543 trainacc: 99.43181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0251 cost = 0.243194610 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0252 cost = 0.231095999 trainacc: 99.43181818181819 testacc: 57.142857142857146\n",
      "Epoch: 0253 cost = 0.240165949 trainacc: 99.43181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0254 cost = 0.222459435 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0255 cost = 0.208720148 trainacc: 99.43181818181819 testacc: 64.28571428571429\n",
      "Epoch: 0256 cost = 0.226253733 trainacc: 99.43181818181819 testacc: 60.714285714285715\n",
      "Epoch: 0257 cost = 0.202004373 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0258 cost = 0.195946142 trainacc: 100.0 testacc: 53.57142857142857\n",
      "Epoch: 0259 cost = 0.183094218 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0260 cost = 0.183845684 trainacc: 99.43181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0261 cost = 0.178945228 trainacc: 99.43181818181819 testacc: 75.0\n",
      "Epoch: 0262 cost = 0.174472913 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0263 cost = 0.166026950 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0264 cost = 0.176456392 trainacc: 99.43181818181819 testacc: 67.85714285714286\n",
      "Epoch: 0265 cost = 0.160916999 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0266 cost = 0.155456185 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0267 cost = 0.149543941 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0268 cost = 0.146439359 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0269 cost = 0.134837270 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0270 cost = 0.133639038 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0271 cost = 0.128930092 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0272 cost = 0.123862058 trainacc: 100.0 testacc: 71.42857142857143\n",
      "Epoch: 0273 cost = 0.125286624 trainacc: 100.0 testacc: 53.57142857142857\n",
      "Epoch: 0274 cost = 0.117125481 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0275 cost = 0.112528019 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0276 cost = 0.108479835 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0277 cost = 0.110179640 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0278 cost = 0.102601320 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0279 cost = 0.105249479 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0280 cost = 0.097609475 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0281 cost = 0.092365637 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0282 cost = 0.090771787 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0283 cost = 0.089842238 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0284 cost = 0.086979374 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0285 cost = 0.082266778 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0286 cost = 0.082045689 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0287 cost = 0.079129182 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0288 cost = 0.074982584 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0289 cost = 0.073680788 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0290 cost = 0.069848962 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0291 cost = 0.068918623 trainacc: 100.0 testacc: 71.42857142857143\n",
      "Epoch: 0292 cost = 0.068268873 trainacc: 100.0 testacc: 53.57142857142857\n",
      "Epoch: 0293 cost = 0.066183366 trainacc: 100.0 testacc: 71.42857142857143\n",
      "Epoch: 0294 cost = 0.062439002 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0295 cost = 0.060713366 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0296 cost = 0.062757030 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0297 cost = 0.057405233 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0298 cost = 0.062444679 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0299 cost = 0.054268852 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0300 cost = 0.053180046 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0301 cost = 0.052890237 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0302 cost = 0.050837424 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0303 cost = 0.050092340 trainacc: 100.0 testacc: 71.42857142857143\n",
      "Epoch: 0304 cost = 0.050073601 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0305 cost = 0.048771515 trainacc: 100.0 testacc: 75.0\n",
      "Epoch: 0306 cost = 0.045615476 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0307 cost = 0.046763636 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0308 cost = 0.043250274 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0309 cost = 0.042717837 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0310 cost = 0.041603770 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0311 cost = 0.039965812 trainacc: 100.0 testacc: 71.42857142857143\n",
      "Epoch: 0312 cost = 0.039704874 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0313 cost = 0.038125940 trainacc: 100.0 testacc: 71.42857142857143\n",
      "Epoch: 0314 cost = 0.038906530 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0315 cost = 0.037447464 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0316 cost = 0.036478315 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0317 cost = 0.035702925 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0318 cost = 0.035421528 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0319 cost = 0.033827778 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0320 cost = 0.033093303 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0321 cost = 0.032235097 trainacc: 100.0 testacc: 71.42857142857143\n",
      "Epoch: 0322 cost = 0.032619484 trainacc: 100.0 testacc: 71.42857142857143\n",
      "Epoch: 0323 cost = 0.031043703 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0324 cost = 0.030161873 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0325 cost = 0.030102711 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0326 cost = 0.029577898 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0327 cost = 0.029252850 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0328 cost = 0.028922660 trainacc: 100.0 testacc: 71.42857142857143\n",
      "Epoch: 0329 cost = 0.028459467 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0330 cost = 0.027409833 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0331 cost = 0.026620824 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0332 cost = 0.027273316 trainacc: 100.0 testacc: 71.42857142857143\n",
      "Epoch: 0333 cost = 0.025511626 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0334 cost = 0.025266381 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0335 cost = 0.025139954 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0336 cost = 0.024321789 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0337 cost = 0.024301672 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0338 cost = 0.023478014 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0339 cost = 0.023313904 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0340 cost = 0.022623859 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0341 cost = 0.022879804 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0342 cost = 0.021552656 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0343 cost = 0.021452079 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0344 cost = 0.020350445 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0345 cost = 0.021128273 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0346 cost = 0.020644153 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0347 cost = 0.020266417 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0348 cost = 0.020089060 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0349 cost = 0.019797226 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0350 cost = 0.019452760 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0351 cost = 0.019027699 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0352 cost = 0.018735491 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0353 cost = 0.018992184 trainacc: 100.0 testacc: 71.42857142857143\n",
      "Epoch: 0354 cost = 0.018464640 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0355 cost = 0.017828967 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0356 cost = 0.017741520 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0357 cost = 0.017372962 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0358 cost = 0.016937511 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0359 cost = 0.017039977 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0360 cost = 0.016691204 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0361 cost = 0.016532296 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0362 cost = 0.016179863 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0363 cost = 0.016069178 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0364 cost = 0.015992541 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0365 cost = 0.015529597 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0366 cost = 0.015289607 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0367 cost = 0.015253857 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0368 cost = 0.014954282 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0369 cost = 0.014533312 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0370 cost = 0.014458574 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0371 cost = 0.014482822 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0372 cost = 0.013924637 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0373 cost = 0.013656635 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0374 cost = 0.014263045 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0375 cost = 0.013353002 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0376 cost = 0.013859255 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0377 cost = 0.013068300 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0378 cost = 0.013011156 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0379 cost = 0.013059691 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0380 cost = 0.012599062 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0381 cost = 0.012653127 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0382 cost = 0.012263439 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0383 cost = 0.012185746 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0384 cost = 0.012051958 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0385 cost = 0.011977550 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0386 cost = 0.011847628 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0387 cost = 0.011900235 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0388 cost = 0.011299574 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0389 cost = 0.011502474 trainacc: 100.0 testacc: 57.142857142857146\n",
      "Epoch: 0390 cost = 0.011233728 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0391 cost = 0.011047415 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0392 cost = 0.011094552 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0393 cost = 0.011091438 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0394 cost = 0.010854110 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0395 cost = 0.010753062 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0396 cost = 0.010690314 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0397 cost = 0.010720538 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0398 cost = 0.010546511 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0399 cost = 0.010213159 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0400 cost = 0.010118239 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0401 cost = 0.009799151 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0402 cost = 0.009895718 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0403 cost = 0.009824086 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0404 cost = 0.009908472 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0405 cost = 0.010123949 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0406 cost = 0.009547864 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0407 cost = 0.009414760 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0408 cost = 0.009350786 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0409 cost = 0.008928657 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0410 cost = 0.009520387 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0411 cost = 0.009049784 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0412 cost = 0.008886013 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0413 cost = 0.008976880 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0414 cost = 0.008676479 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0415 cost = 0.008692785 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0416 cost = 0.008537397 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0417 cost = 0.008468386 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0418 cost = 0.008536522 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0419 cost = 0.008342700 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0420 cost = 0.008205037 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0421 cost = 0.008644114 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0422 cost = 0.008327532 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0423 cost = 0.008270022 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0424 cost = 0.008468012 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0425 cost = 0.008241663 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0426 cost = 0.008097863 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0427 cost = 0.007886788 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0428 cost = 0.007879118 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0429 cost = 0.008405289 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0430 cost = 0.007528885 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0431 cost = 0.007704864 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0432 cost = 0.007372410 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0433 cost = 0.007598610 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0434 cost = 0.007562285 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0435 cost = 0.007284212 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0436 cost = 0.007393030 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0437 cost = 0.007148088 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0438 cost = 0.007188029 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0439 cost = 0.007203466 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0440 cost = 0.006980460 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0441 cost = 0.006884922 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0442 cost = 0.006844717 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0443 cost = 0.006836317 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0444 cost = 0.006658672 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0445 cost = 0.006855794 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0446 cost = 0.006386246 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0447 cost = 0.006644394 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0448 cost = 0.006764304 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0449 cost = 0.006566534 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0450 cost = 0.006338555 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0451 cost = 0.006692676 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0452 cost = 0.006285215 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0453 cost = 0.006275205 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0454 cost = 0.006268763 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0455 cost = 0.006200159 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0456 cost = 0.006116108 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0457 cost = 0.006214354 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0458 cost = 0.005922339 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0459 cost = 0.006012062 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0460 cost = 0.006037995 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0461 cost = 0.005865972 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0462 cost = 0.006311878 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0463 cost = 0.005859020 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0464 cost = 0.005727171 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0465 cost = 0.005820722 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0466 cost = 0.005724974 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0467 cost = 0.005666253 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0468 cost = 0.005723987 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0469 cost = 0.005657808 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0470 cost = 0.005544978 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0471 cost = 0.005331292 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0472 cost = 0.005395956 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0473 cost = 0.005426825 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0474 cost = 0.005416950 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0475 cost = 0.005482562 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0476 cost = 0.005290310 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0477 cost = 0.005401189 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0478 cost = 0.005172683 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0479 cost = 0.005088507 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0480 cost = 0.005216478 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0481 cost = 0.005117078 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0482 cost = 0.005042792 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0483 cost = 0.004945103 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0484 cost = 0.005051949 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0485 cost = 0.005031023 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0486 cost = 0.005127889 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0487 cost = 0.005021772 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0488 cost = 0.004928811 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0489 cost = 0.005221064 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0490 cost = 0.004868183 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0491 cost = 0.004835716 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0492 cost = 0.004736258 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0493 cost = 0.004766042 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0494 cost = 0.004898390 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0495 cost = 0.004600345 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0496 cost = 0.004614405 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0497 cost = 0.004556743 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0498 cost = 0.004724055 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0499 cost = 0.004542602 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0500 cost = 0.004659985 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0501 cost = 0.004502529 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0502 cost = 0.004735497 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0503 cost = 0.004398365 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0504 cost = 0.004391788 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0505 cost = 0.004470932 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0506 cost = 0.004241938 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0507 cost = 0.004313767 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0508 cost = 0.004368894 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0509 cost = 0.004808849 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0510 cost = 0.004321445 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0511 cost = 0.004409411 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0512 cost = 0.004239518 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0513 cost = 0.004406310 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0514 cost = 0.004268298 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0515 cost = 0.004184862 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0516 cost = 0.004063523 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0517 cost = 0.004208506 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0518 cost = 0.004246400 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0519 cost = 0.003985050 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0520 cost = 0.004049925 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0521 cost = 0.003932101 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0522 cost = 0.004184390 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0523 cost = 0.004058175 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0524 cost = 0.003885912 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0525 cost = 0.003848005 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0526 cost = 0.003920693 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0527 cost = 0.004055201 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0528 cost = 0.003948261 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0529 cost = 0.003834574 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0530 cost = 0.003855265 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0531 cost = 0.003833309 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0532 cost = 0.003789202 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0533 cost = 0.003736586 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0534 cost = 0.003812788 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0535 cost = 0.003652103 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0536 cost = 0.003843927 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0537 cost = 0.003858407 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0538 cost = 0.003622725 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0539 cost = 0.003715561 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0540 cost = 0.003736547 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0541 cost = 0.003468991 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0542 cost = 0.003605259 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0543 cost = 0.003519372 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0544 cost = 0.003517533 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0545 cost = 0.003600689 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0546 cost = 0.003487875 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0547 cost = 0.003509883 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0548 cost = 0.003448735 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0549 cost = 0.003555307 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0550 cost = 0.003417687 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0551 cost = 0.003397000 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0552 cost = 0.003461123 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0553 cost = 0.003417878 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0554 cost = 0.003375748 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0555 cost = 0.003370624 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0556 cost = 0.003390365 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0557 cost = 0.003325934 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0558 cost = 0.003350105 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0559 cost = 0.003371510 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0560 cost = 0.003319851 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0561 cost = 0.003231852 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0562 cost = 0.003414619 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0563 cost = 0.003222220 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0564 cost = 0.003250827 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0565 cost = 0.003142038 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0566 cost = 0.003286145 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0567 cost = 0.003156249 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0568 cost = 0.003184940 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0569 cost = 0.003152569 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0570 cost = 0.003175442 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0571 cost = 0.003129268 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0572 cost = 0.003079093 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0573 cost = 0.003141519 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0574 cost = 0.003116305 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0575 cost = 0.003041396 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0576 cost = 0.003252534 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0577 cost = 0.003134810 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0578 cost = 0.003167076 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0579 cost = 0.003046477 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0580 cost = 0.003051373 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0581 cost = 0.003082295 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0582 cost = 0.003210413 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0583 cost = 0.002889066 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0584 cost = 0.002874090 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0585 cost = 0.002936355 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0586 cost = 0.002866022 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0587 cost = 0.002974672 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0588 cost = 0.002848073 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0589 cost = 0.002842348 trainacc: 100.0 testacc: 60.714285714285715\n",
      "Epoch: 0590 cost = 0.002871020 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0591 cost = 0.003022354 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0592 cost = 0.002793323 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0593 cost = 0.002809236 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0594 cost = 0.002791816 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0595 cost = 0.002782190 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0596 cost = 0.002755541 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0597 cost = 0.002814977 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0598 cost = 0.002828103 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Epoch: 0599 cost = 0.002812417 trainacc: 100.0 testacc: 64.28571428571429\n",
      "Epoch: 0600 cost = 0.002709457 trainacc: 100.0 testacc: 67.85714285714286\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader_train)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    avg_cost = 0\n",
    "\n",
    "    # for X, Y in data_loader_train:\n",
    "    #     X = torch.autograd.Variable(X).to(device).float()\n",
    "    #     Y = Y.to(device)\n",
    "\n",
    "    #     optimizer.zero_grad()\n",
    "    #     hypothesis = model(X)\n",
    "    #     cost = criterion(hypothesis, Y)\n",
    "    #     cost.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    #     avg_cost += cost / total_batch\n",
    "    # model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader_train):\n",
    "        inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        avg_cost += loss / total_batch\n",
    "\n",
    "    \n",
    "    # if avg_cost < 0.1:\n",
    "    #     break\n",
    "    model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for X_test, Y_test in data_loader_test:\n",
    "    #         X_test = X_test.to(device).float()\n",
    "    #         Y_test = Y_test.to(device)\n",
    "\n",
    "    #     prediction = model(X_test)\n",
    "    #     correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    #     accuracy = correct_prediction.float().mean()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader_train):\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    trainacc = 100.*correct/total\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader_test):\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    testacc = 100.*correct/total\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost), 'trainacc:', trainacc, 'testacc:', testacc)\n",
    "\n",
    "print('Learning finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.1940e-01, -5.1265e-01],\n",
      "        [ 6.0453e-01, -8.6352e-01],\n",
      "        [ 6.5567e-01, -8.6844e-01],\n",
      "        [-5.2810e-01,  2.6343e-01],\n",
      "        [-5.4656e-04, -3.3572e-01],\n",
      "        [ 1.0053e+00, -1.0513e+00],\n",
      "        [-9.1860e-01,  5.8246e-01],\n",
      "        [-3.0382e-01,  5.4991e-02],\n",
      "        [ 1.7246e-01, -3.8440e-01],\n",
      "        [ 7.2333e-01, -8.8116e-01],\n",
      "        [-1.8611e+00,  1.4877e+00],\n",
      "        [ 2.9441e-01, -4.8765e-01],\n",
      "        [ 1.0303e-01, -3.9827e-01],\n",
      "        [-1.0586e+00,  7.6144e-01],\n",
      "        [ 1.7942e-01, -5.0740e-01],\n",
      "        [-2.0355e+00,  1.6891e+00]], device='cuda:0') tensor([False,  True, False,  True, False,  True, False,  True,  True, False,\n",
      "         True,  True, False, False,  True,  True], device='cuda:0') tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.5625\n",
      "tensor([[ 1.7355, -1.8531],\n",
      "        [ 0.2200, -0.5593],\n",
      "        [-0.1905, -0.0892],\n",
      "        [-0.7441,  0.3058],\n",
      "        [-0.2657,  0.0051],\n",
      "        [ 0.1859, -0.3884],\n",
      "        [ 0.2653, -0.4530],\n",
      "        [ 0.5729, -0.7141],\n",
      "        [ 1.1230, -1.3257],\n",
      "        [-0.3772,  0.1047],\n",
      "        [-0.1522, -0.1588],\n",
      "        [-3.3005,  2.9048]], device='cuda:0') tensor([ True,  True,  True, False,  True,  True,  True,  True, False,  True,\n",
      "         True,  True], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.8333333730697632\n"
     ]
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "# model.load_state_dict(torch.load(\".\\merge_save\\merge_attack0normal10_05-07--21-10-32.pth\")[\"state_dict\"])\n",
    "# model.load_state_dict(torch.load(r\".\\myfed_normal_save\\model18-24-02.pth\")[\"state_dict\"])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_test, Y_test in data_loader_test:\n",
    "        X_test = X_test.to(device).float()\n",
    "        Y_test = Y_test.to(device)\n",
    "\n",
    "        prediction = model(X_test)\n",
    "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        print(prediction,correct_prediction, Y_test)\n",
    "        accuracy = correct_prediction.float().mean()\n",
    "        print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # Get one and predict\n",
    "    # r = random.randint(0, len(mnist_test) - 1)\n",
    "    # X_single_data = mnist_test.data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    # Y_single_data = mnist_test.targets[r:r + 1].to(device)\n",
    "\n",
    "    # print('Label: ', Y_single_data.item())\n",
    "    # single_prediction = model(X_single_data)\n",
    "    # print('Prediction: ', torch.argmax(single_prediction, 1).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./classifier/single=07-18--21-27-13.pth\n"
     ]
    }
   ],
   "source": [
    "savepath = \"./classifier/single=\"\n",
    "now = datetime.datetime.now()\n",
    "state = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}\n",
    "print(savepath+now.strftime(\"%m-%d--%H-%M-%S\")+\".pth\")\n",
    "torch.save(state, savepath+now.strftime(\"%m-%d--%H-%M-%S\")+\".pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d28dcb1746f058774bc7e3103f8fd674f9a46f28636659ba6a6fdaf0b17e1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
