{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# input_data shape\n",
    "Input: (batch_size, in_channel, width, height)\n",
    "# conv layer\n",
    "class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "input: (Batch_size, C_in, H_in, W_in)\n",
    "output: (Batch_size, C_out, H_out, W_out)\n",
    "\n",
    "weight(tensor): (out_channels, in_channels,kernel_size)\n",
    "bias(tensor): (out_channel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ripser import Rips\n",
    "from persim import PersistenceImager\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import persim\n",
    "\n",
    "import re\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(111)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "batchsize = 25\n",
    "testbatchsize = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = os.listdir(\"./metric\")\n",
    "# data = []\n",
    "# a = []\n",
    "# for metric in dir:\n",
    "#     a = re.findall(\"\\d+\\.?\\d*\", metric)\n",
    "#     data.append([np.loadtxt(\"./metric/\"+metric), metric])\n",
    "#     # print(a[-2])\n",
    "#     print(metric[-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_dataset(Dataset):\n",
    "    def __init__(self, train):\n",
    "        super().__init__()\n",
    "        dirtrain = os.listdir(\"./grids_fed_trainset\")\n",
    "        dirtest = os.listdir(\"./grids_fed_testset\")\n",
    "        data = []\n",
    "        \n",
    "        if train:\n",
    "            dir = dirtrain\n",
    "            path = \"./grids_fed_trainset/\"\n",
    "        else:\n",
    "            dir = dirtest\n",
    "            path = \"./grids_fed_testset/\"\n",
    "\n",
    "        T_normal = []\n",
    "        T_attack = []\n",
    "        for metric in dir[0:4]:\n",
    "            dgm = []\n",
    "            grid = pd.DataFrame(np.loadtxt(path+metric))\n",
    "            for i in range(grid.shape[0]):\n",
    "                for j in range(grid.shape[1]):\n",
    "                    if grid[i][j] > 0:\n",
    "                        dgm.append([i,j])\n",
    "            T_normal.append(dgm)\n",
    "        for metric in dir[-4:]:\n",
    "            dgm = []\n",
    "            grid = pd.DataFrame(np.loadtxt(path+metric))\n",
    "            for i in range(grid.shape[0]):\n",
    "                for j in range(grid.shape[1]):\n",
    "                    if grid[i][j] > 0:\n",
    "                        dgm.append([i,j])\n",
    "            T_attack.append(dgm)\n",
    "\n",
    "        print(len(T_attack), len(T_normal))\n",
    "        \n",
    "        sample = []\n",
    "        distance = 1\n",
    "        normal_attack = [0,0]\n",
    "        for metric in dir:\n",
    "            sample.append([np.loadtxt(path+metric), metric])\n",
    "            label = int(\"attack\" in metric)\n",
    "\n",
    "            # if (label == 0):\n",
    "            #     distance_bottleneck, matching = persim.bottleneck(sample[len(sample)-1][0], T_normal[len(sample)-1], matching=True)\n",
    "            # else:\n",
    "            #     distance_bottleneck, matching = persim.bottleneck(sample[len(sample)-1][0], T_attack[len(sample)-1], matching=True)\n",
    "            # distance += distance_bottleneck\n",
    "            if len(sample) == 4:\n",
    "                res = cv2.merge([i[0] for i in sample])\n",
    "                res = np.transpose(res,(2,0,1))\n",
    "                # d = 1/(1+math.exp(math.log(distance)))\n",
    "                data.append([res, np.array((label,1.2))])\n",
    "                sample = []\n",
    "            \n",
    "        if train:\n",
    "            for i in data:\n",
    "                normal_attack[int(i[1][0])] += 1\n",
    "            print(normal_attack)\n",
    "            data.sort(key=lambda x: x[1][0])\n",
    "            data = data[0:normal_attack[1]]+data[-normal_attack[1]:]\n",
    "            normal_attack = [0,0]\n",
    "            for i in data:\n",
    "                normal_attack[int(i[1][0])] += 1\n",
    "            print(normal_attack)\n",
    "        \n",
    "        self.x = [item[0] for item in data]\n",
    "        self.y = [item[1] for item in data]\n",
    "        # self.y = [int(re.findall(\"\\d+\\.?\\d*\", item[1])[0]) for item in data]\n",
    "        self.src,  self.trg = [], []\n",
    "        \n",
    "\n",
    "        for i in range(len(data)):\n",
    "            self.src.append(self.x[i])\n",
    "            self.trg.append(self.y[i])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.trg[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    " # 或者return len(self.trg), src和trg长度一样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "[342, 137]\n",
      "[137, 137]\n",
      "4 4\n",
      "train\n",
      "0\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([[0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "1\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([[0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "2\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([[0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "3\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([[1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "4\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([[0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "5\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([[1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000]], dtype=torch.float64)\n",
      "6\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([[1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "7\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([[0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "8\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([[1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "9\n",
      "torch.Size([25, 4, 128, 128])\n",
      "tensor([[1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "10\n",
      "torch.Size([24, 4, 128, 128])\n",
      "tensor([[1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "test\n",
      "0\n",
      "torch.Size([10, 4, 128, 128])\n",
      "tensor([[1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "1\n",
      "torch.Size([10, 4, 128, 128])\n",
      "tensor([[0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "2\n",
      "torch.Size([10, 4, 128, 128])\n",
      "tensor([[1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000]], dtype=torch.float64)\n",
      "3\n",
      "torch.Size([10, 4, 128, 128])\n",
      "tensor([[1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [0.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [1.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n",
      "4\n",
      "torch.Size([2, 4, 128, 128])\n",
      "tensor([[0.0000, 1.2000],\n",
      "        [0.0000, 1.2000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "data_tf = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5], [0.5])])\n",
    "     \n",
    "data_train = My_dataset(train=True)\n",
    "data_test = My_dataset(train=False)\n",
    "data_loader_train = DataLoader(data_train, batch_size=batchsize, shuffle=True)\n",
    "data_loader_test = DataLoader(data_test, batch_size=testbatchsize, shuffle=False)\n",
    "\n",
    "\n",
    "# i_batch的多少根据batch size和def __len__(self)返回的长度确定\n",
    "# batch_data返回的值根据def __getitem__(self, index)来确定\n",
    "# 对训练集：(不太清楚enumerate返回什么的时候就多print试试)\n",
    "print(\"train\")\n",
    "for i_batch, batch_data in enumerate(data_loader_train):\n",
    "    print(i_batch)  # 打印batch编号\n",
    "    print(batch_data[0].shape)  # 打印该batch里面src\n",
    "    print(batch_data[1])  # 打印该batch里面trg\n",
    "# # 对测试集：（下面的语句也可以）\n",
    "print(\"test\")\n",
    "for i_batch, batch_data in enumerate(data_loader_test):\n",
    "    print(i_batch)  # 打印batch编号\n",
    "    print(batch_data[0].shape)  # 打印该batch里面src\n",
    "    print(batch_data[1])  # 打印该batch里面trg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(4, 16, kernel_size=64),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16 * 32 * 32, 1024),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(1024, 1024),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(1024, 128),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss2(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    def forward(self, output, target):\n",
    "        res = -output.gather(dim=1, index=target.view(-1, 1))\n",
    "        res += torch.log(torch.exp(output).sum(dim=1).view(-1, 1))\n",
    "        res = res.mean()\n",
    "        return res\n",
    "    def forward(self, output, target, distance):\n",
    "        res = -output.gather(dim=1, index=target.view(-1, 1))\n",
    "        res += torch.log(torch.exp(output).sum(dim=1).view(-1, 1))\n",
    "        res = res.mean() * distance.mean()\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import _reduction as _Reduction\n",
    "from torch import Tensor\n",
    "class _Loss(nn.Module):\n",
    "    reduction: str\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction: str = _Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "class _WeightedLoss(_Loss):\n",
    "    def __init__(self, weight: Optional[Tensor] = None, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(_WeightedLoss, self).__init__(size_average, reduce, reduction)\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.weight: Optional[Tensor]\n",
    "class CELoss(_WeightedLoss): # 注意继承 nn.Module\n",
    "    __constants__ = ['ignore_index', 'reduction', 'label_smoothing']\n",
    "    ignore_index: int\n",
    "    label_smoothing: float\n",
    "\n",
    "    def __init__(self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100,\n",
    "                 reduce=None, reduction: str = 'mean', label_smoothing: float = 0.0) -> None:\n",
    "        super(CELoss, self).__init__(weight, size_average, reduce, reduction)\n",
    "        self.ignore_index = ignore_index\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor, distance) -> Tensor:\n",
    "        return distance.mean() * F.cross_entropy(input, target, weight=self.weight,\n",
    "                               ignore_index=self.ignore_index, reduction=self.reduction,\n",
    "                               label_smoothing=self.label_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MyNet().to(device)\n",
    "# define cost/loss & optimizer\n",
    "# Softmax is internally computed.\n",
    "# criterion = CELoss().to(device)\n",
    "criterion = CELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.831959069 trainacc: 50.0 testacc: 50.0 test_loss: 4.164531588554382\n",
      "Epoch: 0002 cost = 0.831540048 trainacc: 51.45985401459854 testacc: 50.0 test_loss: 4.160634994506836\n",
      "Epoch: 0003 cost = 0.831135094 trainacc: 56.934306569343065 testacc: 47.61904761904762 test_loss: 4.159648776054382\n",
      "Epoch: 0004 cost = 0.830775499 trainacc: 61.67883211678832 testacc: 45.23809523809524 test_loss: 4.159980595111847\n",
      "Epoch: 0005 cost = 0.830538511 trainacc: 62.77372262773723 testacc: 45.23809523809524 test_loss: 4.160377323627472\n",
      "Epoch: 0006 cost = 0.830294251 trainacc: 64.5985401459854 testacc: 47.61904761904762 test_loss: 4.160988867282867\n",
      "Epoch: 0007 cost = 0.829936802 trainacc: 64.5985401459854 testacc: 47.61904761904762 test_loss: 4.1615095138549805\n",
      "Epoch: 0008 cost = 0.829776943 trainacc: 67.51824817518248 testacc: 47.61904761904762 test_loss: 4.161812841892242\n",
      "Epoch: 0009 cost = 0.829425871 trainacc: 68.24817518248175 testacc: 42.857142857142854 test_loss: 4.162522912025452\n",
      "Epoch: 0010 cost = 0.829128385 trainacc: 68.24817518248175 testacc: 45.23809523809524 test_loss: 4.162872612476349\n",
      "Epoch: 0011 cost = 0.828981221 trainacc: 69.34306569343066 testacc: 45.23809523809524 test_loss: 4.163657128810883\n",
      "Epoch: 0012 cost = 0.828695059 trainacc: 70.43795620437956 testacc: 45.23809523809524 test_loss: 4.163565754890442\n",
      "Epoch: 0013 cost = 0.828344464 trainacc: 70.07299270072993 testacc: 45.23809523809524 test_loss: 4.163559436798096\n",
      "Epoch: 0014 cost = 0.828191876 trainacc: 71.53284671532846 testacc: 40.476190476190474 test_loss: 4.163852035999298\n",
      "Epoch: 0015 cost = 0.827792168 trainacc: 71.53284671532846 testacc: 42.857142857142854 test_loss: 4.165005564689636\n",
      "Epoch: 0016 cost = 0.827504396 trainacc: 71.16788321167883 testacc: 45.23809523809524 test_loss: 4.165692090988159\n",
      "Epoch: 0017 cost = 0.827155411 trainacc: 73.72262773722628 testacc: 42.857142857142854 test_loss: 4.165973424911499\n",
      "Epoch: 0018 cost = 0.826913178 trainacc: 73.72262773722628 testacc: 40.476190476190474 test_loss: 4.165753364562988\n",
      "Epoch: 0019 cost = 0.826616645 trainacc: 74.45255474452554 testacc: 40.476190476190474 test_loss: 4.167362213134766\n",
      "Epoch: 0020 cost = 0.826553464 trainacc: 74.45255474452554 testacc: 40.476190476190474 test_loss: 4.167014122009277\n",
      "Epoch: 0021 cost = 0.826139688 trainacc: 75.18248175182482 testacc: 40.476190476190474 test_loss: 4.168121695518494\n",
      "Epoch: 0022 cost = 0.825682282 trainacc: 76.27737226277372 testacc: 40.476190476190474 test_loss: 4.168146550655365\n",
      "Epoch: 0023 cost = 0.825274110 trainacc: 76.64233576642336 testacc: 38.095238095238095 test_loss: 4.168938517570496\n",
      "Epoch: 0024 cost = 0.824994206 trainacc: 78.46715328467154 testacc: 35.714285714285715 test_loss: 4.168554067611694\n",
      "Epoch: 0025 cost = 0.824585021 trainacc: 78.10218978102189 testacc: 38.095238095238095 test_loss: 4.169658184051514\n",
      "Epoch: 0026 cost = 0.824291229 trainacc: 77.37226277372262 testacc: 38.095238095238095 test_loss: 4.172132313251495\n",
      "Epoch: 0027 cost = 0.824068964 trainacc: 76.64233576642336 testacc: 42.857142857142854 test_loss: 4.173112273216248\n",
      "Epoch: 0028 cost = 0.823383510 trainacc: 79.1970802919708 testacc: 38.095238095238095 test_loss: 4.172357738018036\n",
      "Epoch: 0029 cost = 0.823107898 trainacc: 80.65693430656934 testacc: 38.095238095238095 test_loss: 4.173343300819397\n",
      "Epoch: 0030 cost = 0.822699428 trainacc: 81.02189781021897 testacc: 33.333333333333336 test_loss: 4.1736984848976135\n",
      "Epoch: 0031 cost = 0.822212577 trainacc: 83.21167883211679 testacc: 30.952380952380953 test_loss: 4.17370468378067\n",
      "Epoch: 0032 cost = 0.821723282 trainacc: 82.11678832116789 testacc: 33.333333333333336 test_loss: 4.1749982833862305\n",
      "Epoch: 0033 cost = 0.821563303 trainacc: 80.2919708029197 testacc: 38.095238095238095 test_loss: 4.1780476570129395\n",
      "Epoch: 0034 cost = 0.820984840 trainacc: 82.84671532846716 testacc: 38.095238095238095 test_loss: 4.177272439002991\n",
      "Epoch: 0035 cost = 0.820492864 trainacc: 83.21167883211679 testacc: 35.714285714285715 test_loss: 4.17725545167923\n",
      "Epoch: 0036 cost = 0.819975495 trainacc: 83.21167883211679 testacc: 38.095238095238095 test_loss: 4.179214358329773\n",
      "Epoch: 0037 cost = 0.819836795 trainacc: 83.21167883211679 testacc: 38.095238095238095 test_loss: 4.1802403926849365\n",
      "Epoch: 0038 cost = 0.819119692 trainacc: 83.21167883211679 testacc: 42.857142857142854 test_loss: 4.181031227111816\n",
      "Epoch: 0039 cost = 0.818468630 trainacc: 84.30656934306569 testacc: 42.857142857142854 test_loss: 4.181391477584839\n",
      "Epoch: 0040 cost = 0.818116367 trainacc: 83.21167883211679 testacc: 42.857142857142854 test_loss: 4.1830567717552185\n",
      "Epoch: 0041 cost = 0.817668438 trainacc: 85.76642335766424 testacc: 38.095238095238095 test_loss: 4.1824631690979\n",
      "Epoch: 0042 cost = 0.817259490 trainacc: 86.86131386861314 testacc: 35.714285714285715 test_loss: 4.182992100715637\n",
      "Epoch: 0043 cost = 0.816789627 trainacc: 87.22627737226277 testacc: 35.714285714285715 test_loss: 4.18393874168396\n",
      "Epoch: 0044 cost = 0.815836608 trainacc: 87.22627737226277 testacc: 33.333333333333336 test_loss: 4.183643043041229\n",
      "Epoch: 0045 cost = 0.815500379 trainacc: 88.32116788321167 testacc: 30.952380952380953 test_loss: 4.183914840221405\n",
      "Epoch: 0046 cost = 0.814481735 trainacc: 89.05109489051095 testacc: 30.952380952380953 test_loss: 4.182810723781586\n",
      "Epoch: 0047 cost = 0.813952744 trainacc: 87.22627737226277 testacc: 33.333333333333336 test_loss: 4.1821205615997314\n",
      "Epoch: 0048 cost = 0.813601017 trainacc: 91.24087591240875 testacc: 28.571428571428573 test_loss: 4.184392750263214\n",
      "Epoch: 0049 cost = 0.812538445 trainacc: 89.05109489051095 testacc: 28.571428571428573 test_loss: 4.186647355556488\n",
      "Epoch: 0050 cost = 0.812499583 trainacc: 90.51094890510949 testacc: 30.952380952380953 test_loss: 4.188040494918823\n",
      "Epoch: 0051 cost = 0.811356187 trainacc: 87.5912408759124 testacc: 40.476190476190474 test_loss: 4.192073285579681\n",
      "Epoch: 0052 cost = 0.810573339 trainacc: 91.24087591240875 testacc: 26.19047619047619 test_loss: 4.190345644950867\n",
      "Epoch: 0053 cost = 0.809885144 trainacc: 91.24087591240875 testacc: 30.952380952380953 test_loss: 4.189223527908325\n",
      "Epoch: 0054 cost = 0.809444785 trainacc: 91.97080291970804 testacc: 28.571428571428573 test_loss: 4.192204654216766\n",
      "Epoch: 0055 cost = 0.808171868 trainacc: 91.24087591240875 testacc: 35.714285714285715 test_loss: 4.189908444881439\n",
      "Epoch: 0056 cost = 0.807468057 trainacc: 88.32116788321167 testacc: 40.476190476190474 test_loss: 4.198503255844116\n",
      "Epoch: 0057 cost = 0.806825757 trainacc: 91.24087591240875 testacc: 26.19047619047619 test_loss: 4.197570979595184\n",
      "Epoch: 0058 cost = 0.805495620 trainacc: 91.24087591240875 testacc: 28.571428571428573 test_loss: 4.198070764541626\n",
      "Epoch: 0059 cost = 0.804603577 trainacc: 91.60583941605839 testacc: 30.952380952380953 test_loss: 4.197094619274139\n",
      "Epoch: 0060 cost = 0.803573370 trainacc: 91.97080291970804 testacc: 28.571428571428573 test_loss: 4.20091438293457\n",
      "Epoch: 0061 cost = 0.802396894 trainacc: 93.06569343065694 testacc: 30.952380952380953 test_loss: 4.1979475021362305\n",
      "Epoch: 0062 cost = 0.800852239 trainacc: 92.33576642335767 testacc: 28.571428571428573 test_loss: 4.203266084194183\n",
      "Epoch: 0063 cost = 0.800210297 trainacc: 93.06569343065694 testacc: 33.333333333333336 test_loss: 4.198889493942261\n",
      "Epoch: 0064 cost = 0.798505604 trainacc: 93.7956204379562 testacc: 33.333333333333336 test_loss: 4.2086076736450195\n",
      "Epoch: 0065 cost = 0.797133625 trainacc: 92.33576642335767 testacc: 26.19047619047619 test_loss: 4.207636892795563\n",
      "Epoch: 0066 cost = 0.796065152 trainacc: 93.43065693430657 testacc: 30.952380952380953 test_loss: 4.205927610397339\n",
      "Epoch: 0067 cost = 0.794719458 trainacc: 92.7007299270073 testacc: 42.857142857142854 test_loss: 4.216297447681427\n",
      "Epoch: 0068 cost = 0.794384897 trainacc: 93.43065693430657 testacc: 30.952380952380953 test_loss: 4.211475253105164\n",
      "Epoch: 0069 cost = 0.791610241 trainacc: 94.8905109489051 testacc: 30.952380952380953 test_loss: 4.217402160167694\n",
      "Epoch: 0070 cost = 0.790420413 trainacc: 94.16058394160584 testacc: 30.952380952380953 test_loss: 4.213730037212372\n",
      "Epoch: 0071 cost = 0.789994597 trainacc: 95.25547445255475 testacc: 30.952380952380953 test_loss: 4.220945179462433\n",
      "Epoch: 0072 cost = 0.787648082 trainacc: 94.8905109489051 testacc: 30.952380952380953 test_loss: 4.217158496379852\n",
      "Epoch: 0073 cost = 0.786405802 trainacc: 92.7007299270073 testacc: 45.23809523809524 test_loss: 4.230912685394287\n",
      "Epoch: 0074 cost = 0.784677804 trainacc: 95.25547445255475 testacc: 33.333333333333336 test_loss: 4.219218075275421\n",
      "Epoch: 0075 cost = 0.782625556 trainacc: 94.8905109489051 testacc: 30.952380952380953 test_loss: 4.228595733642578\n",
      "Epoch: 0076 cost = 0.780834019 trainacc: 92.33576642335767 testacc: 28.571428571428573 test_loss: 4.216763973236084\n",
      "Epoch: 0077 cost = 0.778439343 trainacc: 97.08029197080292 testacc: 38.095238095238095 test_loss: 4.234417736530304\n",
      "Epoch: 0078 cost = 0.776524186 trainacc: 92.7007299270073 testacc: 45.23809523809524 test_loss: 4.2473264336586\n",
      "Epoch: 0079 cost = 0.775252581 trainacc: 97.44525547445255 testacc: 40.476190476190474 test_loss: 4.239966094493866\n",
      "Epoch: 0080 cost = 0.773182631 trainacc: 97.44525547445255 testacc: 30.952380952380953 test_loss: 4.239032447338104\n",
      "Epoch: 0081 cost = 0.770021379 trainacc: 97.08029197080292 testacc: 28.571428571428573 test_loss: 4.238956868648529\n",
      "Epoch: 0082 cost = 0.768328846 trainacc: 96.71532846715328 testacc: 28.571428571428573 test_loss: 4.2411205768585205\n",
      "Epoch: 0083 cost = 0.764274061 trainacc: 96.71532846715328 testacc: 28.571428571428573 test_loss: 4.237884998321533\n",
      "Epoch: 0084 cost = 0.764142454 trainacc: 93.7956204379562 testacc: 28.571428571428573 test_loss: 4.231966495513916\n",
      "Epoch: 0085 cost = 0.758971453 trainacc: 97.08029197080292 testacc: 28.571428571428573 test_loss: 4.242249846458435\n",
      "Epoch: 0086 cost = 0.757591426 trainacc: 98.17518248175182 testacc: 38.095238095238095 test_loss: 4.267722904682159\n",
      "Epoch: 0087 cost = 0.754355073 trainacc: 89.41605839416059 testacc: 33.333333333333336 test_loss: 4.231749713420868\n",
      "Epoch: 0088 cost = 0.752230048 trainacc: 98.17518248175182 testacc: 30.952380952380953 test_loss: 4.26530921459198\n",
      "Epoch: 0089 cost = 0.748025119 trainacc: 99.27007299270073 testacc: 40.476190476190474 test_loss: 4.279621958732605\n",
      "Epoch: 0090 cost = 0.744577527 trainacc: 99.27007299270073 testacc: 40.476190476190474 test_loss: 4.285102069377899\n",
      "Epoch: 0091 cost = 0.741931617 trainacc: 98.17518248175182 testacc: 28.571428571428573 test_loss: 4.27070814371109\n",
      "Epoch: 0092 cost = 0.740420222 trainacc: 98.17518248175182 testacc: 26.19047619047619 test_loss: 4.272644102573395\n",
      "Epoch: 0093 cost = 0.733892441 trainacc: 96.35036496350365 testacc: 28.571428571428573 test_loss: 4.269435942173004\n",
      "Epoch: 0094 cost = 0.734222889 trainacc: 98.9051094890511 testacc: 33.333333333333336 test_loss: 4.297692656517029\n",
      "Epoch: 0095 cost = 0.727588236 trainacc: 99.27007299270073 testacc: 40.476190476190474 test_loss: 4.3180161118507385\n",
      "Epoch: 0096 cost = 0.722357631 trainacc: 97.08029197080292 testacc: 28.571428571428573 test_loss: 4.281213104724884\n",
      "Epoch: 0097 cost = 0.715073407 trainacc: 98.17518248175182 testacc: 26.19047619047619 test_loss: 4.301088213920593\n",
      "Epoch: 0098 cost = 0.712622762 trainacc: 98.17518248175182 testacc: 26.19047619047619 test_loss: 4.308266520500183\n",
      "Epoch: 0099 cost = 0.704697907 trainacc: 99.27007299270073 testacc: 30.952380952380953 test_loss: 4.329374670982361\n",
      "Epoch: 0100 cost = 0.700001478 trainacc: 99.27007299270073 testacc: 35.714285714285715 test_loss: 4.370671093463898\n",
      "Epoch: 0101 cost = 0.696306705 trainacc: 99.27007299270073 testacc: 38.095238095238095 test_loss: 4.381342589855194\n",
      "Epoch: 0102 cost = 0.689439297 trainacc: 98.17518248175182 testacc: 26.19047619047619 test_loss: 4.336169362068176\n",
      "Epoch: 0103 cost = 0.682615161 trainacc: 99.63503649635037 testacc: 28.571428571428573 test_loss: 4.368652105331421\n",
      "Epoch: 0104 cost = 0.673511386 trainacc: 99.63503649635037 testacc: 28.571428571428573 test_loss: 4.371542155742645\n",
      "Epoch: 0105 cost = 0.664251387 trainacc: 100.0 testacc: 38.095238095238095 test_loss: 4.415484428405762\n",
      "Epoch: 0106 cost = 0.659034431 trainacc: 97.08029197080292 testacc: 28.571428571428573 test_loss: 4.348304569721222\n",
      "Epoch: 0107 cost = 0.657648921 trainacc: 99.63503649635037 testacc: 26.19047619047619 test_loss: 4.411377966403961\n",
      "Epoch: 0108 cost = 0.642231584 trainacc: 98.54014598540147 testacc: 26.19047619047619 test_loss: 4.410309612751007\n",
      "Epoch: 0109 cost = 0.630199134 trainacc: 100.0 testacc: 38.095238095238095 test_loss: 4.482845604419708\n",
      "Epoch: 0110 cost = 0.623406410 trainacc: 99.63503649635037 testacc: 26.19047619047619 test_loss: 4.450346529483795\n",
      "Epoch: 0111 cost = 0.611163557 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 4.48530912399292\n",
      "Epoch: 0112 cost = 0.595032811 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 4.494930326938629\n",
      "Epoch: 0113 cost = 0.587300718 trainacc: 98.17518248175182 testacc: 28.571428571428573 test_loss: 4.464787006378174\n",
      "Epoch: 0114 cost = 0.575046718 trainacc: 99.27007299270073 testacc: 30.952380952380953 test_loss: 4.486958980560303\n",
      "Epoch: 0115 cost = 0.575029194 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 4.544928669929504\n",
      "Epoch: 0116 cost = 0.547607541 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 4.630513310432434\n",
      "Epoch: 0117 cost = 0.538501143 trainacc: 98.9051094890511 testacc: 47.61904761904762 test_loss: 4.7960129380226135\n",
      "Epoch: 0118 cost = 0.523266971 trainacc: 99.63503649635037 testacc: 30.952380952380953 test_loss: 4.62028968334198\n",
      "Epoch: 0119 cost = 0.504520714 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 4.689923286437988\n",
      "Epoch: 0120 cost = 0.488894135 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 4.675925314426422\n",
      "Epoch: 0121 cost = 0.467844874 trainacc: 99.27007299270073 testacc: 45.23809523809524 test_loss: 4.951054751873016\n",
      "Epoch: 0122 cost = 0.458119214 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 4.744329988956451\n",
      "Epoch: 0123 cost = 0.435005248 trainacc: 99.63503649635037 testacc: 30.952380952380953 test_loss: 4.765826344490051\n",
      "Epoch: 0124 cost = 0.418759674 trainacc: 100.0 testacc: 40.476190476190474 test_loss: 5.085316061973572\n",
      "Epoch: 0125 cost = 0.395577997 trainacc: 100.0 testacc: 38.095238095238095 test_loss: 5.068693161010742\n",
      "Epoch: 0126 cost = 0.375899285 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 4.940811991691589\n",
      "Epoch: 0127 cost = 0.357902139 trainacc: 100.0 testacc: 38.095238095238095 test_loss: 5.2441086769104\n",
      "Epoch: 0128 cost = 0.343140513 trainacc: 100.0 testacc: 23.80952380952381 test_loss: 5.179904401302338\n",
      "Epoch: 0129 cost = 0.328067720 trainacc: 100.0 testacc: 40.476190476190474 test_loss: 5.305948495864868\n",
      "Epoch: 0130 cost = 0.305707932 trainacc: 100.0 testacc: 23.80952380952381 test_loss: 5.331389605998993\n",
      "Epoch: 0131 cost = 0.281904817 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 5.307419717311859\n",
      "Epoch: 0132 cost = 0.265625805 trainacc: 100.0 testacc: 38.095238095238095 test_loss: 5.508323907852173\n",
      "Epoch: 0133 cost = 0.245393202 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 5.553364992141724\n",
      "Epoch: 0134 cost = 0.233998463 trainacc: 100.0 testacc: 35.714285714285715 test_loss: 5.821658372879028\n",
      "Epoch: 0135 cost = 0.224527359 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 5.647502303123474\n",
      "Epoch: 0136 cost = 0.196305841 trainacc: 100.0 testacc: 35.714285714285715 test_loss: 5.844966888427734\n",
      "Epoch: 0137 cost = 0.183227107 trainacc: 100.0 testacc: 23.80952380952381 test_loss: 5.825352370738983\n",
      "Epoch: 0138 cost = 0.169977054 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 6.058339536190033\n",
      "Epoch: 0139 cost = 0.154004872 trainacc: 100.0 testacc: 23.80952380952381 test_loss: 5.9971078634262085\n",
      "Epoch: 0140 cost = 0.143737867 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 6.074391186237335\n",
      "Epoch: 0141 cost = 0.137183815 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 6.26742148399353\n",
      "Epoch: 0142 cost = 0.125533581 trainacc: 100.0 testacc: 19.047619047619047 test_loss: 6.411642670631409\n",
      "Epoch: 0143 cost = 0.116033398 trainacc: 100.0 testacc: 23.80952380952381 test_loss: 6.364998042583466\n",
      "Epoch: 0144 cost = 0.107475966 trainacc: 100.0 testacc: 23.80952380952381 test_loss: 6.428946495056152\n",
      "Epoch: 0145 cost = 0.100386009 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 6.6903722286224365\n",
      "Epoch: 0146 cost = 0.094894856 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 6.7554901242256165\n",
      "Epoch: 0147 cost = 0.087065563 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 6.779879570007324\n",
      "Epoch: 0148 cost = 0.081052549 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 6.89704817533493\n",
      "Epoch: 0149 cost = 0.076830998 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 6.889424800872803\n",
      "Epoch: 0150 cost = 0.071028963 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 7.009391248226166\n",
      "Epoch: 0151 cost = 0.067290917 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 7.091512739658356\n",
      "Epoch: 0152 cost = 0.063304484 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 7.166693389415741\n",
      "Epoch: 0153 cost = 0.059909292 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 7.272196471691132\n",
      "Epoch: 0154 cost = 0.055868469 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 7.2357776165008545\n",
      "Epoch: 0155 cost = 0.052998252 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 7.47723263502121\n",
      "Epoch: 0156 cost = 0.050352398 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 7.402902066707611\n",
      "Epoch: 0157 cost = 0.047903530 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 7.4064584374427795\n",
      "Epoch: 0158 cost = 0.045540404 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 7.575972735881805\n",
      "Epoch: 0159 cost = 0.042851061 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 7.61098176240921\n",
      "Epoch: 0160 cost = 0.040823102 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 7.599925518035889\n",
      "Epoch: 0161 cost = 0.039323803 trainacc: 100.0 testacc: 23.80952380952381 test_loss: 7.738987624645233\n",
      "Epoch: 0162 cost = 0.037159961 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 7.826114654541016\n",
      "Epoch: 0163 cost = 0.035549480 trainacc: 100.0 testacc: 33.333333333333336 test_loss: 8.01204138994217\n",
      "Epoch: 0164 cost = 0.034563962 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 7.918681740760803\n",
      "Epoch: 0165 cost = 0.032688323 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 7.987354874610901\n",
      "Epoch: 0166 cost = 0.031322006 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 8.03131252527237\n",
      "Epoch: 0167 cost = 0.029835416 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 8.021114587783813\n",
      "Epoch: 0168 cost = 0.028689001 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 8.199897110462189\n",
      "Epoch: 0169 cost = 0.027517132 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 8.242904782295227\n",
      "Epoch: 0170 cost = 0.026561100 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 8.254134833812714\n",
      "Epoch: 0171 cost = 0.025513289 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 8.233986377716064\n",
      "Epoch: 0172 cost = 0.024760747 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 8.270854234695435\n",
      "Epoch: 0173 cost = 0.023648653 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 8.322850704193115\n",
      "Epoch: 0174 cost = 0.022859680 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 8.396344900131226\n",
      "Epoch: 0175 cost = 0.022094766 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 8.453795433044434\n",
      "Epoch: 0176 cost = 0.021348787 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 8.512801051139832\n",
      "Epoch: 0177 cost = 0.020625884 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 8.580095171928406\n",
      "Epoch: 0178 cost = 0.020002104 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 8.580342173576355\n",
      "Epoch: 0179 cost = 0.019399909 trainacc: 100.0 testacc: 23.80952380952381 test_loss: 8.559622645378113\n",
      "Epoch: 0180 cost = 0.018625427 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 8.609801411628723\n",
      "Epoch: 0181 cost = 0.018200461 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 8.749147415161133\n",
      "Epoch: 0182 cost = 0.017567920 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 8.761886954307556\n",
      "Epoch: 0183 cost = 0.017002141 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 8.82712733745575\n",
      "Epoch: 0184 cost = 0.016495336 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 8.785422682762146\n",
      "Epoch: 0185 cost = 0.016085727 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 8.849452018737793\n",
      "Epoch: 0186 cost = 0.015657118 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 8.922455191612244\n",
      "Epoch: 0187 cost = 0.015161613 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 8.894428253173828\n",
      "Epoch: 0188 cost = 0.014782240 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 9.034315586090088\n",
      "Epoch: 0189 cost = 0.014434929 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 9.010730981826782\n",
      "Epoch: 0190 cost = 0.014046490 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 9.053725361824036\n",
      "Epoch: 0191 cost = 0.013738332 trainacc: 100.0 testacc: 21.428571428571427 test_loss: 9.009419798851013\n",
      "Epoch: 0192 cost = 0.013386628 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 9.087826371192932\n",
      "Epoch: 0193 cost = 0.012954801 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 9.193508863449097\n",
      "Epoch: 0194 cost = 0.012691111 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 9.275377631187439\n",
      "Epoch: 0195 cost = 0.012403260 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 9.210302352905273\n",
      "Epoch: 0196 cost = 0.012100622 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 9.258354783058167\n",
      "Epoch: 0197 cost = 0.011805370 trainacc: 100.0 testacc: 30.952380952380953 test_loss: 9.339958906173706\n",
      "Epoch: 0198 cost = 0.011524016 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 9.285973906517029\n",
      "Epoch: 0199 cost = 0.011298208 trainacc: 100.0 testacc: 26.19047619047619 test_loss: 9.326423645019531\n",
      "Epoch: 0200 cost = 0.011027446 trainacc: 100.0 testacc: 28.571428571428573 test_loss: 9.35055947303772\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader_train)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    avg_cost = 0\n",
    "\n",
    "    # for X, Y in data_loader_train:\n",
    "    #     X = torch.autograd.Variable(X).to(device).float()\n",
    "    #     Y = Y.to(device)\n",
    "\n",
    "    #     optimizer.zero_grad()\n",
    "    #     hypothesis = model(X)\n",
    "    #     cost = criterion(hypothesis, Y)\n",
    "    #     cost.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    #     avg_cost += cost / total_batch\n",
    "    # model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader_train):\n",
    "        distance = targets[:,1].to(device).float()\n",
    "        targets = targets[:,0]\n",
    "        inputs, targets = inputs.to(device).float(), targets.to(device).to(torch.int64)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets, distance)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        avg_cost += loss / total_batch\n",
    "\n",
    "    \n",
    "    # if avg_cost < 0.1:\n",
    "    #     break\n",
    "    model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for X_test, Y_test in data_loader_test:\n",
    "    #         X_test = X_test.to(device).float()\n",
    "    #         Y_test = Y_test.to(device)\n",
    "\n",
    "    #     prediction = model(X_test)\n",
    "    #     correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    #     accuracy = correct_prediction.float().mean()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader_train):\n",
    "            distance = targets[:,1].to(device).float()\n",
    "            targets = targets[:,0]\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device).to(torch.int64)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets, distance)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    trainacc = 100.*correct/total\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader_test):\n",
    "            distance = targets[:,1].to(device).float()\n",
    "            targets = targets[:,0]\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device).to(torch.int64)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets, distance)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    testacc = 100.*correct/total\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost), 'trainacc:', trainacc, 'testacc:', testacc, 'test_loss:', test_loss)\n",
    "\n",
    "print('Learning finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1], device='cuda:0') tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0], device='cuda:0')\n",
      "tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1], device='cuda:0') tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1], device='cuda:0') tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1], device='cuda:0')\n",
      "tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0], device='cuda:0') tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0], device='cuda:0')\n",
      "tensor([1, 1], device='cuda:0') tensor([0, 0], device='cuda:0')\n",
      "trainacc: 100.0 testacc: 28.571428571428573\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader_test):\n",
    "        distance = targets[:,1].to(device).float()\n",
    "        targets = targets[:,0]\n",
    "        inputs, targets = inputs.to(device).float(), targets.to(device).to(torch.int64)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(torch.argmax(outputs, 1), targets)\n",
    "    testacc = 100.*correct/total\n",
    "    print('trainacc:', trainacc, 'testacc:', testacc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d28dcb1746f058774bc7e3103f8fd674f9a46f28636659ba6a6fdaf0b17e1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
