{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# input_data shape\n",
    "Input: (batch_size, in_channel, width, height)\n",
    "# conv layer\n",
    "class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "input: (Batch_size, C_in, H_in, W_in)\n",
    "output: (Batch_size, C_out, H_out, W_out)\n",
    "\n",
    "weight(tensor): (out_channels, in_channels,kernel_size)\n",
    "bias(tensor): (out_channel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ripser import Rips\n",
    "from persim import PersistenceImager\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import persim\n",
    "\n",
    "import re\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(111)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 500\n",
    "batchsize = 25\n",
    "testbatchsize = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = os.listdir(\"./metric\")\n",
    "# data = []\n",
    "# a = []\n",
    "# for metric in dir:\n",
    "#     a = re.findall(\"\\d+\\.?\\d*\", metric)\n",
    "#     data.append([np.loadtxt(\"./metric/\"+metric), metric])\n",
    "#     # print(a[-2])\n",
    "#     print(metric[-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_dataset(Dataset):\n",
    "    def __init__(self, train):\n",
    "        super().__init__()\n",
    "        dirtrain = os.listdir(\"./grids_trainset_vgg\")\n",
    "        dirtest = os.listdir(\"./grids_testset_vgg\")\n",
    "        data = []\n",
    "        \n",
    "        if train:\n",
    "            dir = dirtrain\n",
    "            path = \"./grids_trainset_vgg/\"\n",
    "        else:\n",
    "            dir = dirtest\n",
    "            path = \"./grids_testset_vgg/\"\n",
    "\n",
    "        T_normal = []\n",
    "        T_attack = []\n",
    "        for metric in dir[0:5]:\n",
    "            dgm = []\n",
    "            grid = pd.DataFrame(np.loadtxt(path+metric))\n",
    "            for i in range(grid.shape[0]):\n",
    "                for j in range(grid.shape[1]):\n",
    "                    if grid[i][j] > 0:\n",
    "                        dgm.append([i,j])\n",
    "            T_normal.append(dgm)\n",
    "        for metric in dir[-5:]:\n",
    "            dgm = []\n",
    "            grid = pd.DataFrame(np.loadtxt(path+metric))\n",
    "            for i in range(grid.shape[0]):\n",
    "                for j in range(grid.shape[1]):\n",
    "                    if grid[i][j] > 0:\n",
    "                        dgm.append([i,j])\n",
    "            T_attack.append(dgm)\n",
    "\n",
    "        print(len(T_attack), len(T_normal))\n",
    "        \n",
    "        sample = []\n",
    "        distance = 1\n",
    "        for metric in dir:\n",
    "            sample.append([np.loadtxt(path+metric), metric])\n",
    "            label = int(metric[0:len(\"h_vr_metric-single-VGG16_CIFAR=VGG16_CIFAR_attack\")]== \"h_vr_metric-single-VGG16_CIFAR=VGG16_CIFAR_attack\")\n",
    "            # if (label == 0):\n",
    "            #     distance_bottleneck, matching = persim.bottleneck(sample[len(sample)-1][0], T_normal[len(sample)-1], matching=True)\n",
    "            # else:\n",
    "            #     distance_bottleneck, matching = persim.bottleneck(sample[len(sample)-1][0], T_attack[len(sample)-1], matching=True)\n",
    "            # distance += distance_bottleneck\n",
    "            if len(sample) == 5:\n",
    "                res = cv2.merge([i[0] for i in sample])\n",
    "                res = np.transpose(res,(2,0,1))\n",
    "                # data.append([res, np.array((label,1/(1+math.exp(math.log(distance)))))])\n",
    "                data.append([res, np.array((label,1))])\n",
    "                sample = []\n",
    "\n",
    "        self.x = [item[0] for item in data]\n",
    "        self.y = [item[1] for item in data]\n",
    "        # self.y = [int(re.findall(\"\\d+\\.?\\d*\", item[1])[0]) for item in data]\n",
    "        self.src,  self.trg = [], []\n",
    "        for i in range(len(data)):\n",
    "            self.src.append(self.x[i])\n",
    "            self.trg.append(self.y[i])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.trg[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    " # 或者return len(self.trg), src和trg长度一样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "5 5\n",
      "train\n",
      "0\n",
      "torch.Size([25, 5, 128, 128])\n",
      "tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]], dtype=torch.int32)\n",
      "1\n",
      "torch.Size([25, 5, 128, 128])\n",
      "tensor([[0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1]], dtype=torch.int32)\n",
      "test\n",
      "0\n",
      "torch.Size([10, 5, 128, 128])\n",
      "tensor([[0, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [0, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "data_tf = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5], [0.5])])\n",
    "     \n",
    "data_train = My_dataset(train=True)\n",
    "data_test = My_dataset(train=False)\n",
    "data_loader_train = DataLoader(data_train, batch_size=batchsize, shuffle=True)\n",
    "data_loader_test = DataLoader(data_test, batch_size=testbatchsize, shuffle=True)\n",
    "\n",
    "\n",
    "# i_batch的多少根据batch size和def __len__(self)返回的长度确定\n",
    "# batch_data返回的值根据def __getitem__(self, index)来确定\n",
    "# 对训练集：(不太清楚enumerate返回什么的时候就多print试试)\n",
    "print(\"train\")\n",
    "for i_batch, batch_data in enumerate(data_loader_train):\n",
    "    print(i_batch)  # 打印batch编号\n",
    "    print(batch_data[0].shape)  # 打印该batch里面src\n",
    "    print(batch_data[1])  # 打印该batch里面trg\n",
    "# # 对测试集：（下面的语句也可以）\n",
    "print(\"test\")\n",
    "for i_batch, batch_data in enumerate(data_loader_test):\n",
    "    print(i_batch)  # 打印batch编号\n",
    "    print(batch_data[0].shape)  # 打印该batch里面src\n",
    "    print(batch_data[1])  # 打印该batch里面trg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(5,64,3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(64,64,3,padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64,128,3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3,padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128,128, 3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 128, 3,padding=1)\n",
    "        self.conv7 = nn.Conv2d(128, 128, 1,padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv8 = nn.Conv2d(128, 256, 3,padding=1)\n",
    "        self.conv9 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(256, 256, 1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv11 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv13 = nn.Conv2d(512, 512, 1, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc14 = nn.Linear(512*7*7,1024)\n",
    "        self.drop1 = nn.Dropout2d()\n",
    "        self.fc15 = nn.Linear(1024,128)\n",
    "        self.drop2 = nn.Dropout2d()\n",
    "        self.fc16 = nn.Linear(128,2)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.conv11(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.pool5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu5(x)\n",
    "        # print(\" x shape \",x.size())\n",
    "        x = x.view(-1,512*7*7)\n",
    "        x = F.relu(self.fc14(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc15(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc16(x)\n",
    "\n",
    "        return x\n",
    "model = MyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss2(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    def forward(self, output, target):\n",
    "        res = -output.gather(dim=1, index=target.view(-1, 1))\n",
    "        res += torch.log(torch.exp(output).sum(dim=1).view(-1, 1))\n",
    "        res = res.mean()\n",
    "        return res\n",
    "    def forward(self, output, target, distance):\n",
    "        res = -output.gather(dim=1, index=target.view(-1, 1))\n",
    "        res += torch.log(torch.exp(output).sum(dim=1).view(-1, 1))\n",
    "        res = res.mean() * distance.mean()\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import _reduction as _Reduction\n",
    "from torch import Tensor\n",
    "class _Loss(nn.Module):\n",
    "    reduction: str\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction: str = _Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "class _WeightedLoss(_Loss):\n",
    "    def __init__(self, weight: Optional[Tensor] = None, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(_WeightedLoss, self).__init__(size_average, reduce, reduction)\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.weight: Optional[Tensor]\n",
    "class CELoss(_WeightedLoss): # 注意继承 nn.Module\n",
    "    __constants__ = ['ignore_index', 'reduction', 'label_smoothing']\n",
    "    ignore_index: int\n",
    "    label_smoothing: float\n",
    "\n",
    "    def __init__(self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100,\n",
    "                 reduce=None, reduction: str = 'mean', label_smoothing: float = 0.0) -> None:\n",
    "        super(CELoss, self).__init__(weight, size_average, reduce, reduction)\n",
    "        self.ignore_index = ignore_index\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor, distance) -> Tensor:\n",
    "        return distance.mean() * F.cross_entropy(input, target, weight=self.weight,\n",
    "                               ignore_index=self.ignore_index, reduction=self.reduction,\n",
    "                               label_smoothing=self.label_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "# Softmax is internally computed.\n",
    "# criterion = CELoss().to(device)\n",
    "criterion = CELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.682000160 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0002 cost = 0.706642091 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0003 cost = 0.708194315 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0004 cost = 0.707595527 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0005 cost = 0.693349481 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0006 cost = 0.686443090 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0007 cost = 0.688715816 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0008 cost = 0.692870140 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0009 cost = 0.700643182 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0010 cost = 0.690934777 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0011 cost = 0.685119748 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0012 cost = 0.707437515 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0013 cost = 0.676886797 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0014 cost = 0.709610701 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0015 cost = 0.704070687 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0016 cost = 0.719791889 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0017 cost = 0.708718181 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0018 cost = 0.686077237 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0019 cost = 0.701664686 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0020 cost = 0.695474327 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0021 cost = 0.698429167 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0022 cost = 0.693367362 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0023 cost = 0.723731041 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0024 cost = 0.700468898 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0025 cost = 0.681956410 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0026 cost = 0.693851829 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0027 cost = 0.679750741 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0028 cost = 0.681343973 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0029 cost = 0.696119070 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0030 cost = 0.705863953 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0031 cost = 0.689142585 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0032 cost = 0.698743939 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0033 cost = 0.708221197 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0034 cost = 0.710030198 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0035 cost = 0.709252834 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0036 cost = 0.695902705 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0037 cost = 0.707478166 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0038 cost = 0.704869628 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0039 cost = 0.695574224 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0040 cost = 0.681878924 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0041 cost = 0.688933313 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0042 cost = 0.697260499 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0043 cost = 0.691898346 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0044 cost = 0.710807085 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0045 cost = 0.696065903 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0046 cost = 0.678479910 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0047 cost = 0.713324666 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0048 cost = 0.697844982 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0049 cost = 0.693826377 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0050 cost = 0.710878491 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0051 cost = 0.682936549 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0052 cost = 0.693376780 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0053 cost = 0.698490202 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0054 cost = 0.683332860 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0055 cost = 0.690220118 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0056 cost = 0.711999178 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0057 cost = 0.707572937 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0058 cost = 0.689828753 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0059 cost = 0.705757022 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0060 cost = 0.674983025 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0061 cost = 0.694412112 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0062 cost = 0.692118585 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0063 cost = 0.711422145 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0064 cost = 0.687227070 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0065 cost = 0.698460817 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0066 cost = 0.694452882 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0067 cost = 0.678589106 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0068 cost = 0.686016679 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0069 cost = 0.699087977 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0070 cost = 0.693126440 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0071 cost = 0.673670530 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0072 cost = 0.690710783 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0073 cost = 0.701041102 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0074 cost = 0.696036220 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0075 cost = 0.692180514 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0076 cost = 0.688286662 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0077 cost = 0.704155087 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0078 cost = 0.701502919 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0079 cost = 0.706721902 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0080 cost = 0.699650347 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0081 cost = 0.688056350 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0082 cost = 0.699694037 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0083 cost = 0.712064743 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0084 cost = 0.705712318 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0085 cost = 0.718790650 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0086 cost = 0.687519073 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0087 cost = 0.680257082 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0088 cost = 0.699636698 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0089 cost = 0.694367051 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0090 cost = 0.687492788 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0091 cost = 0.702330291 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0092 cost = 0.703833103 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0093 cost = 0.690638185 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0094 cost = 0.683247030 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0095 cost = 0.689579844 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0096 cost = 0.703326106 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0097 cost = 0.688979566 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0098 cost = 0.686099052 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0099 cost = 0.684302151 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0100 cost = 0.700155139 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0101 cost = 0.714599788 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0102 cost = 0.710438251 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0103 cost = 0.697782040 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0104 cost = 0.710549772 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0105 cost = 0.689375997 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0106 cost = 0.715482652 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0107 cost = 0.694241405 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0108 cost = 0.681513548 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0109 cost = 0.703276038 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0110 cost = 0.696309984 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0111 cost = 0.703800321 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0112 cost = 0.711457610 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0113 cost = 0.697310567 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0114 cost = 0.700446486 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0115 cost = 0.693530262 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0116 cost = 0.693034410 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0117 cost = 0.693656802 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0118 cost = 0.684194505 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0119 cost = 0.694111288 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0120 cost = 0.689931154 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0121 cost = 0.697920263 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0122 cost = 0.678762078 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0123 cost = 0.704215527 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0124 cost = 0.716263652 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0125 cost = 0.705973625 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0126 cost = 0.701196969 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0127 cost = 0.686702490 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0128 cost = 0.674386859 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0129 cost = 0.704099417 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0130 cost = 0.695250154 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0131 cost = 0.685884356 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0132 cost = 0.689175308 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0133 cost = 0.715268075 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0134 cost = 0.706607699 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0135 cost = 0.713878453 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0136 cost = 0.679485202 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0137 cost = 0.722724795 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0138 cost = 0.704592705 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0139 cost = 0.707864344 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0140 cost = 0.685780406 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0141 cost = 0.688237071 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0142 cost = 0.692401946 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0143 cost = 0.682411730 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0144 cost = 0.707806587 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0145 cost = 0.701675653 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0146 cost = 0.701195598 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0147 cost = 0.704117894 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0148 cost = 0.695520937 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0149 cost = 0.694904387 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0150 cost = 0.711104155 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0151 cost = 0.693012953 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0152 cost = 0.695498347 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0153 cost = 0.720115662 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0154 cost = 0.695122659 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0155 cost = 0.713852286 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0156 cost = 0.709736586 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0157 cost = 0.707257509 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0158 cost = 0.680824399 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0159 cost = 0.689299583 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0160 cost = 0.700919390 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0161 cost = 0.688571453 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0162 cost = 0.685262680 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0163 cost = 0.698943496 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0164 cost = 0.689560771 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0165 cost = 0.699290872 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0166 cost = 0.695398450 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0167 cost = 0.701769352 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0168 cost = 0.701203704 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0169 cost = 0.679199815 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0170 cost = 0.696780205 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0171 cost = 0.703306317 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0172 cost = 0.712462544 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0173 cost = 0.697280467 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0174 cost = 0.702780306 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0175 cost = 0.693101883 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0176 cost = 0.707473397 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0177 cost = 0.708489418 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0178 cost = 0.721408129 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0179 cost = 0.705587745 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0180 cost = 0.686732173 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0181 cost = 0.701337099 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0182 cost = 0.685933590 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0183 cost = 0.687014699 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0184 cost = 0.692644477 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0185 cost = 0.690795422 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0186 cost = 0.707366467 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0187 cost = 0.704517365 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0188 cost = 0.687339008 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0189 cost = 0.694510579 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0190 cost = 0.694715261 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0191 cost = 0.699175239 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0192 cost = 0.714419603 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0193 cost = 0.688216925 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0194 cost = 0.704845428 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0195 cost = 0.714170992 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0196 cost = 0.705670059 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0197 cost = 0.702820241 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0198 cost = 0.696622610 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0199 cost = 0.685969234 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0200 cost = 0.694375277 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0201 cost = 0.714462638 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0202 cost = 0.706332862 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0203 cost = 0.714697599 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0204 cost = 0.694962502 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0205 cost = 0.699494004 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0206 cost = 0.683801889 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0207 cost = 0.691334367 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0208 cost = 0.697018743 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0209 cost = 0.697136760 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0210 cost = 0.701234102 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0211 cost = 0.691476703 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0212 cost = 0.699655235 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0213 cost = 0.687313080 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0214 cost = 0.691357017 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0215 cost = 0.704775751 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0216 cost = 0.705135345 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0217 cost = 0.708520412 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0218 cost = 0.705790043 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0219 cost = 0.711388350 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0220 cost = 0.709553003 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0221 cost = 0.682985306 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0222 cost = 0.704574287 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0223 cost = 0.700315952 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0224 cost = 0.694243073 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0225 cost = 0.704066396 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0226 cost = 0.711701035 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0227 cost = 0.724999487 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0228 cost = 0.697429061 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0229 cost = 0.679003060 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0230 cost = 0.719104230 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0231 cost = 0.713008642 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0232 cost = 0.686738610 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0233 cost = 0.700625718 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0234 cost = 0.703241110 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0235 cost = 0.691742897 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0236 cost = 0.698565960 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0237 cost = 0.687186420 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0238 cost = 0.703575730 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0239 cost = 0.688317418 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0240 cost = 0.698199391 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0241 cost = 0.682554960 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0242 cost = 0.707598090 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0243 cost = 0.700123429 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0244 cost = 0.702113748 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0245 cost = 0.689071178 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0246 cost = 0.677072287 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0247 cost = 0.703511357 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0248 cost = 0.680893421 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0249 cost = 0.697831154 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0250 cost = 0.711867929 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0251 cost = 0.720275760 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0252 cost = 0.690641880 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0253 cost = 0.699567914 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0254 cost = 0.690241933 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0255 cost = 0.691306472 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0256 cost = 0.715540290 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0257 cost = 0.713276267 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0258 cost = 0.705894589 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0259 cost = 0.691898346 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0260 cost = 0.691130996 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0261 cost = 0.689569950 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0262 cost = 0.713117361 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0263 cost = 0.709099770 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0264 cost = 0.701363146 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0265 cost = 0.684566259 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0266 cost = 0.714771390 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0267 cost = 0.700727642 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0268 cost = 0.678411424 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0269 cost = 0.692685246 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0270 cost = 0.693761110 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0271 cost = 0.702592373 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0272 cost = 0.718718648 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0273 cost = 0.699937224 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0274 cost = 0.715046823 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0275 cost = 0.697889388 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0276 cost = 0.695495725 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0277 cost = 0.701132059 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0278 cost = 0.683182359 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0279 cost = 0.718699813 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0280 cost = 0.690280080 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0281 cost = 0.698154688 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0282 cost = 0.696933687 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0283 cost = 0.702023447 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0284 cost = 0.706072986 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0285 cost = 0.707631111 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0286 cost = 0.712024033 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0287 cost = 0.696301579 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0288 cost = 0.685614645 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0289 cost = 0.690334320 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0290 cost = 0.697456956 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0291 cost = 0.692383647 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0292 cost = 0.683497667 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0293 cost = 0.695345283 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0294 cost = 0.690502763 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0295 cost = 0.699183106 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0296 cost = 0.698969126 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0297 cost = 0.702520251 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0298 cost = 0.715427279 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0299 cost = 0.697292805 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0300 cost = 0.702978075 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0301 cost = 0.693005800 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0302 cost = 0.700997353 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0303 cost = 0.687744856 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0304 cost = 0.686934948 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0305 cost = 0.707883596 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0306 cost = 0.715405464 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0307 cost = 0.711194158 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0308 cost = 0.684753120 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0309 cost = 0.691444755 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0310 cost = 0.708481312 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0311 cost = 0.711795628 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0312 cost = 0.715060234 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0313 cost = 0.694290400 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0314 cost = 0.699534655 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0315 cost = 0.680167854 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0316 cost = 0.673669815 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0317 cost = 0.701020956 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0318 cost = 0.701878667 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0319 cost = 0.686375976 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0320 cost = 0.690588593 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0321 cost = 0.689297557 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0322 cost = 0.688808203 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0323 cost = 0.681673050 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0324 cost = 0.711996496 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0325 cost = 0.697845101 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0326 cost = 0.701235175 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0327 cost = 0.683496594 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0328 cost = 0.724067390 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0329 cost = 0.683806181 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0330 cost = 0.711886406 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0331 cost = 0.685089409 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0332 cost = 0.708607078 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0333 cost = 0.703271747 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0334 cost = 0.691619754 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0335 cost = 0.693603992 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0336 cost = 0.692243338 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0337 cost = 0.698457778 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0338 cost = 0.696271002 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0339 cost = 0.696036816 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0340 cost = 0.701918781 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0341 cost = 0.717560887 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0342 cost = 0.706247211 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0343 cost = 0.690008998 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0344 cost = 0.686928093 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0345 cost = 0.703335941 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0346 cost = 0.699024558 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0347 cost = 0.708277225 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0348 cost = 0.686759889 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0349 cost = 0.699543715 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0350 cost = 0.689960599 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0351 cost = 0.696926117 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0352 cost = 0.698307991 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0353 cost = 0.691077173 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0354 cost = 0.689914584 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0355 cost = 0.678747535 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0356 cost = 0.708485961 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0357 cost = 0.711847246 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0358 cost = 0.719418883 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0359 cost = 0.696871996 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0360 cost = 0.691574454 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0361 cost = 0.693864167 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0362 cost = 0.704980731 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0363 cost = 0.700077057 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0364 cost = 0.708392739 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0365 cost = 0.695435941 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0366 cost = 0.685283899 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0367 cost = 0.695696533 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0368 cost = 0.688099980 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0369 cost = 0.683612227 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0370 cost = 0.691432118 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0371 cost = 0.682265759 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0372 cost = 0.696927488 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0373 cost = 0.701228261 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0374 cost = 0.707796693 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0375 cost = 0.685903013 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0376 cost = 0.713263810 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0377 cost = 0.701759219 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0378 cost = 0.690144241 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0379 cost = 0.691269457 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0380 cost = 0.706334710 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0381 cost = 0.706046581 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0382 cost = 0.712103963 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0383 cost = 0.721933484 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0384 cost = 0.689323723 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0385 cost = 0.690899611 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0386 cost = 0.708848417 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0387 cost = 0.700458348 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0388 cost = 0.695548296 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0389 cost = 0.686303377 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0390 cost = 0.702748895 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0391 cost = 0.701670408 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0392 cost = 0.692620039 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0393 cost = 0.702985823 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0394 cost = 0.712534606 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0395 cost = 0.722892523 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0396 cost = 0.703453004 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0397 cost = 0.707632899 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0398 cost = 0.704867721 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0399 cost = 0.683135808 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0400 cost = 0.694944382 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0401 cost = 0.703083754 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0402 cost = 0.686205924 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0403 cost = 0.698538423 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0404 cost = 0.693762898 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0405 cost = 0.682616115 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0406 cost = 0.701952934 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0407 cost = 0.713125944 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0408 cost = 0.703827739 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0409 cost = 0.693003416 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0410 cost = 0.678806543 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0411 cost = 0.691388488 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0412 cost = 0.712903023 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0413 cost = 0.711512208 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0414 cost = 0.693551302 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0415 cost = 0.700940728 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0416 cost = 0.693060398 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0417 cost = 0.696594000 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0418 cost = 0.698544025 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0419 cost = 0.704829931 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0420 cost = 0.696089983 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0421 cost = 0.692443728 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0422 cost = 0.695357919 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0423 cost = 0.687200248 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0424 cost = 0.695928216 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0425 cost = 0.713506043 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0426 cost = 0.693398952 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0427 cost = 0.699573159 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0428 cost = 0.713675857 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0429 cost = 0.698238552 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0430 cost = 0.699878216 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0431 cost = 0.694970727 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0432 cost = 0.705532193 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0433 cost = 0.703243613 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0434 cost = 0.688589156 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0435 cost = 0.717293084 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0436 cost = 0.708225250 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0437 cost = 0.681650579 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0438 cost = 0.713369370 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0439 cost = 0.683199763 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0440 cost = 0.690713286 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0441 cost = 0.691385150 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0442 cost = 0.712670565 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0443 cost = 0.689840555 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0444 cost = 0.718831539 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0445 cost = 0.708983779 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0446 cost = 0.707166314 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0447 cost = 0.687127650 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0448 cost = 0.688018084 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0449 cost = 0.690654159 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0450 cost = 0.710150898 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0451 cost = 0.697908759 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0452 cost = 0.699734449 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0453 cost = 0.697457790 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0454 cost = 0.672519505 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0455 cost = 0.684860170 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0456 cost = 0.711172700 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0457 cost = 0.699359357 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0458 cost = 0.699272573 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0459 cost = 0.693834901 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0460 cost = 0.691516995 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0461 cost = 0.699879885 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0462 cost = 0.700131476 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0463 cost = 0.684910178 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0464 cost = 0.702516675 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0465 cost = 0.695109606 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0466 cost = 0.702569842 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0467 cost = 0.722124338 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0468 cost = 0.696035862 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0469 cost = 0.699033320 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0470 cost = 0.690286875 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0471 cost = 0.693034291 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0472 cost = 0.688770056 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0473 cost = 0.672897398 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0474 cost = 0.701904595 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0475 cost = 0.708297014 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0476 cost = 0.705390453 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0477 cost = 0.705976427 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0478 cost = 0.696864069 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0479 cost = 0.688467503 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0480 cost = 0.716959000 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0481 cost = 0.716768682 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0482 cost = 0.695228398 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0483 cost = 0.692666531 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0484 cost = 0.697606444 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0485 cost = 0.683103800 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0486 cost = 0.705657840 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0487 cost = 0.680324316 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0488 cost = 0.676705539 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0489 cost = 0.677794099 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0490 cost = 0.686285555 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0491 cost = 0.705105305 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0492 cost = 0.697525620 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0493 cost = 0.698973596 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0494 cost = 0.682561755 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0495 cost = 0.701203465 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0496 cost = 0.692777216 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0497 cost = 0.718543530 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0498 cost = 0.700970769 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0499 cost = 0.698575854 trainacc: 50.0 testacc: 50.0\n",
      "Epoch: 0500 cost = 0.692294061 trainacc: 50.0 testacc: 50.0\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader_train)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    avg_cost = 0\n",
    "\n",
    "    # for X, Y in data_loader_train:\n",
    "    #     X = torch.autograd.Variable(X).to(device).float()\n",
    "    #     Y = Y.to(device)\n",
    "\n",
    "    #     optimizer.zero_grad()\n",
    "    #     hypothesis = model(X)\n",
    "    #     cost = criterion(hypothesis, Y)\n",
    "    #     cost.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    #     avg_cost += cost / total_batch\n",
    "    # model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader_train):\n",
    "        distance = targets[:,1].to(device).float()\n",
    "        targets = targets[:,0]\n",
    "        inputs, targets = inputs.to(device).float(), targets.to(device).to(torch.int64)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets, distance)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        avg_cost += loss / total_batch\n",
    "\n",
    "    \n",
    "    # if avg_cost < 0.1:\n",
    "    #     break\n",
    "    model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for X_test, Y_test in data_loader_test:\n",
    "    #         X_test = X_test.to(device).float()\n",
    "    #         Y_test = Y_test.to(device)\n",
    "\n",
    "    #     prediction = model(X_test)\n",
    "    #     correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    #     accuracy = correct_prediction.float().mean()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader_train):\n",
    "            distance = targets[:,1].to(device).float()\n",
    "            targets = targets[:,0]\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device).to(torch.int64)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets, distance)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    trainacc = 100.*correct/total\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader_test):\n",
    "            distance = targets[:,1].to(device).float()\n",
    "            targets = targets[:,0]\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device).to(torch.int64)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets, distance)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    testacc = 100.*correct/total\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost), 'trainacc:', trainacc, 'testacc:', testacc)\n",
    "\n",
    "print('Learning finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5041,  1.4875, -3.8505, -3.7793, -3.7843, -3.8735, -3.8153, -3.8244,\n",
      "         -3.8521, -3.7147],\n",
      "        [ 1.5041,  1.4875, -3.8505, -3.7793, -3.7843, -3.8735, -3.8153, -3.8244,\n",
      "         -3.8521, -3.7147],\n",
      "        [ 1.5041,  1.4875, -3.8505, -3.7793, -3.7843, -3.8735, -3.8153, -3.8244,\n",
      "         -3.8521, -3.7147],\n",
      "        [ 1.5041,  1.4875, -3.8505, -3.7793, -3.7843, -3.8735, -3.8153, -3.8244,\n",
      "         -3.8521, -3.7147],\n",
      "        [ 1.5041,  1.4875, -3.8505, -3.7793, -3.7843, -3.8735, -3.8153, -3.8244,\n",
      "         -3.8521, -3.7147],\n",
      "        [ 1.5041,  1.4875, -3.8505, -3.7793, -3.7843, -3.8735, -3.8153, -3.8244,\n",
      "         -3.8521, -3.7147],\n",
      "        [ 1.5041,  1.4875, -3.8505, -3.7793, -3.7843, -3.8735, -3.8153, -3.8244,\n",
      "         -3.8521, -3.7147],\n",
      "        [ 1.5041,  1.4875, -3.8505, -3.7793, -3.7843, -3.8735, -3.8153, -3.8244,\n",
      "         -3.8521, -3.7147],\n",
      "        [ 1.5041,  1.4875, -3.8505, -3.7793, -3.7843, -3.8735, -3.8153, -3.8244,\n",
      "         -3.8521, -3.7147],\n",
      "        [ 1.5041,  1.4875, -3.8505, -3.7793, -3.7843, -3.8735, -3.8153, -3.8244,\n",
      "         -3.8521, -3.7147]], device='cuda:0') tensor([False, False,  True,  True,  True, False, False,  True,  True, False],\n",
      "       device='cuda:0') tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1], device='cuda:0')\n",
      "testacc: 50.0\n"
     ]
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "# model.load_state_dict(torch.load(\".\\merge_save\\merge_attack0normal10_05-07--21-10-32.pth\")[\"state_dict\"])\n",
    "# model.load_state_dict(torch.load(r\".\\myfed_normal_save\\model18-24-02.pth\")[\"state_dict\"])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader_test):\n",
    "        distance = targets[:,1].to(device).float()\n",
    "        targets = targets[:,0]\n",
    "        inputs, targets = inputs.to(device).float(), targets.to(device).to(torch.int64)\n",
    "        prediction = model(inputs)\n",
    "        _, predicted = prediction.max(1)\n",
    "\n",
    "        correct_prediction = torch.argmax(prediction, 1) == targets\n",
    "        print(prediction,correct_prediction, targets)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    testacc = 100.*correct/total\n",
    "    print('testacc:', testacc)\n",
    "    # Get one and predict\n",
    "    # r = random.randint(0, len(mnist_test) - 1)\n",
    "    # X_single_data = mnist_test.data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    # Y_single_data = mnist_test.targets[r:r + 1].to(device)\n",
    "\n",
    "    # print('Label: ', Y_single_data.item())\n",
    "    # single_prediction = model(X_single_data)\n",
    "    # print('Prediction: ', torch.argmax(single_prediction, 1).item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d28dcb1746f058774bc7e3103f8fd674f9a46f28636659ba6a6fdaf0b17e1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
