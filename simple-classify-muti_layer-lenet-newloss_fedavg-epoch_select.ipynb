{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# input_data shape\n",
    "Input: (batch_size, in_channel, width, height)\n",
    "# conv layer\n",
    "class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "input: (Batch_size, C_in, H_in, W_in)\n",
    "output: (Batch_size, C_out, H_out, W_out)\n",
    "\n",
    "weight(tensor): (out_channels, in_channels,kernel_size)\n",
    "bias(tensor): (out_channel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ripser import Rips\n",
    "from persim import PersistenceImager\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import persim\n",
    "\n",
    "from typing import Callable, Optional\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import _reduction as _Reduction\n",
    "from torch import Tensor\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import re\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from operator import index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(111)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "batchsize = 25\n",
    "testbatchsize = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = os.listdir(\"./metric\")\n",
    "# data = []\n",
    "# a = []\n",
    "# for metric in dir:\n",
    "#     a = re.findall(\"\\d+\\.?\\d*\", metric)\n",
    "#     data.append([np.loadtxt(\"./metric/\"+metric), metric])\n",
    "#     # print(a[-2])\n",
    "#     print(metric[-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_dataset(Dataset):\n",
    "    def __init__(self, train, index=[i for i in range(0,100)]):\n",
    "        super().__init__()\n",
    "        added_data = \"\"\n",
    "        added_path = \"\"\n",
    "        if train:\n",
    "            dir_data = os.listdir(\"/mnt/sda3/docker_space/Code/TDA-NN/3090/result_save/saved_models_vgg16\")\n",
    "            path = \"/mnt/sda3/docker_space/Code/TDA-NN/3090/result_save/saved_models_vgg16/\"\n",
    "            # added_data = os.listdir(\"/mnt/sda3/docker_space/Code/TDA-NN/3090/grid_save/model_mnist_Nov.25_19.38.54\")\n",
    "            # added_path = \"/mnt/sda3/docker_space/Code/TDA-NN/3090/grid_save/model_mnist_Nov.25_19.38.54/\"\n",
    "        else:\n",
    "            dir_data = os.listdir(\"/mnt/sda3/docker_space/Code/TDA-NN/3090/result_save/saved_models_vgg16\")\n",
    "            path = \"/mnt/sda3/docker_space/Code/TDA-NN/3090/result_save/saved_models_vgg16/\"\n",
    "        data = []\n",
    "        sample = []\n",
    "        distance = 1\n",
    "        T_normal = []\n",
    "        T_attack = []\n",
    "        normal_attack = [0,0]\n",
    "        for metric in dir_data[0:4]:\n",
    "            dgm = []\n",
    "            grid = pd.DataFrame(np.loadtxt(path+metric))\n",
    "            for i in range(grid.shape[0]):\n",
    "                for j in range(grid.shape[1]):\n",
    "                    if grid[i][j] > 0:\n",
    "                        dgm.append([i,j])\n",
    "            T_normal.append(dgm)\n",
    "        for metric in dir_data[-4:]:\n",
    "            dgm = []\n",
    "            grid = pd.DataFrame(np.loadtxt(path+metric))\n",
    "            for i in range(grid.shape[0]):\n",
    "                for j in range(grid.shape[1]):\n",
    "                    if grid[i][j] > 0:\n",
    "                        dgm.append([i,j])\n",
    "            T_attack.append(dgm)\n",
    "            \n",
    "        for metric in dir_data:\n",
    "            if int(re.findall(\"\\d+\\.?\\d*\", metric)[0]) in index:\n",
    "                sample.append([np.loadtxt(path+metric), metric])\n",
    "                label = int(\"True\" in metric)\n",
    "\n",
    "                # if (label == 0):\n",
    "                #     distance_bottleneck, matching = persim.bottleneck(sample[len(sample)-1][0], T_normal[len(sample)-1], matching=True)\n",
    "                # else:\n",
    "                #     distance_bottleneck, matching = persim.bottleneck(sample[len(sample)-1][0], T_attack[len(sample)-1], matching=True)\n",
    "                # distance += distance_bottleneck\n",
    "                if len(sample) == 4:\n",
    "                    res = cv2.merge([i[0] for i in sample])\n",
    "                    res = np.transpose(res,(2,0,1))\n",
    "                    # d = 1/(1+math.exp(math.log(distance)))\n",
    "                    data.append([res, np.array((label,distance))])\n",
    "                    sample = []\n",
    "\n",
    "        if added_data:\n",
    "            for metric in added_data:\n",
    "                if int(re.findall(\"\\d+\\.?\\d*\", metric)[0]) in index:\n",
    "                    sample.append([np.loadtxt(added_path+metric), metric])\n",
    "                    label = int(\"True\" in metric)\n",
    "\n",
    "                    # if (label == 0):\n",
    "                    #     distance_bottleneck, matching = persim.bottleneck(sample[len(sample)-1][0], T_normal[len(sample)-1], matching=True)\n",
    "                    # else:\n",
    "                    #     distance_bottleneck, matching = persim.bottleneck(sample[len(sample)-1][0], T_attack[len(sample)-1], matching=True)\n",
    "                    # distance += distance_bottleneck\n",
    "                    if len(sample) == 4:\n",
    "                        res = cv2.merge([i[0] for i in sample])\n",
    "                        res = np.transpose(res,(2,0,1))\n",
    "                        # d = 1/(1+math.exp(math.log(distance)))\n",
    "                        data.append([res, np.array((label,distance))])\n",
    "                        sample = []\n",
    "                        \n",
    "        data.sort(key=lambda x: x[1][0])\n",
    "        if train or not train:\n",
    "            for i in data:\n",
    "                normal_attack[int(i[1][0])] += 1\n",
    "            print(normal_attack)\n",
    "            normal_attack = [max(normal_attack), min(normal_attack)]\n",
    "            data.sort(key=lambda x: x[1][0])\n",
    "            data = data[0:normal_attack[1]]+data[-normal_attack[1]:]\n",
    "            normal_attack = [0,0]\n",
    "            for i in data:\n",
    "                normal_attack[int(i[1][0])] += 1\n",
    "            print(normal_attack)\n",
    "        \n",
    "        self.x = [item[0] for item in data]\n",
    "        self.y = [item[1] for item in data]\n",
    "        # self.y = [int(re.findall(\"\\d+\\.?\\d*\", item[1])[0]) for item in data]\n",
    "        self.src,  self.trg = [], []\n",
    "        \n",
    "\n",
    "        for i in range(len(data)):\n",
    "            self.src.append(self.x[i])\n",
    "            self.trg.append(self.y[i])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.trg[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    " # 或者return len(self.trg), src和trg长度一样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) /io/opencv/modules/core/src/merge.dispatch.cpp:129: error: (-215:Assertion failed) mv[i].size == mv[0].size && mv[i].depth() == depth in function 'merge'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m data_tf \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m      2\u001b[0m     [transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      3\u001b[0m      transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.5\u001b[39m], [\u001b[38;5;241m0.5\u001b[39m])])\n\u001b[0;32m----> 5\u001b[0m data_train \u001b[38;5;241m=\u001b[39m \u001b[43mMy_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m data_test \u001b[38;5;241m=\u001b[39m My_dataset(train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m[i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m30\u001b[39m)])\n\u001b[1;32m      7\u001b[0m data_loader_train \u001b[38;5;241m=\u001b[39m DataLoader(data_train, batch_size\u001b[38;5;241m=\u001b[39mbatchsize, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [9], line 48\u001b[0m, in \u001b[0;36mMy_dataset.__init__\u001b[0;34m(self, train, index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# if (label == 0):\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#     distance_bottleneck, matching = persim.bottleneck(sample[len(sample)-1][0], T_normal[len(sample)-1], matching=True)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#     distance_bottleneck, matching = persim.bottleneck(sample[len(sample)-1][0], T_attack[len(sample)-1], matching=True)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# distance += distance_bottleneck\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sample) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     res \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(res,(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# d = 1/(1+math.exp(math.log(distance)))\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/core/src/merge.dispatch.cpp:129: error: (-215:Assertion failed) mv[i].size == mv[0].size && mv[i].depth() == depth in function 'merge'\n"
     ]
    }
   ],
   "source": [
    "data_tf = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5], [0.5])])\n",
    "     \n",
    "data_train = My_dataset(train=True, index=[i for i in range(10,25)])\n",
    "data_test = My_dataset(train=False, index=[i for i in range(25,30)])\n",
    "data_loader_train = DataLoader(data_train, batch_size=batchsize, shuffle=True)\n",
    "data_loader_test = DataLoader(data_test, batch_size=testbatchsize, shuffle=False)\n",
    "\n",
    "\n",
    "# i_batch的多少根据batch size和def __len__(self)返回的长度确定\n",
    "# batch_data返回的值根据def __getitem__(self, index)来确定\n",
    "# 对训练集：(不太清楚enumerate返回什么的时候就多print试试)\n",
    "print(\"train\")\n",
    "for i_batch, batch_data in enumerate(data_loader_train):\n",
    "    print(i_batch)  # 打印batch编号\n",
    "    print(batch_data[0].shape)  # 打印该batch里面src\n",
    "    # print(batch_data[1])  # 打印该batch里面trg\n",
    "# # 对测试集：（下面的语句也可以）\n",
    "print(\"test\")\n",
    "for i_batch, batch_data in enumerate(data_loader_test):\n",
    "    print(i_batch)  # 打印batch编号\n",
    "    print(batch_data[0].shape)  # 打印该batch里面src\n",
    "    # print(batch_data[1])  # 打印该batch里面trg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(4, 32, kernel_size=7),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    " \n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    " \n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=5),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    " \n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=5),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    " \n",
    "        self.layer6 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(18432, 1024),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(1024, 128),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss2(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    def forward(self, output, target):\n",
    "        res = -output.gather(dim=1, index=target.view(-1, 1))\n",
    "        res += torch.log(torch.exp(output).sum(dim=1).view(-1, 1))\n",
    "        res = res.mean()\n",
    "        return res\n",
    "    def forward(self, output, target, distance):\n",
    "        res = -output.gather(dim=1, index=target.view(-1, 1))\n",
    "        res += torch.log(torch.exp(output).sum(dim=1).view(-1, 1))\n",
    "        res = res.mean() * distance.mean()\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _Loss(nn.Module):\n",
    "    reduction: str\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction: str = _Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "class _WeightedLoss(_Loss):\n",
    "    def __init__(self, weight: Optional[Tensor] = None, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(_WeightedLoss, self).__init__(size_average, reduce, reduction)\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.weight: Optional[Tensor]\n",
    "class CELoss(_WeightedLoss): # 注意继承 nn.Module\n",
    "    __constants__ = ['ignore_index', 'reduction', 'label_smoothing']\n",
    "    ignore_index: int\n",
    "    label_smoothing: float\n",
    "\n",
    "    def __init__(self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100,\n",
    "                 reduce=None, reduction: str = 'mean', label_smoothing: float = 0.0) -> None:\n",
    "        super(CELoss, self).__init__(weight, size_average, reduce, reduction)\n",
    "        self.ignore_index = ignore_index\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor, distance) -> Tensor:\n",
    "        return distance.mean() * F.cross_entropy(input, target, weight=self.weight,\n",
    "                               ignore_index=self.ignore_index, reduction=self.reduction,\n",
    "                               label_smoothing=self.label_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MyNet().to(device)\n",
    "# define cost/loss & optimizer\n",
    "# Softmax is internally computed.\n",
    "# criterion = CELoss().to(device)\n",
    "criterion = CELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.688548207 trainacc: 50.0 testacc: 50.0 test_loss: 4.155261278152466\n",
      "Epoch: 0002 cost = 0.695226610 trainacc: 50.0 testacc: 50.0 test_loss: 4.160943925380707\n",
      "Epoch: 0003 cost = 0.688922107 trainacc: 50.0 testacc: 50.0 test_loss: 4.157195031642914\n",
      "Epoch: 0004 cost = 0.686273873 trainacc: 50.0 testacc: 50.0 test_loss: 4.15454775094986\n",
      "Epoch: 0005 cost = 0.693543196 trainacc: 52.27272727272727 testacc: 50.0 test_loss: 4.161407947540283\n",
      "Epoch: 0006 cost = 0.683184266 trainacc: 50.0 testacc: 50.0 test_loss: 4.157709717750549\n",
      "Epoch: 0007 cost = 0.690755367 trainacc: 63.06818181818182 testacc: 50.0 test_loss: 4.16610062122345\n",
      "Epoch: 0008 cost = 0.689772189 trainacc: 73.29545454545455 testacc: 44.642857142857146 test_loss: 4.170531630516052\n",
      "Epoch: 0009 cost = 0.686904550 trainacc: 76.70454545454545 testacc: 48.214285714285715 test_loss: 4.175461411476135\n",
      "Epoch: 0010 cost = 0.681653559 trainacc: 72.1590909090909 testacc: 46.42857142857143 test_loss: 4.164863348007202\n",
      "Epoch: 0011 cost = 0.679295838 trainacc: 78.4090909090909 testacc: 44.642857142857146 test_loss: 4.173791468143463\n",
      "Epoch: 0012 cost = 0.680942595 trainacc: 67.04545454545455 testacc: 50.0 test_loss: 4.158870995044708\n",
      "Epoch: 0013 cost = 0.681206346 trainacc: 56.81818181818182 testacc: 51.785714285714285 test_loss: 4.156812250614166\n",
      "Epoch: 0014 cost = 0.680866599 trainacc: 78.4090909090909 testacc: 48.214285714285715 test_loss: 4.165165543556213\n",
      "Epoch: 0015 cost = 0.675195873 trainacc: 65.9090909090909 testacc: 48.214285714285715 test_loss: 4.155968368053436\n",
      "Epoch: 0016 cost = 0.673008323 trainacc: 53.97727272727273 testacc: 50.0 test_loss: 4.15074610710144\n",
      "Epoch: 0017 cost = 0.669453442 trainacc: 52.27272727272727 testacc: 50.0 test_loss: 4.1488378047943115\n",
      "Epoch: 0018 cost = 0.680538416 trainacc: 77.8409090909091 testacc: 51.785714285714285 test_loss: 4.159016966819763\n",
      "Epoch: 0019 cost = 0.672946692 trainacc: 81.25 testacc: 44.642857142857146 test_loss: 4.170343577861786\n",
      "Epoch: 0020 cost = 0.667441845 trainacc: 75.0 testacc: 51.785714285714285 test_loss: 4.151135444641113\n",
      "Epoch: 0021 cost = 0.665468574 trainacc: 82.38636363636364 testacc: 42.857142857142854 test_loss: 4.168553590774536\n",
      "Epoch: 0022 cost = 0.670257449 trainacc: 78.4090909090909 testacc: 50.0 test_loss: 4.186382412910461\n",
      "Epoch: 0023 cost = 0.667812467 trainacc: 78.4090909090909 testacc: 50.0 test_loss: 4.186500728130341\n",
      "Epoch: 0024 cost = 0.669994354 trainacc: 69.88636363636364 testacc: 46.42857142857143 test_loss: 4.1996495723724365\n",
      "Epoch: 0025 cost = 0.665589809 trainacc: 83.52272727272727 testacc: 50.0 test_loss: 4.157918334007263\n",
      "Epoch: 0026 cost = 0.659121037 trainacc: 80.11363636363636 testacc: 50.0 test_loss: 4.151445508003235\n",
      "Epoch: 0027 cost = 0.651366472 trainacc: 73.29545454545455 testacc: 48.214285714285715 test_loss: 4.14778470993042\n",
      "Epoch: 0028 cost = 0.661319196 trainacc: 83.52272727272727 testacc: 51.785714285714285 test_loss: 4.1691723465919495\n",
      "Epoch: 0029 cost = 0.652752161 trainacc: 79.54545454545455 testacc: 50.0 test_loss: 4.199289619922638\n",
      "Epoch: 0030 cost = 0.649036109 trainacc: 82.38636363636364 testacc: 48.214285714285715 test_loss: 4.157501399517059\n",
      "Epoch: 0031 cost = 0.647112370 trainacc: 81.25 testacc: 48.214285714285715 test_loss: 4.185575008392334\n",
      "Epoch: 0032 cost = 0.644324481 trainacc: 83.52272727272727 testacc: 44.642857142857146 test_loss: 4.157047986984253\n",
      "Epoch: 0033 cost = 0.664677918 trainacc: 81.25 testacc: 50.0 test_loss: 4.177037417888641\n",
      "Epoch: 0034 cost = 0.642522633 trainacc: 78.4090909090909 testacc: 53.57142857142857 test_loss: 4.201619446277618\n",
      "Epoch: 0035 cost = 0.641777337 trainacc: 82.38636363636364 testacc: 48.214285714285715 test_loss: 4.156492292881012\n",
      "Epoch: 0036 cost = 0.636180997 trainacc: 80.11363636363636 testacc: 46.42857142857143 test_loss: 4.1484270095825195\n",
      "Epoch: 0037 cost = 0.642572105 trainacc: 68.75 testacc: 48.214285714285715 test_loss: 4.140294849872589\n",
      "Epoch: 0038 cost = 0.640559375 trainacc: 81.81818181818181 testacc: 48.214285714285715 test_loss: 4.187652587890625\n",
      "Epoch: 0039 cost = 0.630349338 trainacc: 82.95454545454545 testacc: 46.42857142857143 test_loss: 4.152682423591614\n",
      "Epoch: 0040 cost = 0.635243595 trainacc: 71.02272727272727 testacc: 48.214285714285715 test_loss: 4.14533394575119\n",
      "Epoch: 0041 cost = 0.619054437 trainacc: 78.4090909090909 testacc: 51.785714285714285 test_loss: 4.153235018253326\n",
      "Epoch: 0042 cost = 0.622451603 trainacc: 81.81818181818181 testacc: 50.0 test_loss: 4.193588495254517\n",
      "Epoch: 0043 cost = 0.617397487 trainacc: 83.52272727272727 testacc: 48.214285714285715 test_loss: 4.156451344490051\n",
      "Epoch: 0044 cost = 0.607800126 trainacc: 83.52272727272727 testacc: 48.214285714285715 test_loss: 4.158629357814789\n",
      "Epoch: 0045 cost = 0.614294052 trainacc: 81.81818181818181 testacc: 48.214285714285715 test_loss: 4.156171381473541\n",
      "Epoch: 0046 cost = 0.614747345 trainacc: 71.5909090909091 testacc: 50.0 test_loss: 4.155452132225037\n",
      "Epoch: 0047 cost = 0.597688019 trainacc: 82.95454545454545 testacc: 48.214285714285715 test_loss: 4.167344629764557\n",
      "Epoch: 0048 cost = 0.597078323 trainacc: 82.95454545454545 testacc: 46.42857142857143 test_loss: 4.171494841575623\n",
      "Epoch: 0049 cost = 0.609254718 trainacc: 71.5909090909091 testacc: 48.214285714285715 test_loss: 4.168106317520142\n",
      "Epoch: 0050 cost = 0.597484946 trainacc: 75.0 testacc: 50.0 test_loss: 4.175452172756195\n",
      "Epoch: 0051 cost = 0.595559835 trainacc: 83.52272727272727 testacc: 48.214285714285715 test_loss: 4.231711447238922\n",
      "Epoch: 0052 cost = 0.614008904 trainacc: 78.97727272727273 testacc: 50.0 test_loss: 4.174118101596832\n",
      "Epoch: 0053 cost = 0.591403365 trainacc: 78.97727272727273 testacc: 50.0 test_loss: 4.174309194087982\n",
      "Epoch: 0054 cost = 0.592176437 trainacc: 75.56818181818181 testacc: 50.0 test_loss: 4.181668817996979\n",
      "Epoch: 0055 cost = 0.581460476 trainacc: 83.52272727272727 testacc: 50.0 test_loss: 4.183005273342133\n",
      "Epoch: 0056 cost = 0.582211614 trainacc: 84.0909090909091 testacc: 48.214285714285715 test_loss: 4.180894613265991\n",
      "Epoch: 0057 cost = 0.568163872 trainacc: 82.38636363636364 testacc: 50.0 test_loss: 4.18402236700058\n",
      "Epoch: 0058 cost = 0.572937429 trainacc: 82.38636363636364 testacc: 50.0 test_loss: 4.2519078850746155\n",
      "Epoch: 0059 cost = 0.586633086 trainacc: 75.56818181818181 testacc: 48.214285714285715 test_loss: 4.33204573392868\n",
      "Epoch: 0060 cost = 0.567301691 trainacc: 77.27272727272727 testacc: 50.0 test_loss: 4.322306156158447\n",
      "Epoch: 0061 cost = 0.558881462 trainacc: 85.22727272727273 testacc: 51.785714285714285 test_loss: 4.199238717556\n",
      "Epoch: 0062 cost = 0.579916239 trainacc: 80.68181818181819 testacc: 50.0 test_loss: 4.299716830253601\n",
      "Epoch: 0063 cost = 0.550391734 trainacc: 86.93181818181819 testacc: 53.57142857142857 test_loss: 4.211383938789368\n",
      "Epoch: 0064 cost = 0.530384421 trainacc: 79.54545454545455 testacc: 51.785714285714285 test_loss: 4.3279144167900085\n",
      "Epoch: 0065 cost = 0.559644818 trainacc: 82.38636363636364 testacc: 50.0 test_loss: 4.208095908164978\n",
      "Epoch: 0066 cost = 0.531635940 trainacc: 82.38636363636364 testacc: 51.785714285714285 test_loss: 4.3114288449287415\n",
      "Epoch: 0067 cost = 0.546093285 trainacc: 73.29545454545455 testacc: 50.0 test_loss: 4.4514628648757935\n",
      "Epoch: 0068 cost = 0.528402328 trainacc: 86.36363636363636 testacc: 51.785714285714285 test_loss: 4.235860466957092\n",
      "Epoch: 0069 cost = 0.529810131 trainacc: 78.97727272727273 testacc: 51.785714285714285 test_loss: 4.421355426311493\n",
      "Epoch: 0070 cost = 0.515213192 trainacc: 88.63636363636364 testacc: 53.57142857142857 test_loss: 4.274693787097931\n",
      "Epoch: 0071 cost = 0.576235414 trainacc: 83.52272727272727 testacc: 48.214285714285715 test_loss: 4.371470808982849\n",
      "Epoch: 0072 cost = 0.494253218 trainacc: 88.06818181818181 testacc: 53.57142857142857 test_loss: 4.2580883502960205\n",
      "Epoch: 0073 cost = 0.500839710 trainacc: 78.4090909090909 testacc: 51.785714285714285 test_loss: 4.4861356019973755\n",
      "Epoch: 0074 cost = 0.513718367 trainacc: 77.27272727272727 testacc: 48.214285714285715 test_loss: 4.55587899684906\n",
      "Epoch: 0075 cost = 0.523853362 trainacc: 76.13636363636364 testacc: 46.42857142857143 test_loss: 4.601069509983063\n",
      "Epoch: 0076 cost = 0.471974105 trainacc: 87.5 testacc: 58.92857142857143 test_loss: 4.338389754295349\n",
      "Epoch: 0077 cost = 0.502479613 trainacc: 75.56818181818181 testacc: 48.214285714285715 test_loss: 4.705543339252472\n",
      "Epoch: 0078 cost = 0.524906397 trainacc: 78.97727272727273 testacc: 53.57142857142857 test_loss: 4.374922513961792\n",
      "Epoch: 0079 cost = 0.471658766 trainacc: 79.54545454545455 testacc: 51.785714285714285 test_loss: 4.380230903625488\n",
      "Epoch: 0080 cost = 0.456967443 trainacc: 79.54545454545455 testacc: 50.0 test_loss: 4.618087351322174\n",
      "Epoch: 0081 cost = 0.469882369 trainacc: 82.38636363636364 testacc: 51.785714285714285 test_loss: 4.400791883468628\n",
      "Epoch: 0082 cost = 0.473679662 trainacc: 80.68181818181819 testacc: 50.0 test_loss: 4.436087429523468\n",
      "Epoch: 0083 cost = 0.448113441 trainacc: 85.79545454545455 testacc: 50.0 test_loss: 4.422586143016815\n",
      "Epoch: 0084 cost = 0.453033537 trainacc: 85.22727272727273 testacc: 51.785714285714285 test_loss: 4.4351626932621\n",
      "Epoch: 0085 cost = 0.428559750 trainacc: 85.22727272727273 testacc: 48.214285714285715 test_loss: 4.6105905175209045\n",
      "Epoch: 0086 cost = 0.412145436 trainacc: 85.79545454545455 testacc: 51.785714285714285 test_loss: 4.602636158466339\n",
      "Epoch: 0087 cost = 0.435660124 trainacc: 87.5 testacc: 53.57142857142857 test_loss: 4.46728852391243\n",
      "Epoch: 0088 cost = 0.435875416 trainacc: 81.81818181818181 testacc: 50.0 test_loss: 4.545303374528885\n",
      "Epoch: 0089 cost = 0.402547777 trainacc: 85.22727272727273 testacc: 46.42857142857143 test_loss: 4.6961681842803955\n",
      "Epoch: 0090 cost = 0.478917867 trainacc: 68.75 testacc: 55.357142857142854 test_loss: 4.820749074220657\n",
      "Epoch: 0091 cost = 0.404395342 trainacc: 84.0909090909091 testacc: 44.642857142857146 test_loss: 4.801889419555664\n",
      "Epoch: 0092 cost = 0.412842542 trainacc: 80.68181818181819 testacc: 48.214285714285715 test_loss: 5.020692855119705\n",
      "Epoch: 0093 cost = 0.450358272 trainacc: 71.5909090909091 testacc: 55.357142857142854 test_loss: 4.814556330442429\n",
      "Epoch: 0094 cost = 0.374766320 trainacc: 90.9090909090909 testacc: 50.0 test_loss: 4.69824081659317\n",
      "Epoch: 0095 cost = 0.367074192 trainacc: 84.0909090909091 testacc: 46.42857142857143 test_loss: 4.896589457988739\n",
      "Epoch: 0096 cost = 0.413000524 trainacc: 70.45454545454545 testacc: 53.57142857142857 test_loss: 4.917654633522034\n",
      "Epoch: 0097 cost = 0.358510345 trainacc: 93.75 testacc: 55.357142857142854 test_loss: 4.54288312792778\n",
      "Epoch: 0098 cost = 0.348655671 trainacc: 90.3409090909091 testacc: 53.57142857142857 test_loss: 4.6318182945251465\n",
      "Epoch: 0099 cost = 0.334361464 trainacc: 88.06818181818181 testacc: 46.42857142857143 test_loss: 4.814872741699219\n",
      "Epoch: 0100 cost = 0.333568037 trainacc: 93.75 testacc: 53.57142857142857 test_loss: 4.577649801969528\n",
      "Epoch: 0101 cost = 0.348493755 trainacc: 82.38636363636364 testacc: 48.214285714285715 test_loss: 4.788898676633835\n",
      "Epoch: 0102 cost = 0.337002337 trainacc: 88.06818181818181 testacc: 46.42857142857143 test_loss: 5.020341515541077\n",
      "Epoch: 0103 cost = 0.312001258 trainacc: 90.3409090909091 testacc: 50.0 test_loss: 4.928569972515106\n",
      "Epoch: 0104 cost = 0.307354897 trainacc: 88.06818181818181 testacc: 50.0 test_loss: 4.809073284268379\n",
      "Epoch: 0105 cost = 0.306497753 trainacc: 88.06818181818181 testacc: 50.0 test_loss: 4.8406490087509155\n",
      "Epoch: 0106 cost = 0.294765979 trainacc: 94.88636363636364 testacc: 53.57142857142857 test_loss: 4.72328644990921\n",
      "Epoch: 0107 cost = 0.312139899 trainacc: 80.11363636363636 testacc: 51.785714285714285 test_loss: 4.973523944616318\n",
      "Epoch: 0108 cost = 0.319566250 trainacc: 82.38636363636364 testacc: 50.0 test_loss: 5.472507059574127\n",
      "Epoch: 0109 cost = 0.274615228 trainacc: 95.45454545454545 testacc: 51.785714285714285 test_loss: 4.8759284019470215\n",
      "Epoch: 0110 cost = 0.288508803 trainacc: 84.6590909090909 testacc: 46.42857142857143 test_loss: 5.553106814622879\n",
      "Epoch: 0111 cost = 0.278395683 trainacc: 88.06818181818181 testacc: 50.0 test_loss: 5.410800576210022\n",
      "Epoch: 0112 cost = 0.269648373 trainacc: 88.06818181818181 testacc: 50.0 test_loss: 5.453789919614792\n",
      "Epoch: 0113 cost = 0.254630268 trainacc: 94.88636363636364 testacc: 53.57142857142857 test_loss: 4.943526715040207\n",
      "Epoch: 0114 cost = 0.302021861 trainacc: 57.95454545454545 testacc: 48.214285714285715 test_loss: 7.471220403909683\n",
      "Epoch: 0115 cost = 0.297328711 trainacc: 88.63636363636364 testacc: 50.0 test_loss: 5.621224254369736\n",
      "Epoch: 0116 cost = 0.250019252 trainacc: 90.9090909090909 testacc: 50.0 test_loss: 5.485203504562378\n",
      "Epoch: 0117 cost = 0.229131013 trainacc: 97.1590909090909 testacc: 51.785714285714285 test_loss: 5.151706576347351\n",
      "Epoch: 0118 cost = 0.213702202 trainacc: 97.72727272727273 testacc: 53.57142857142857 test_loss: 5.084820449352264\n",
      "Epoch: 0119 cost = 0.254467905 trainacc: 69.88636363636364 testacc: 53.57142857142857 test_loss: 6.672482192516327\n",
      "Epoch: 0120 cost = 0.283677995 trainacc: 65.9090909090909 testacc: 53.57142857142857 test_loss: 6.967347219586372\n",
      "Epoch: 0121 cost = 0.246718660 trainacc: 87.5 testacc: 50.0 test_loss: 5.348714172840118\n",
      "Epoch: 0122 cost = 0.198928148 trainacc: 97.72727272727273 testacc: 53.57142857142857 test_loss: 5.194118559360504\n",
      "Epoch: 0123 cost = 0.268975019 trainacc: 53.40909090909091 testacc: 50.0 test_loss: 8.978957265615463\n",
      "Epoch: 0124 cost = 0.250114501 trainacc: 97.72727272727273 testacc: 57.142857142857146 test_loss: 5.030984461307526\n",
      "Epoch: 0125 cost = 0.187775090 trainacc: 96.5909090909091 testacc: 48.214285714285715 test_loss: 5.583132028579712\n",
      "Epoch: 0126 cost = 0.179441914 trainacc: 98.29545454545455 testacc: 51.785714285714285 test_loss: 5.498836040496826\n",
      "Epoch: 0127 cost = 0.191805422 trainacc: 94.88636363636364 testacc: 50.0 test_loss: 5.48304095864296\n",
      "Epoch: 0128 cost = 0.176494867 trainacc: 97.1590909090909 testacc: 55.357142857142854 test_loss: 5.279601722955704\n",
      "Epoch: 0129 cost = 0.159644067 trainacc: 98.86363636363636 testacc: 50.0 test_loss: 5.370700001716614\n",
      "Epoch: 0130 cost = 0.154575303 trainacc: 98.86363636363636 testacc: 48.214285714285715 test_loss: 5.406920671463013\n",
      "Epoch: 0131 cost = 0.145844340 trainacc: 98.29545454545455 testacc: 50.0 test_loss: 5.513124465942383\n",
      "Epoch: 0132 cost = 0.144514501 trainacc: 98.86363636363636 testacc: 53.57142857142857 test_loss: 5.242809891700745\n",
      "Epoch: 0133 cost = 0.149299592 trainacc: 98.29545454545455 testacc: 51.785714285714285 test_loss: 5.8193581104278564\n",
      "Epoch: 0134 cost = 0.132451892 trainacc: 98.86363636363636 testacc: 51.785714285714285 test_loss: 5.4135841727256775\n",
      "Epoch: 0135 cost = 0.138939276 trainacc: 98.86363636363636 testacc: 53.57142857142857 test_loss: 5.663494348526001\n",
      "Epoch: 0136 cost = 0.131223381 trainacc: 98.86363636363636 testacc: 57.142857142857146 test_loss: 5.515708029270172\n",
      "Epoch: 0137 cost = 0.145777151 trainacc: 81.81818181818181 testacc: 53.57142857142857 test_loss: 6.661691896617413\n",
      "Epoch: 0138 cost = 0.154028878 trainacc: 94.31818181818181 testacc: 51.785714285714285 test_loss: 6.0272752568125725\n",
      "Epoch: 0139 cost = 0.131036535 trainacc: 99.43181818181819 testacc: 55.357142857142854 test_loss: 5.509745717048645\n",
      "Epoch: 0140 cost = 0.118322782 trainacc: 98.86363636363636 testacc: 53.57142857142857 test_loss: 5.42038568854332\n",
      "Epoch: 0141 cost = 0.116879337 trainacc: 99.43181818181819 testacc: 51.785714285714285 test_loss: 5.6593576073646545\n",
      "Epoch: 0142 cost = 0.106140412 trainacc: 99.43181818181819 testacc: 48.214285714285715 test_loss: 5.599269449710846\n",
      "Epoch: 0143 cost = 0.100798607 trainacc: 99.43181818181819 testacc: 51.785714285714285 test_loss: 5.759352803230286\n",
      "Epoch: 0144 cost = 0.098807476 trainacc: 99.43181818181819 testacc: 50.0 test_loss: 5.6104350090026855\n",
      "Epoch: 0145 cost = 0.103265397 trainacc: 99.43181818181819 testacc: 48.214285714285715 test_loss: 5.476911544799805\n",
      "Epoch: 0146 cost = 0.111042313 trainacc: 99.43181818181819 testacc: 55.357142857142854 test_loss: 5.583047911524773\n",
      "Epoch: 0147 cost = 0.096627854 trainacc: 99.43181818181819 testacc: 48.214285714285715 test_loss: 5.730410993099213\n",
      "Epoch: 0148 cost = 0.092659146 trainacc: 99.43181818181819 testacc: 53.57142857142857 test_loss: 5.595344722270966\n",
      "Epoch: 0149 cost = 0.093832716 trainacc: 99.43181818181819 testacc: 50.0 test_loss: 6.30949592590332\n",
      "Epoch: 0150 cost = 0.095782198 trainacc: 98.86363636363636 testacc: 55.357142857142854 test_loss: 5.679968774318695\n",
      "Epoch: 0151 cost = 0.089241192 trainacc: 99.43181818181819 testacc: 55.357142857142854 test_loss: 5.633127123117447\n",
      "Epoch: 0152 cost = 0.083322212 trainacc: 99.43181818181819 testacc: 53.57142857142857 test_loss: 5.5993263721466064\n",
      "Epoch: 0153 cost = 0.079081528 trainacc: 99.43181818181819 testacc: 53.57142857142857 test_loss: 5.794640630483627\n",
      "Epoch: 0154 cost = 0.088955022 trainacc: 98.86363636363636 testacc: 50.0 test_loss: 7.093008309602737\n",
      "Epoch: 0155 cost = 0.095124535 trainacc: 99.43181818181819 testacc: 55.357142857142854 test_loss: 6.1561179757118225\n",
      "Epoch: 0156 cost = 0.079679668 trainacc: 99.43181818181819 testacc: 53.57142857142857 test_loss: 5.959212809801102\n",
      "Epoch: 0157 cost = 0.071936630 trainacc: 99.43181818181819 testacc: 51.785714285714285 test_loss: 6.004988670349121\n",
      "Epoch: 0158 cost = 0.071702033 trainacc: 99.43181818181819 testacc: 50.0 test_loss: 5.850873112678528\n",
      "Epoch: 0159 cost = 0.073422462 trainacc: 99.43181818181819 testacc: 51.785714285714285 test_loss: 6.050731182098389\n",
      "Epoch: 0160 cost = 0.075411774 trainacc: 99.43181818181819 testacc: 53.57142857142857 test_loss: 6.510926067829132\n",
      "Epoch: 0161 cost = 0.078246683 trainacc: 99.43181818181819 testacc: 53.57142857142857 test_loss: 6.296340346336365\n",
      "Epoch: 0162 cost = 0.068751574 trainacc: 99.43181818181819 testacc: 50.0 test_loss: 6.024947226047516\n",
      "Epoch: 0163 cost = 0.069141544 trainacc: 99.43181818181819 testacc: 51.785714285714285 test_loss: 6.197103261947632\n",
      "Epoch: 0164 cost = 0.067284621 trainacc: 99.43181818181819 testacc: 51.785714285714285 test_loss: 6.233783423900604\n",
      "Epoch: 0165 cost = 0.060307253 trainacc: 99.43181818181819 testacc: 53.57142857142857 test_loss: 5.992126435041428\n",
      "Epoch: 0166 cost = 0.071771115 trainacc: 99.43181818181819 testacc: 55.357142857142854 test_loss: 6.458496928215027\n",
      "Epoch: 0167 cost = 0.070519865 trainacc: 100.0 testacc: 55.357142857142854 test_loss: 6.156865686178207\n",
      "Epoch: 0168 cost = 0.064098366 trainacc: 99.43181818181819 testacc: 51.785714285714285 test_loss: 6.747420012950897\n",
      "Epoch: 0169 cost = 0.059339855 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.285669416189194\n",
      "Epoch: 0170 cost = 0.054774195 trainacc: 99.43181818181819 testacc: 50.0 test_loss: 6.46891987323761\n",
      "Epoch: 0171 cost = 0.053994745 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.396312743425369\n",
      "Epoch: 0172 cost = 0.054788869 trainacc: 99.43181818181819 testacc: 50.0 test_loss: 6.533442854881287\n",
      "Epoch: 0173 cost = 0.061236791 trainacc: 98.29545454545455 testacc: 50.0 test_loss: 8.166342407464981\n",
      "Epoch: 0174 cost = 0.057488877 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.318722248077393\n",
      "Epoch: 0175 cost = 0.049092222 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.326420783996582\n",
      "Epoch: 0176 cost = 0.048169699 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.244608938694\n",
      "Epoch: 0177 cost = 0.048117682 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.4437335729599\n",
      "Epoch: 0178 cost = 0.046857569 trainacc: 99.43181818181819 testacc: 53.57142857142857 test_loss: 6.373706489801407\n",
      "Epoch: 0179 cost = 0.044688597 trainacc: 99.43181818181819 testacc: 48.214285714285715 test_loss: 6.54889303445816\n",
      "Epoch: 0180 cost = 0.043027624 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.573428720235825\n",
      "Epoch: 0181 cost = 0.045824390 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.594155669212341\n",
      "Epoch: 0182 cost = 0.043378271 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.491731971502304\n",
      "Epoch: 0183 cost = 0.044159252 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.618421405553818\n",
      "Epoch: 0184 cost = 0.046560656 trainacc: 99.43181818181819 testacc: 51.785714285714285 test_loss: 7.1906551122665405\n",
      "Epoch: 0185 cost = 0.044749979 trainacc: 99.43181818181819 testacc: 50.0 test_loss: 6.714734435081482\n",
      "Epoch: 0186 cost = 0.043275684 trainacc: 99.43181818181819 testacc: 51.785714285714285 test_loss: 6.850287616252899\n",
      "Epoch: 0187 cost = 0.039396863 trainacc: 100.0 testacc: 51.785714285714285 test_loss: 6.613635718822479\n",
      "Epoch: 0188 cost = 0.039618652 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.558383405208588\n",
      "Epoch: 0189 cost = 0.036887817 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.597661763429642\n",
      "Epoch: 0190 cost = 0.037355818 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.650852859020233\n",
      "Epoch: 0191 cost = 0.039138533 trainacc: 100.0 testacc: 51.785714285714285 test_loss: 6.9054646492004395\n",
      "Epoch: 0192 cost = 0.037913125 trainacc: 100.0 testacc: 48.214285714285715 test_loss: 6.470493912696838\n",
      "Epoch: 0193 cost = 0.034762800 trainacc: 100.0 testacc: 51.785714285714285 test_loss: 6.486116349697113\n",
      "Epoch: 0194 cost = 0.037069976 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.541032701730728\n",
      "Epoch: 0195 cost = 0.034193631 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.590524733066559\n",
      "Epoch: 0196 cost = 0.034870468 trainacc: 100.0 testacc: 50.0 test_loss: 6.7162434458732605\n",
      "Epoch: 0197 cost = 0.035074584 trainacc: 100.0 testacc: 51.785714285714285 test_loss: 6.766812264919281\n",
      "Epoch: 0198 cost = 0.035011649 trainacc: 100.0 testacc: 55.357142857142854 test_loss: 6.4976915419101715\n",
      "Epoch: 0199 cost = 0.033014007 trainacc: 100.0 testacc: 53.57142857142857 test_loss: 6.618038177490234\n",
      "Epoch: 0200 cost = 0.032458540 trainacc: 100.0 testacc: 50.0 test_loss: 6.896304070949554\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "total_batch = len(data_loader_train)\n",
    "top = 0\n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    avg_cost = 0\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader_train):\n",
    "        distance = targets[:,1].to(device).float()\n",
    "        targets = targets[:,0]\n",
    "        inputs, targets = inputs.to(device).float(), targets.to(device).to(torch.int64)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets, distance)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        avg_cost += loss / total_batch\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader_train):\n",
    "            distance = targets[:,1].to(device).float()\n",
    "            targets = targets[:,0]\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device).to(torch.int64)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets, distance)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    trainacc = 100.*correct/total\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader_test):\n",
    "            distance = targets[:,1].to(device).float()\n",
    "            targets = targets[:,0]\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device).to(torch.int64)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets, distance)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    testacc = 100.*correct/total\n",
    "\n",
    "    if testacc > top and epoch > 50:\n",
    "        top = testacc\n",
    "        topmodel = deepcopy(model)\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost), 'trainacc:', trainacc, 'testacc:', testacc, 'test_loss:', test_loss)\n",
    "\n",
    "print('Learning finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "tensor([0, 0, 0, 1, 0, 1], device='cuda:0') tensor([1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "testacc: 73.07692307692308\n"
     ]
    }
   ],
   "source": [
    "eval_model = deepcopy(topmodel)\n",
    "eval_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader_test):\n",
    "        # if batch_idx >= 34: break\n",
    "        if batch_idx < 3: continue\n",
    "        # if batch_idx == 3: continue\n",
    "        distance = targets[:,1].to(device).float()\n",
    "        targets = targets[:,0]\n",
    "        inputs, targets = inputs.to(device).float(), targets.to(device).to(torch.int64)\n",
    "        outputs = eval_model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(torch.argmax(outputs, 1), targets)\n",
    "    testacc = 100.*correct/total\n",
    "    print('testacc:', testacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.92857142857143\n"
     ]
    }
   ],
   "source": [
    "print(top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8bff0657a98b8ee576cbe89028f6a544b770fee234379978e147d89ef60d92ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
